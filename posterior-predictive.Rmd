# Posterior Predictive Checks

One way to evaluate the fit of a model is posterior predictive checks. 

- Fit the model to the data to get the posterior distribution of the parameters: $p(\theta | D)$
- Simulate data from the fitted model: $p(\tilde{D} | \theta, D)$
- Compare the simulated data (or a statistic thereof) to the observed data and a statistic thereof. This comparison can be formal or visual.


See  @Gelman2007a,@Gelman2009a,@GelmanCarlinSternEtAl2013a,@Gelman2014 for discussions of this.

Let $y = (y_1, \dots, y_n)$ be the observed data.
Suppose the model has been fit and there is a set of simulation $\theta^(s)$, $s = 1, \dots, n_sims$.
For each draw a replicated dataset, $y^{rep(s)$, has been generated from the predictive distribution
of the data, $p(y^{(rep)} | \theta = \theta^{(s)}$.
Then the ensemble of simulated datasets, $(y^{rep(s)}, \dots, y^{rep(nsims)})$, is a sample from the posterior predictive
distribution, $p(y^{(rep)} | y)$

This gives a *Bayeisan p-value*.
The model can be tested by means of discrepency statistics, which are some function of the data and parameters, $T(y, \theta)$.
If $\theta$ was known, then compare discrepency by $T(y^{(rep)}, \theta)$.
The statistical significance is $p = \Pr(T(y^{(rep)}, \theta) > T(y, \theta) | y, \theta)$.
If $\theta$ is unknown, then average over the posterior distribution of $\theta$,
$$
p = \Pr(T(y^{(rep)}, \theta) > T(y, \theta) | y) = \int Pr(T(y^{(rep)}, \theta) > T(y, \theta) | y, \theta) p(\theta | y) d\,\theta ,
$$
which is easily estimated from the MCMC samples as,
$$
p = \frac{1}{nsims}\sum_{s = 1}^{n_{sims}} 1( T(y^{rep(s)}, \theta(s)) > T(y, \theta(s)))
$$


Other ways of characterizing the discrepancy between $y$ and $y_rep$ include (LaplacesDemon p. 18):

- *concordance*: percentage of times $y$ is in the 95% percent probability interval of $y^{rep}$ (Gelfand 1996)
- *discrepancy*: $\chi$-squared statistic (BDA)
- *L-criterion*: Distance between $y$ and $y^{rep}$ (Laud Ibrahim 1995)

*Condiational Predictive Ordinate*
The CPO (Gelfant 1996) is the leave-on-out cross-validation predictive density:
$$
p(y_i | y_{-i}) = \int p(y_i | \theta) p(\theta | y_{-i}) d\,\theta
$$
The pointwise predicted LOO probabilitiescan be calculted
using the LOO implemented in the **loo** packae.

**Outliers** inverse-CPOs (ICPOs) larger than 40 are possible outliers, and those higher than 70 are extreme values (Ntzoufras 2009, p. 376). Congdon (2005) scales CPOs by dividing each by its individual max and considers observations with scaled CPOs under 0.01 as outliers.

<!-- The sum of the logged CPOs can be an estimator of the log marginal likelihood and is called the log pseudo marginal likelihood. The ratio of PsMLs can be used as a surrogate for a Bayes Factor (pseudo Bayes Factor) (LaplaceDemon p. 20) -->

**Predictive Concordance and Predictive Quantiles** Gelfand (1996) classifies any $y_i$ that is outside the central 95% predictive posterior of $y^{rep}_i$ is an outlier.
Let the *predictive quantile* ($PQ_i$) be
$$
PQ_i = p(y_i^{(rep)} > y_i) .
$$
Then the *predictive concordance* be the proportion of $y_i$ that are not outliers. Gelfand (1996) argues that the predictive concordance should match 95% - in other words that the posterior preditctive distribution should have the correct coverage. (Laplace Demon p. 20)

## Average Predictive Comparisons

From @GelmanHill [Ch 21.4]
Let $u$ be the input of interest, and $v$ be all other inputs, so that $x = (u, v)$.
$$
b_u(u^{(lo)}, u^{(hi)}, v, \theta) = \frac{E(y | u^{(hi)}, v, \theta) - E(y | u^{(lo)}, v, \theta)}{u^{(hi)} - u^{(lo)}}
$$
the the averge predictive difference per unit change in $u$ is,
$$
B_{u}(u^{(lo)}, u^{(hi)}) = \frac{1}{n} \sum_{i = 1}^n b_u(u^{(lo)}, u^{(hi)}, v_i, \theta) .
$$
This can be adjusted to use observed (weighted) differences of $u$ for each point.
See the Gelman paper on it.

# Priors

Priors for coefficients and scales 

-   Improper uniform priors $(-\infty, +\infty)$ or $(0, +\infty)$.
-   Uninformative Proper Priors such as $\sigma^2 \sim \dinvgamma(0.001, 0.001)$.
-   Weakly Informative Priors
-   Bounded Priors
-   Informative Priors
-   Conjugate priors
-   Hyperparameters - "priors on priors" These are usually specified in models.
    They should be specified to ensure that the posterior is proper and not
    sensitive statistically or computationally to wide tails in the priors.
-   Boundary avoiding priors - In MAP, priors can be specified to keep parameters away from boundaries, e.g. 0 in a scale model.

## Conjugate Priors

In a few cases, the posterior distribution,
$$
p(\theta | y) = \frac{p(y | \theta) p(\theta)}{\int p(y | \theta') p(\theta') d\theta'},
$$
has a [closed-form solution](https://en.wikipedia.org/wiki/Closed-form_expression) and can be calculated exactly.
In those cases, the posterior distribution is calculated exactly, and more costly numerical approximation methods do not need to be used. 
Unfortunately, these cases are few.

Most of those cases involve **conjugate priors**.
In the case of a conjugate prior, the posterior distribution is in the same family as the prior distribution.

Here is a diagram of a few common conjugate priors.[^conjugate]

```{r conjugate-gamma, echo=FALSE}
DiagrammeR::grViz("diagrams/conjugate_gamma.gv")
DiagrammeR::grViz("diagrams/conjugate_beta.gv")
```

[^conjugate]: Based on John Cook's a [Diagram of Conjugate Prior distributions](https://www.johndcook.com/blog/conjugate_prior_diagram/).

The table in the Wikipedia page for [Conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior#cite_note-beta-interp-4) is as complete as any out there. 
@Fink1997a for a compendium of references.
Also see [Distributions] for more information about probability distributions.

### Binomial-Beta

Binomial distribution: If $N \in \Nats$ (number of trials), $\theta \in (0, 1)$ (success probability in each trial), 
then for $n \in \{0, \dots, N\}$,
$$
\dBinom(n | N, \theta) = \binom{N}{n} \theta^{n} (1 - \theta)^{N - n} .
$$

Beta distribution: If $\alpha \in \RealPos$ (shape) and $\beta \in \RealPos$ (shape), then for $\theta \in (0, 1)$,
$$
\dbeta(\theta | \alpha, \beta) = \frac{1}{\mathrm{B}(\alpha, \beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}, 
$$
where $\mathrm{B}$ is the beta function,
$$
\mathrm{B}(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)} .
$$

Then,
$$
\begin{aligned}[t]
p(\theta | \alpha, \beta) &= \dbeta(\theta | \alpha, \beta) && \text{Beta prior} \\
p(y | \theta) &= \dBinom(y | n, \theta)  && \text{Binomial likelihood} \\
p \theta | y, \alpha, \beta) &= \dbeta(\theta | \alpha + y, \beta + n - y) && \text{Beta posterior}
\end{aligned}
$$

### Categorical-Dirichlet

The Dirichlet distribution is a multivariate generalization of the Beta,
If $K \in \N$ and $\alpha \in (\R^{+})^{K}$, then for the $\theta \in K-\text{simplex}$ and $\theta_k > 0$ for all $k$,
$$
\ddirichlet(\theta | \alpha) = \frac{\Gamma(\sum_{k = 1}^K \alpha_k)}{\prod_{k = 1}^K \Gamma(\alpha_k)} \prod_{k = 1}^K \theta_{k}^{\alpha_k - 1}
$$

The multinomial distribution is a generalization of the binomial distribution with $K$ categories instead of 2.

If $K \in \n$, $N \in \N$, and $\theta \in K-\text{simplex}$, then for $y \in \N^{K}$ such that $\sum_{k = 1}^K y_k = N$,
$$
\dmultinom(y | \theta) = \binom{N}{y_1, \dots, y_K} \prod_{k = 1}^{K} \theta_k^{y_k},
$$
where the multinomial coefficient is defined as,
$$
\binom{N}{y_1, \dots, y_K} = \frac{N!}{\prod_{k = 1}^K y_k!}
$$

$$
\begin{aligned}[t]
p(\theta | \alpha) &= \ddirichlet(\theta | \alpha) && \text{Dirichlet prior} \\
p(y | \theta) &= \dmultinom(y | n, \theta)  && \text{Multinomial likelihood} \\
p(\theta | y, \alpha) &= \ddirichlet(\theta | \alpha + y) && \text{Dirichlet posterior}
\end{aligned}
$$


### Poisson-Gamma

Let $\lamba$ be the rate parameter of the Poisson distribution.

If $\lambda \in \R^+$ (rate parameter), then for $n \in \N$,
$$
\dpois(n|\lambda) = \frac{1}{n!} \lambda^n \exp(-\lambda) 
$$
If $\alpha \in \R^{+}$ (shape parameter), $\beta \in \R^{+}$ (inverse scale parameter), then for $y \in \R^{+}$,
$$
\dgamma(y | \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{\alpha - 1} \exp(- \beta y)
$$

Then,
$$
\begin{aligned}[t]
p(\lambda) &= \dgamma(\lambda | \alpha, \beta) \\
p(n | \lambda) &= \dpois(n | \lambda) \\
p(\lambda | n, \alpha, \beta) &= \dgamma(\lambda | \alpha + n, \beta + 1)
\end{aligned}
$$


### Normal with known variance

$$
\begin{aligned}[t]
p(\mu | \mu_0, \sigma_0) &= \dnorm(\mu | \mu, \sigma_0^2) && \text{Normal prior} \\
p(y | \mu) &= \dnorm(y | \mu, \sigma^2)  && \text{Normal likelihood} \\
p(\mu | y, \mu, \sigma^2, \mu_0, \sigma_0^2) &= \dbeta(\mu | \tilde{\mu}, \tilde{\sigma}^2) && \text{Normal posterior} \\
\tilde{\mu} &= \tilde{\sigma}^{2} \left(\frac{\mu_0}{\sigma_0^2} + \frac{y}{\sigma^2} \right) \\
\tilde{\sigma}^2 &= \left(\frac{1}{\sigma_0^2} +\frac{1}{\sigma^2}\right)^{-1} \\
\end{aligned}
$$


### Exponential Family

Likelihood functions in the [exponential family](https://en.wikipedia.org/wiki/Exponential_family) have conjugate priors, often also in the exponential family.[^expconj]

[^expconj]: <https://en.wikipedia.org/wiki/Exponential_family#Bayesian_estimation:_conjugate_distributions>

## Improper Priors

If prior distributions are given an [improper uniform prior](https://en.wikipedia.org/wiki/Prior_probability), $p(\theta) \propto 1$, then the posterior distribution is proportional to the likelihood,
$$
p(\theta | y) \propto p(y | \theta) p(\theta) \propto p(y | \theta)
$$

## References

-   [Stan Wiki](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) and the [rstanarm](https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html) vignette includes comprehensive advice for prior choice recommendations.
-   @Betancourt2017a provides numerical simulation of how the shapes of weakly informative priors affects inferences.
-   @Stan2016a for discussion of some types of priors in regression models
-   @ChungRabe-HeskethDorieEtAl2013a discuss scale priors in penalized MLE models
-   @GelmanJakulinPittauEtAl2008a discusses using Cauchy(0, 2.5) for prior distributions
-   @Gelman2006a provides a prior distribution on variance parameters in hierarchical models.
-   @PolsonScott2012a on using Half-Cauchy priors for scale parameters

<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Updating: A Set of Bayesian Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Updating: A Set of Bayesian Notes">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Updating: A Set of Bayesian Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://jrnold.github.io/bayesian_notes" />
  
  
  <meta name="github-repo" content="jrnold/bayesian_notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Updating: A Set of Bayesian Notes" />
  <meta name="twitter:site" content="@jrnld" />
  
  

<meta name="author" content="Jeffrey B. Arnold">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="shrinkage-regularization.html">
<link rel="next" href="notes.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>

\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\median}{median}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\logistic}{Logistic}
\DeclareMathOperator{\logit}{Logit}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

% This follows BDA
\newcommand{\dunif}{\mathrm{U}}
\newcommand{\dnorm}{\mathrm{N}}
\newcommand{\dlnorm}{\mathrm{lognormal}}
\newcommand{\dmvnorm}{\mathrm{N}}
\newcommand{\dgamma}{\mathrm{Gamma}}
\newcommand{\dinvgamma}{\mathrm{Inv-Gamma}}
\newcommand{\dchisq}[1]{\chi^2_{#1}}
\newcommand{\dinvchisq}[1]{\mathrm{Inv-}\chi^2_{#1}}
\newcommand{\dexp}{\mathrm{Expon}}
\newcommand{\dlaplace}{\mathrm{Laplace}}
\newcommand{\dweibull}{\mathrm{Weibull}}
\newcommand{\dwishart}[1]{\mathrm{Wishart}_{#1}}
\newcommand{\dinvwishart}[1]{\mathrm{Inv-Wishart}_{#1}}
\newcommand{\dlkj}{\mathrm{LkjCorr}}
\newcommand{\dt}[1]{t_{#1}}
\newcommand{\dbeta}{\mathrm{Beta}}
\newcommand{\ddirichlet}{\mathrm{Dirichlet}}
\newcommand{\dlogistic}{\mathrm{Logistic}}
\newcommand{\dllogistic}{\mathrm{Log-logistic}}
\newcommand{\dpois}{\mathrm{Poisson}}
\newcommand{\dbin}{\mathrm{Bin}}
\newcommand{\dmultinom}{\mathrm{Multinom}}
\newcommand{\dnbinom}{\mathrm{Neg-bin}}
\newcommand{\dnbinomalt}{\mathrm{Neg-bin2}}
\newcommand{\dbetabinom}{\mathrm{Beta-bin}}
\newcommand{\dcauchy}{\mathrm{Cauchy}}
\newcommand{\dhalfcauchy}{\mathrm{Cauchy}^{+}}
\newcommand{\dlkjcorr}{\mathrm{LKJ}^{+}}



\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Bayesian Notes</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Theory</b></span></li>
<li class="chapter" data-level="1" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>1</b> Bayesian Inference</a></li>
<li class="part"><span><b>II Computation</b></span></li>
<li class="chapter" data-level="2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>2</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>2.1</b> Monte Carlo Sampling</a></li>
<li class="chapter" data-level="2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo-sampling"><i class="fa fa-check"></i><b>2.2</b> Markov Chain Monte Carlo Sampling</a></li>
<li class="chapter" data-level="2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#references"><i class="fa fa-check"></i><b>2.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>3</b> MCMC Diagnostics</a><ul>
<li class="chapter" data-level="3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#reparameterize-models"><i class="fa fa-check"></i><b>3.1</b> Reparameterize Models</a></li>
<li class="chapter" data-level="3.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#convergence-diagnostics"><i class="fa fa-check"></i><b>3.2</b> Convergence Diagnostics</a><ul>
<li class="chapter" data-level="3.2.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#potential-scale-reduction-hatr"><i class="fa fa-check"></i><b>3.2.1</b> Potential Scale Reduction (<span class="math inline">\(\hat{R}\)</span>)</a></li>
<li class="chapter" data-level="3.2.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#references-1"><i class="fa fa-check"></i><b>3.2.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation-effective-sample-size-and-mcse"><i class="fa fa-check"></i><b>3.3</b> Autocorrelation, Effective Sample Size, and MCSE</a><ul>
<li class="chapter" data-level="3.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>3.3.1</b> Effective Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#thinning"><i class="fa fa-check"></i><b>3.4</b> Thinning</a><ul>
<li class="chapter" data-level="3.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#traceplots"><i class="fa fa-check"></i><b>3.4.1</b> Traceplots</a></li>
<li class="chapter" data-level="3.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#monte-carlo-standard-error-mcse"><i class="fa fa-check"></i><b>3.4.2</b> Monte Carlo Standard Error (MCSE)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#hmc-nut-specific-diagnostics"><i class="fa fa-check"></i><b>3.5</b> HMC-NUT Specific Diagnostics</a><ul>
<li class="chapter" data-level="3.5.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#divergent-transitions"><i class="fa fa-check"></i><b>3.5.1</b> Divergent transitions</a></li>
<li class="chapter" data-level="3.5.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#maximum-treedepth"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Treedepth</a></li>
<li class="chapter" data-level="3.5.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#bayesian-fraction-of-missing-information"><i class="fa fa-check"></i><b>3.5.3</b> Bayesian Fraction of Missing Information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="posterior-inference.html"><a href="posterior-inference.html"><i class="fa fa-check"></i><b>4</b> Posterior Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="posterior-inference.html"><a href="posterior-inference.html#prerequisites"><i class="fa fa-check"></i><b>4.1</b> Prerequisites</a></li>
<li class="chapter" data-level="4.2" data-path="posterior-inference.html"><a href="posterior-inference.html#introduction"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="posterior-inference.html"><a href="posterior-inference.html#functions-of-the-posterior-distribution"><i class="fa fa-check"></i><b>4.3</b> Functions of the Posterior Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="posterior-inference.html"><a href="posterior-inference.html#marginal-effects"><i class="fa fa-check"></i><b>4.4</b> Marginal Effects</a><ul>
<li class="chapter" data-level="4.4.1" data-path="posterior-inference.html"><a href="posterior-inference.html#example-marginal-effect-plot-for-x"><i class="fa fa-check"></i><b>4.4.1</b> Example: Marginal Effect Plot for X</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>5</b> Model Checking</a><ul>
<li class="chapter" data-level="5.1" data-path="model-checking.html"><a href="model-checking.html#why-check-models"><i class="fa fa-check"></i><b>5.1</b> Why check models?</a></li>
<li class="chapter" data-level="5.2" data-path="model-checking.html"><a href="model-checking.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>5.2</b> Posterior Predictive Checks</a><ul>
<li class="chapter" data-level="5.2.1" data-path="model-checking.html"><a href="model-checking.html#bayesian-p-values"><i class="fa fa-check"></i><b>5.2.1</b> Bayesian p-values</a></li>
<li class="chapter" data-level="5.2.2" data-path="model-checking.html"><a href="model-checking.html#test-quantities"><i class="fa fa-check"></i><b>5.2.2</b> Test quantities</a></li>
<li class="chapter" data-level="5.2.3" data-path="model-checking.html"><a href="model-checking.html#p-values-vs.u-values"><i class="fa fa-check"></i><b>5.2.3</b> p-values vs. u-values</a></li>
<li class="chapter" data-level="5.2.4" data-path="model-checking.html"><a href="model-checking.html#marginal-predictive-checks"><i class="fa fa-check"></i><b>5.2.4</b> Marginal predictive checks</a></li>
<li class="chapter" data-level="5.2.5" data-path="model-checking.html"><a href="model-checking.html#outliers"><i class="fa fa-check"></i><b>5.2.5</b> Outliers</a></li>
<li class="chapter" data-level="5.2.6" data-path="model-checking.html"><a href="model-checking.html#grapical-posterior-predictive-checks"><i class="fa fa-check"></i><b>5.2.6</b> Grapical Posterior Predictive Checks</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="model-checking.html"><a href="model-checking.html#sources"><i class="fa fa-check"></i><b>5.3</b> Sources</a></li>
</ul></li>
<li class="part"><span><b>III Models</b></span></li>
<li class="chapter" data-level="6" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Introduction to Stan and Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#prerequites"><i class="fa fa-check"></i><b>6.1</b> Prerequites</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#the-statistical-model"><i class="fa fa-check"></i><b>6.2</b> The Statistical Model</a><ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling"><i class="fa fa-check"></i><b>6.2.1</b> Sampling</a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#convergence-diagnostics-and-model-fit"><i class="fa fa-check"></i><b>6.2.2</b> Convergence Diagnostics and Model Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html"><i class="fa fa-check"></i><b>7</b> Heteroskedasticity and Robust Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#linear-regression-with-student-t-distributed-errors"><i class="fa fa-check"></i><b>7.2</b> Linear Regression with Student t distributed errors</a><ul>
<li class="chapter" data-level="7.2.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#double-exponential-laplace-errors"><i class="fa fa-check"></i><b>7.2.1</b> Double Exponential (Laplace) Errors</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#heteroskedasticity"><i class="fa fa-check"></i><b>7.3</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="7.3.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#covariates"><i class="fa fa-check"></i><b>7.3.1</b> Covariates</a></li>
<li class="chapter" data-level="7.3.2" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#student-t-error"><i class="fa fa-check"></i><b>7.3.2</b> Student-t Error</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#references-2"><i class="fa fa-check"></i><b>7.4</b> References</a><ul>
<li class="chapter" data-level="7.4.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#robust-regression"><i class="fa fa-check"></i><b>7.4.1</b> Robust regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#heteroskedasticity-1"><i class="fa fa-check"></i><b>7.4.2</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="7.4.3" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#qunatile-regression"><i class="fa fa-check"></i><b>7.4.3</b> Qunatile regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>8</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="8.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#generalized-linear-models-1"><i class="fa fa-check"></i><b>8.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="8.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-models"><i class="fa fa-check"></i><b>8.2</b> Count Models</a><ul>
<li class="chapter" data-level="8.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson"><i class="fa fa-check"></i><b>8.2.1</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example"><i class="fa fa-check"></i><b>8.3</b> Example</a></li>
<li class="chapter" data-level="8.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#negative-binomial"><i class="fa fa-check"></i><b>8.4</b> Negative Binomial</a><ul>
<li class="chapter" data-level="8.4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-3"><i class="fa fa-check"></i><b>8.4.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#multinomial-categorical-models"><i class="fa fa-check"></i><b>8.5</b> Multinomial / Categorical Models</a></li>
<li class="chapter" data-level="8.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#gamma-regression"><i class="fa fa-check"></i><b>8.6</b> Gamma Regression</a></li>
<li class="chapter" data-level="8.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#beta-regression"><i class="fa fa-check"></i><b>8.7</b> Beta Regression</a></li>
<li class="chapter" data-level="8.8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#ordered-logistic"><i class="fa fa-check"></i><b>8.8</b> Ordered Logistic</a></li>
<li class="chapter" data-level="8.9" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-4"><i class="fa fa-check"></i><b>8.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="binomial-models.html"><a href="binomial-models.html"><i class="fa fa-check"></i><b>9</b> Binomial Models</a><ul>
<li class="chapter" data-level="9.0.1" data-path="binomial-models.html"><a href="binomial-models.html#link-functions-link-function"><i class="fa fa-check"></i><b>9.0.1</b> Link Functions {link-function}</a></li>
<li class="chapter" data-level="9.0.2" data-path="binomial-models.html"><a href="binomial-models.html#stan"><i class="fa fa-check"></i><b>9.0.2</b> Stan</a></li>
<li class="chapter" data-level="9.0.3" data-path="binomial-models.html"><a href="binomial-models.html#example-vote-turnout"><i class="fa fa-check"></i><b>9.0.3</b> Example: Vote Turnout</a></li>
<li class="chapter" data-level="9.0.4" data-path="binomial-models.html"><a href="binomial-models.html#separation"><i class="fa fa-check"></i><b>9.0.4</b> Separation</a></li>
<li class="chapter" data-level="9.1" data-path="binomial-models.html"><a href="binomial-models.html#rare-events-logit"><i class="fa fa-check"></i><b>9.1</b> Rare Events Logit</a></li>
<li class="chapter" data-level="9.2" data-path="binomial-models.html"><a href="binomial-models.html#case-control"><i class="fa fa-check"></i><b>9.2</b> Case Control</a><ul>
<li class="chapter" data-level="9.2.1" data-path="binomial-models.html"><a href="binomial-models.html#references-6"><i class="fa fa-check"></i><b>9.2.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html"><i class="fa fa-check"></i><b>10</b> Unbounded Count Models</a><ul>
<li class="chapter" data-level="10.1" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#poisson-1"><i class="fa fa-check"></i><b>10.1</b> Poisson</a></li>
<li class="chapter" data-level="10.2" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#negative-binomial-1"><i class="fa fa-check"></i><b>10.2</b> Negative Binomial</a><ul>
<li class="chapter" data-level="10.2.1" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#stan-1"><i class="fa fa-check"></i><b>10.2.1</b> Stan</a></li>
<li class="chapter" data-level="10.2.2" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#example-number-of-number-o"><i class="fa fa-check"></i><b>10.2.2</b> Example: Number of Number o</a></li>
<li class="chapter" data-level="10.2.3" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#references-7"><i class="fa fa-check"></i><b>10.2.3</b> References</a></li>
<li class="chapter" data-level="10.2.4" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#link-functions"><i class="fa fa-check"></i><b>10.2.4</b> Link functions</a></li>
<li class="chapter" data-level="10.2.5" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#stan-2"><i class="fa fa-check"></i><b>10.2.5</b> Stan</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#example-bilateral-sanctions"><i class="fa fa-check"></i><b>10.3</b> Example: Bilateral Sanctions</a></li>
<li class="chapter" data-level="10.4" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#negative-binomial-2"><i class="fa fa-check"></i><b>10.4</b> Negative Binomial</a><ul>
<li class="chapter" data-level="10.4.1" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#example-economic-sanctions-ii-ex-econ-sanctions-2"><i class="fa fa-check"></i><b>10.4.1</b> Example: Economic Sanctions II {ex-econ-sanctions-2}</a></li>
<li class="chapter" data-level="10.4.2" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#references-8"><i class="fa fa-check"></i><b>10.4.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="categorical-variables.html"><a href="categorical-variables.html"><i class="fa fa-check"></i><b>11</b> Categorical Variables</a><ul>
<li class="chapter" data-level="11.1" data-path="categorical-variables.html"><a href="categorical-variables.html#example-mexico-vote-choice"><i class="fa fa-check"></i><b>11.1</b> Example: Mexico Vote Choice</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ordered-categorical-outcomes.html"><a href="ordered-categorical-outcomes.html"><i class="fa fa-check"></i><b>12</b> Ordered Categorical Outcomes</a></li>
<li class="chapter" data-level="13" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html"><i class="fa fa-check"></i><b>13</b> Shrinkage and Regularization</a><ul>
<li class="chapter" data-level="13.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#normal-linear-regression-model"><i class="fa fa-check"></i><b>13.1</b> Normal Linear Regression Model</a></li>
<li class="chapter" data-level="13.2" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#penalized-regression"><i class="fa fa-check"></i><b>13.2</b> Penalized Regression</a><ul>
<li class="chapter" data-level="13.2.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#ridge-regression"><i class="fa fa-check"></i><b>13.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="13.2.2" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#lasso"><i class="fa fa-check"></i><b>13.2.2</b> Lasso</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#bayesian-shrinkage-priors"><i class="fa fa-check"></i><b>13.3</b> Bayesian Shrinkage Priors</a></li>
<li class="chapter" data-level="13.4" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#differences-between-bayesian-shrinkage-and-penalized-likelihood"><i class="fa fa-check"></i><b>13.4</b> Differences between Bayesian Shrinkage and Penalized Likelihood</a></li>
<li class="chapter" data-level="13.5" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#hierarchical-shrinkage-priors"><i class="fa fa-check"></i><b>13.5</b> Hierarchical Shrinkage Priors</a></li>
<li class="chapter" data-level="13.6" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#example-1"><i class="fa fa-check"></i><b>13.6</b> Example</a><ul>
<li class="chapter" data-level="13.6.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#double-exponential-laplace-prior"><i class="fa fa-check"></i><b>13.6.1</b> Double Exponential (Laplace) Prior</a></li>
<li class="chapter" data-level="13.6.2" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#hierarchical-prior-hs"><i class="fa fa-check"></i><b>13.6.2</b> Hierarchical Prior (HS)</a></li>
<li class="chapter" data-level="13.6.3" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#comparison"><i class="fa fa-check"></i><b>13.6.3</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#shrinkage-parameters"><i class="fa fa-check"></i><b>13.7</b> Shrinkage Parameters</a></li>
<li class="chapter" data-level="13.8" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#choice-of-hyperparameter-on-tau"><i class="fa fa-check"></i><b>13.8</b> Choice of Hyperparameter on <span class="math inline">\(\tau\)</span></a></li>
<li class="chapter" data-level="13.9" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#r-implementations"><i class="fa fa-check"></i><b>13.9</b> R Implementations</a></li>
<li class="chapter" data-level="13.10" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>13.10</b> Bayesian Model Averaging</a><ul>
<li class="chapter" data-level="13.10.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#zellners-g-prior"><i class="fa fa-check"></i><b>13.10.1</b> Zellner’s g-prior</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#slab-and-spike-priors"><i class="fa fa-check"></i><b>13.11</b> Slab and Spike Priors</a></li>
<li class="chapter" data-level="13.12" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#technical-notes"><i class="fa fa-check"></i><b>13.12</b> Technical Notes</a></li>
<li class="chapter" data-level="13.13" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#multiple-comparisons-and-thresholding-rules"><i class="fa fa-check"></i><b>13.13</b> Multiple Comparisons and Thresholding rules</a></li>
<li class="chapter" data-level="13.14" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#examples-of-applications-of-sensitivity-analysis"><i class="fa fa-check"></i><b>13.14</b> Examples of Applications of Sensitivity Analysis</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hierarchical-models.html"><a href="hierarchical-models.html"><i class="fa fa-check"></i><b>14</b> Hierarchical Models</a><ul>
<li class="chapter" data-level="14.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#baseball-hitting"><i class="fa fa-check"></i><b>14.1</b> Baseball Hitting</a><ul>
<li class="chapter" data-level="14.1.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#other-examples"><i class="fa fa-check"></i><b>14.1.1</b> Other Examples</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="hierarchical-models.html"><a href="hierarchical-models.html#equivalent-models"><i class="fa fa-check"></i><b>14.2</b> Equivalent Models</a><ul>
<li class="chapter" data-level="14.2.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#group-varying-intercepts"><i class="fa fa-check"></i><b>14.2.1</b> Group Varying Intercepts</a></li>
<li class="chapter" data-level="14.2.2" data-path="hierarchical-models.html"><a href="hierarchical-models.html#separate-local-regressions"><i class="fa fa-check"></i><b>14.2.2</b> Separate local regressions</a></li>
<li class="chapter" data-level="14.2.3" data-path="hierarchical-models.html"><a href="hierarchical-models.html#modeling-the-coefficients-of-a-large-regression-model"><i class="fa fa-check"></i><b>14.2.3</b> Modeling the coefficients of a large regression model</a></li>
<li class="chapter" data-level="14.2.4" data-path="hierarchical-models.html"><a href="hierarchical-models.html#regression-with-multiple-error-terms"><i class="fa fa-check"></i><b>14.2.4</b> Regression with multiple error terms</a></li>
<li class="chapter" data-level="14.2.5" data-path="hierarchical-models.html"><a href="hierarchical-models.html#regresion-with-correlated-errors"><i class="fa fa-check"></i><b>14.2.5</b> Regresion with correlated errors</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Appendix</b></span><ul>
<li class="chapter" data-level="14.3" data-path="hierarchical-models.html"><a href="hierarchical-models.html#miscellaneous-mathematical-background"><i class="fa fa-check"></i><b>14.3</b> Miscellaneous Mathematical Background</a><ul>
<li class="chapter" data-level="14.3.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#location-scale-families"><i class="fa fa-check"></i><b>14.3.1</b> Location-Scale Families</a></li>
<li class="chapter" data-level="14.3.2" data-path="hierarchical-models.html"><a href="hierarchical-models.html#scale-mixtures-of-normal-distributions"><i class="fa fa-check"></i><b>14.3.2</b> Scale Mixtures of Normal Distributions</a></li>
<li class="chapter" data-level="14.3.3" data-path="hierarchical-models.html"><a href="hierarchical-models.html#covariance-correlation-matrix-decomposition"><i class="fa fa-check"></i><b>14.3.3</b> Covariance-Correlation Matrix Decomposition</a></li>
<li class="chapter" data-level="14.3.4" data-path="hierarchical-models.html"><a href="hierarchical-models.html#qr-factorization"><i class="fa fa-check"></i><b>14.3.4</b> QR Factorization</a></li>
<li class="chapter" data-level="14.3.5" data-path="hierarchical-models.html"><a href="hierarchical-models.html#cholesky-decomposition"><i class="fa fa-check"></i><b>14.3.5</b> Cholesky Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>15</b> Notes</a><ul>
<li class="chapter" data-level="15.1" data-path="notes.html"><a href="notes.html#syllabi"><i class="fa fa-check"></i><b>15.1</b> Syllabi</a></li>
<li class="chapter" data-level="15.2" data-path="notes.html"><a href="notes.html#textbooks"><i class="fa fa-check"></i><b>15.2</b> Textbooks</a></li>
<li class="chapter" data-level="15.3" data-path="notes.html"><a href="notes.html#topics"><i class="fa fa-check"></i><b>15.3</b> Topics</a><ul>
<li class="chapter" data-level="15.3.1" data-path="notes.html"><a href="notes.html#overviews"><i class="fa fa-check"></i><b>15.3.1</b> Overviews</a></li>
<li class="chapter" data-level="15.3.2" data-path="notes.html"><a href="notes.html#bayesian-philosophy"><i class="fa fa-check"></i><b>15.3.2</b> Bayesian Philosophy</a></li>
<li class="chapter" data-level="15.3.3" data-path="notes.html"><a href="notes.html#bayesian-frequentist-debates"><i class="fa fa-check"></i><b>15.3.3</b> Bayesian Frequentist Debates</a></li>
<li class="chapter" data-level="15.3.4" data-path="notes.html"><a href="notes.html#categorical"><i class="fa fa-check"></i><b>15.3.4</b> Categorical</a></li>
<li class="chapter" data-level="15.3.5" data-path="notes.html"><a href="notes.html#identifiability"><i class="fa fa-check"></i><b>15.3.5</b> Identifiability</a></li>
<li class="chapter" data-level="15.3.6" data-path="notes.html"><a href="notes.html#time-series"><i class="fa fa-check"></i><b>15.3.6</b> Time Series</a></li>
<li class="chapter" data-level="15.3.7" data-path="notes.html"><a href="notes.html#topic-models"><i class="fa fa-check"></i><b>15.3.7</b> Topic Models</a></li>
<li class="chapter" data-level="15.3.8" data-path="notes.html"><a href="notes.html#nonparametric-bayesian-methods"><i class="fa fa-check"></i><b>15.3.8</b> Nonparametric Bayesian Methods</a></li>
<li class="chapter" data-level="15.3.9" data-path="notes.html"><a href="notes.html#prior-elicitation"><i class="fa fa-check"></i><b>15.3.9</b> Prior Elicitation</a></li>
<li class="chapter" data-level="15.3.10" data-path="notes.html"><a href="notes.html#variable-selection"><i class="fa fa-check"></i><b>15.3.10</b> Variable Selection</a></li>
<li class="chapter" data-level="15.3.11" data-path="notes.html"><a href="notes.html#shrinkage"><i class="fa fa-check"></i><b>15.3.11</b> Shrinkage</a></li>
<li class="chapter" data-level="15.3.12" data-path="notes.html"><a href="notes.html#applied-bayes-rule"><i class="fa fa-check"></i><b>15.3.12</b> Applied Bayes Rule</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="notes.html"><a href="notes.html#computation-methods"><i class="fa fa-check"></i><b>15.4</b> Computation Methods</a><ul>
<li class="chapter" data-level="15.4.1" data-path="notes.html"><a href="notes.html#software"><i class="fa fa-check"></i><b>15.4.1</b> Software</a></li>
<li class="chapter" data-level="15.4.2" data-path="notes.html"><a href="notes.html#stan-3"><i class="fa fa-check"></i><b>15.4.2</b> Stan</a></li>
<li class="chapter" data-level="15.4.3" data-path="notes.html"><a href="notes.html#diagrams"><i class="fa fa-check"></i><b>15.4.3</b> Diagrams</a></li>
<li class="chapter" data-level="15.4.4" data-path="notes.html"><a href="notes.html#political-science-bayesian-works"><i class="fa fa-check"></i><b>15.4.4</b> Political Science Bayesian Works</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="notes.html"><a href="notes.html#model-checking-1"><i class="fa fa-check"></i><b>15.5</b> Model Checking</a></li>
<li class="chapter" data-level="15.6" data-path="notes.html"><a href="notes.html#general-applications-and-models"><i class="fa fa-check"></i><b>15.6</b> General Applications and Models</a><ul>
<li class="chapter" data-level="15.6.1" data-path="notes.html"><a href="notes.html#mixed-methods-and-qualitative-research"><i class="fa fa-check"></i><b>15.6.1</b> Mixed Methods and Qualitative Research</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="notes.html"><a href="notes.html#hierarchical-modeling"><i class="fa fa-check"></i><b>15.7</b> Hierarchical Modeling</a></li>
<li class="chapter" data-level="15.8" data-path="notes.html"><a href="notes.html#shrinkageregularization"><i class="fa fa-check"></i><b>15.8</b> Shrinkage/Regularization</a><ul>
<li class="chapter" data-level="15.8.1" data-path="notes.html"><a href="notes.html#examples"><i class="fa fa-check"></i><b>15.8.1</b> Examples</a></li>
<li class="chapter" data-level="15.8.2" data-path="notes.html"><a href="notes.html#latent-variable-models"><i class="fa fa-check"></i><b>15.8.2</b> Latent Variable Models</a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="notes.html"><a href="notes.html#bayes-theorem-examples"><i class="fa fa-check"></i><b>15.9</b> Bayes Theorem Examples</a><ul>
<li class="chapter" data-level="15.9.1" data-path="notes.html"><a href="notes.html#miscallaneous"><i class="fa fa-check"></i><b>15.9.1</b> Miscallaneous</a></li>
<li class="chapter" data-level="15.9.2" data-path="notes.html"><a href="notes.html#german-tank-problem"><i class="fa fa-check"></i><b>15.9.2</b> German Tank Problem</a></li>
</ul></li>
<li class="chapter" data-level="15.10" data-path="notes.html"><a href="notes.html#good-turing-estimator"><i class="fa fa-check"></i><b>15.10</b> Good-Turing Estimator</a></li>
<li class="chapter" data-level="15.11" data-path="notes.html"><a href="notes.html#reproducibility"><i class="fa fa-check"></i><b>15.11</b> Reproducibility</a><ul>
<li class="chapter" data-level="15.11.1" data-path="notes.html"><a href="notes.html#uncategorized"><i class="fa fa-check"></i><b>15.11.1</b> Uncategorized</a></li>
</ul></li>
<li class="chapter" data-level="15.12" data-path="notes.html"><a href="notes.html#empirical-bayes"><i class="fa fa-check"></i><b>15.12</b> Empirical Bayes</a></li>
<li class="chapter" data-level="15.13" data-path="notes.html"><a href="notes.html#things-to-cover"><i class="fa fa-check"></i><b>15.13</b> Things to cover</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-9.html"><a href="references-9.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Updating: A Set of Bayesian Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hierarchical-models" class="section level1">
<h1><span class="header-section-number">14</span> Hierarchical Models</h1>
<ul>
<li><em>Hierarchical models:</em> often groups of parameters, <span class="math inline">\(\{\theta_1, \dots, \theta_J\}\)</span>, are related.</li>
<li>E.g. countries, states, counties, years, etc. Even the regression coefficients, <span class="math inline">\(\beta_1, \dots, \beta_k\)</span> seen the in the <a href="shrinkage-regularization.html#shrinkage-regularization">Shrinkage and Regularization</a> chapter.</li>
<li>We can treat those <span class="math inline">\(\theta_j\)</span> as drawn from a <em>population distribution</em>, <span class="math inline">\(\theta_j \sim p(\theta)\)</span>.</li>
<li>The prior distribution <span class="math inline">\(p(\theta)\)</span> is called a <em>hyperprior</em> and its parameters are <em>hyperparameters</em></li>
</ul>
<p><em>Exchangeability:</em></p>
<ul>
<li>parameters <span class="math inline">\((\theta_1, \dots, \theta_J)\)</span> are <em>exchangeable</em> if <span class="math inline">\(p(\theta_1, \dots, \theta_J)\)</span> don’t depend on the indexes.</li>
<li>i.i.d. models are a special case of exchangeability.</li>
</ul>
<div id="baseball-hitting" class="section level2">
<h2><span class="header-section-number">14.1</span> Baseball Hitting</h2>
<p><span class="citation">Efron and Morris (1975)</span> analyzed data from 18 players in the 1970 season. The goal was to predict the batting average of these 18 players from their first 45 at-bats for the remainder of the 1970 season.</p>
<p>The following example is based on <span class="citation">Carpenter, Gabry, and Goodrich (2017)</span> and the <strong><a href="https://cran.r-project.org/package=rstanarm">rstanarm</a></strong> vignette <a href="https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html">Hierarchical Partial Pooling for Repeated Binary Trials</a>.</p>
<p>The hitting data used in <span class="citation">Efron and Morris (1975)</span> is included in <strong><a href="https://cran.r-project.org/package=rstanarm">rstanarm</a></strong> as <a href="https://www.rdocumentation.org/packages/rstanarm/topics/bball1970">rstanarm</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;bball1970&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;rstanarm&quot;</span>)
bball1970 &lt;-
<span class="st">  </span><span class="kw">mutate</span>(bball1970,
         <span class="dt">BatAvg1 =</span> Hits <span class="op">/</span><span class="st"> </span>AB,
         <span class="dt">BatAvg2 =</span> RemainingHits <span class="op">/</span><span class="st"> </span>RemainingAB)
bball1970
<span class="co">#&gt;        Player AB Hits RemainingAB RemainingHits BatAvg1 BatAvg2</span>
<span class="co">#&gt; 1    Clemente 45   18         367           127   0.400   0.346</span>
<span class="co">#&gt; 2    Robinson 45   17         426           127   0.378   0.298</span>
<span class="co">#&gt; 3      Howard 45   16         521           144   0.356   0.276</span>
<span class="co">#&gt; 4   Johnstone 45   15         275            61   0.333   0.222</span>
<span class="co">#&gt; 5       Berry 45   14         418           114   0.311   0.273</span>
<span class="co">#&gt; 6     Spencer 45   14         466           126   0.311   0.270</span>
<span class="co">#&gt; 7   Kessinger 45   13         586           155   0.289   0.265</span>
<span class="co">#&gt; 8    Alvarado 45   12         138            29   0.267   0.210</span>
<span class="co">#&gt; 9       Santo 45   11         510           137   0.244   0.269</span>
<span class="co">#&gt; 10    Swaboda 45   11         200            46   0.244   0.230</span>
<span class="co">#&gt; 11 Petrocelli 45   10         538           142   0.222   0.264</span>
<span class="co">#&gt; 12  Rodriguez 45   10         186            42   0.222   0.226</span>
<span class="co">#&gt; 13      Scott 45   10         435           132   0.222   0.303</span>
<span class="co">#&gt; 14      Unser 45   10         277            73   0.222   0.264</span>
<span class="co">#&gt; 15   Williams 45   10         591           195   0.222   0.330</span>
<span class="co">#&gt; 16 Campaneris 45    9         558           159   0.200   0.285</span>
<span class="co">#&gt; 17     Munson 45    8         408           129   0.178   0.316</span>
<span class="co">#&gt; 18      Alvis 45    7          70            14   0.156   0.200</span></code></pre></div>
<p>Let <span class="math inline">\(y_i\)</span> be the number of hits in the first 45 at bats for player <span class="math inline">\(i\)</span>, <span class="math display">\[
\begin{aligned}[t]
y_i &amp; \sim \dbin(45, \mu_i),
\end{aligned}
\]</span> where <span class="math inline">\(\mu_i \in (0, 1)\)</span> is the player-specific batting average. Priors will be placed on the log-odds parameter, <span class="math inline">\(\eta \in \R\)</span>, <span class="math display">\[
\begin{aligned}[t]
\mu_i &amp;\sim \frac{1}{1 + \exp(-\eta_i)} . \\
\end{aligned}
\]</span></p>
<p>This example considers three ways of modeling <span class="math inline">\(\mu_i\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Complete Pooling:</strong> All players have the same batting average parameter. <span class="math display">\[
\eta_i = \eta .
\]</span> The common (log-odds) batting average is given a weakly informative prior, <span class="math display">\[
\eta \sim \dnorm(0, 2.5)
\]</span> On the log odds scale, this places 95% of the probability mass between 0.7 and 99.3 on the proportion scale.</p></li>
<li><p><strong>Non-pooled:</strong> Each players (log-odds) batting average is independent, with each assigned a separate weak prior. <span class="math display">\[
\begin{aligned}[t]
\eta_i &amp;\sim \dnorm(0, 2.5)
\end{aligned}
\]</span></p></li>
<li><p><strong>Partial-pooling:</strong> Each player has a separate (log-odds) batting average, but these batting average parameters are drawn from a common normal distribution. <span class="math display">\[
\begin{aligned}[t]
\eta_i &amp;\sim \dnorm(0, \tau) \\
\tau &amp;\sim \dnorm(0, 1)
\end{aligned}
\]</span></p></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bball1970_data &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">N =</span> <span class="kw">nrow</span>(bball1970),
  <span class="dt">k =</span> bball1970<span class="op">$</span>AB,
  <span class="dt">y =</span> bball1970<span class="op">$</span>Hits,
  <span class="dt">k_new =</span> bball1970<span class="op">$</span>RemainingAB,
  <span class="dt">y_new =</span> bball1970<span class="op">$</span>RemainingHits
)</code></pre></div>
<p>Create a list to store models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">list</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[[<span class="st">&quot;nopool&quot;</span>]] &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&quot;stan/binomial-no-pooling.stan&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[[<span class="st">&quot;nopool&quot;</span>]]</code></pre></div>
<pre>
  <code class="stan">/* Binomial Model (No pooling)

  A binomial model for $i = 1, \dots, N$, no pooling:
  $$
  p(y_i | n_i, \mu_i) &\sim \mathsf{Binomial}(y_i | n_i, \mu_i) \\
  \mu_i &= \logit^{-1}(\eta_i) \\
  p(\eta_i) &\sim \mathsf{Normal}^+(0, 10)
  $$

*/
data {
  int N;
  int y[N];
  int k[N];
  // new data
  int y_new[N];
  int k_new[N];
}
parameters {
  vector[N] eta;
}
model {
  eta ~ normal(0., 10.);
  y ~ binomial_logit(k, eta);
}
generated quantities {
  int y_rep[N];
  vector[N] log_lik;
  vector[N] log_lik_new;
  vector<lower = 0., upper = 1.>[N] mu;
  mu = inv_logit(eta);
  for (n in 1:N) {
    y_rep[n] = binomial_rng(k[n], mu[n]);
    log_lik[n] = binomial_logit_lpmf(y[n] | k[n], eta[n]);
    log_lik_new[n] = binomial_logit_lpmf(y_new[n] | k_new[n], eta[n]);
  }
}</code>
</pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[[<span class="st">&quot;pool&quot;</span>]] &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&quot;stan/binomial-complete-pooling.stan&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[[<span class="st">&quot;pool&quot;</span>]]</code></pre></div>
<pre>
  <code class="stan">/* Binomial Model

  A binomial model for $i = 1, \dots, N$, with complete pooling
  $$
  \begin{aligned}[t]
  p(y_i | n_i, \mu) &\sim \mathsf{Binomial}(n_i, \mu) \\
  \mu &= \logit^{-1}(\eta) \\
  p(\eta) &\sim \mathsf{Normal}^+(0, 10)
  \end{aligned}
  $$

*/
data {
  int N;
  int y[N];
  int k[N];
  // new data
  int y_new[N];
  int k_new[N];
}
parameters {
  real eta;
}
model {
  eta ~ normal(0., 10.);
  y ~ binomial_logit(k, eta);
}
generated quantities {
  int y_rep[N];
  vector[N] log_lik;
  vector[N] log_lik_new;
  real<lower = 0., upper = 1.> mu;
  mu = inv_logit(eta);
  for (n in 1:N) { //
    y_rep[n] = binomial_rng(k[n], mu);
    log_lik[n] = binomial_logit_lpmf(y[n] | k[n], eta);
    log_lik_new[n] = binomial_logit_lpmf(y_new[n] | k_new[n], eta);
  }
}</code>
</pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[[<span class="st">&quot;partial&quot;</span>]] &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&quot;stan/binomial-partial-pooling-t.stan&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[[<span class="st">&quot;partial&quot;</span>]]</code></pre></div>
<pre>
  <code class="stan">/* Binomial Model

  A binomial model for $i = 1, \dots, N$, with partial pooling
  $$
  \begin{aligned}[t]
  p(y_i | n_i, \mu_i) &\sim \mathsf{Binomial}(y_i | n_i, \mu_i) \\
  \mu_i &= \logit^{-1}(\eta_i) \\
  p(\eta_i | \tau) &\sim \mathsf{Normal}(alpha, \tau) \\
  p(\tau) &\sim \mathsf{Normal}^+(0, 1) \\
  p(alpha) & \sim \mathsf{Normal}(0, 2.5) \\
  \end{aligned}
  $$

*/
data {
  int N;
  int y[N];
  int k[N];
  // new data
  int y_new[N];
  int k_new[N];
}
parameters {
  vector[N] eta;
  real alpha;
  real<lower = 0.> tau;
}
model {
  alpha ~ normal(0., 10.);
  tau ~ normal(0., 1);
  eta ~ student_t(4., alpha, tau);
  y ~ binomial_logit(k, eta);
}
generated quantities {
  int y_rep[N];
  vector[N] log_lik;
  vector[N] log_lik_new;
  vector<lower = 0., upper = 1.>[N] mu;
  mu = inv_logit(eta);
  for (n in 1:N) { //
    y_rep[n] = binomial_rng(k[n], mu[n]);
    log_lik[n] = binomial_logit_lpmf(y[n] | k[n], eta[n]);
    log_lik_new[n] = binomial_logit_lpmf(y_new[n] | k_new[n], eta[n]);
  }
}</code>
</pre>
<p>Sample from all three models a</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fits &lt;-<span class="st"> </span><span class="kw">map</span>(models, sampling, <span class="dt">data =</span> bball1970_data,
            <span class="dt">refresh =</span> <span class="op">-</span><span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">set_names</span>(<span class="kw">names</span>(models))
<span class="co">#&gt; The following numerical problems occurred the indicated number of times on chain 1</span>
<span class="co">#&gt;                                                                                     count</span>
<span class="co">#&gt; Exception thrown at line 31: student_t_lpdf: Scale parameter is 0, but must be &gt; 0!     1</span>
<span class="co">#&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected.</span>
<span class="co">#&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</span>
<span class="co">#&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users.</span>
<span class="co">#&gt; Warning: There were 5 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See</span>
<span class="co">#&gt; http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup</span>
<span class="co">#&gt; Warning: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See</span>
<span class="co">#&gt; http://mc-stan.org/misc/warnings.html#bfmi-low</span>
<span class="co">#&gt; Warning: Examine the pairs() plot to diagnose sampling problems</span></code></pre></div>
<p>For each model calculate the posterior mean of <span class="math inline">\(\mu\)</span> for each player:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bball1970 &lt;-
<span class="st">  </span><span class="kw">map2_df</span>(<span class="kw">names</span>(fits), fits, 
     <span class="cf">function</span>(nm, fit) {
      mu &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>(fit) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="kw">str_detect</span>(term, <span class="st">&quot;^mu&quot;</span>))
      <span class="cf">if</span> (<span class="kw">nrow</span>(mu) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {
        out &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">estimate =</span> <span class="kw">rep</span>(mu<span class="op">$</span>estimate, 18L))
      } <span class="cf">else</span> {
        out &lt;-<span class="st"> </span><span class="kw">select</span>(mu, estimate)
      }
      out<span class="op">$</span>model &lt;-<span class="st"> </span>nm
      out<span class="op">$</span>.id &lt;-<span class="st"> </span><span class="kw">seq_len</span>(<span class="kw">nrow</span>(out))
      out
     }) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread</span>(model, estimate) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(bball1970)</code></pre></div>
<p>The partially pooled estiamtes are shrunk towards the overall average, and are between the no-pooling and pooled estimates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">select</span>(bball1970,
       Player, nopool, partial, pool) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Player =</span> <span class="kw">factor</span>(Player, <span class="dt">levels =</span> Player)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(variable, value, <span class="op">-</span>Player) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> value, <span class="dt">x =</span> <span class="kw">factor</span>(variable), <span class="dt">group =</span> Player)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="kw">expression</span>(mu))</code></pre></div>
<p><img src="hierarchical_files/figure-html/unnamed-chunk-13-1.png" width="70%" style="display: block; margin: auto;" /> We can plot the actual batting averages (<code>BatAvg1</code> and <code>BatAvg2</code>) and the model estimates:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">select</span>(bball1970,
       Player, nopool, partial, pool, BatAvg1, BatAvg2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Player =</span> <span class="kw">factor</span>(Player, <span class="dt">levels =</span> Player)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(variable, value, <span class="op">-</span>Player) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> Player, <span class="dt">x =</span> value, <span class="dt">colour =</span> variable)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(mu), <span class="dt">y =</span> <span class="st">&quot;&quot;</span>)</code></pre></div>
<p><img src="hierarchical_files/figure-html/unnamed-chunk-14-1.png" width="70%" style="display: block; margin: auto;" /> The estimates of the no-pooling model is almost exactly the same as <code>BatAvg1</code>. The out-of-sample batting averages <code>BatAvg2</code> show regression to the mean.</p>
<p>For these models, compare the overall out-of-sample performance by calculating the actual average out-of-sample log-pointwise predictive density (lppd), and the expected lppd using LOO-PSIS. The LOO-PSIS estimates of the out-of-sample lppd are optimistic. However, they still show the pooling and partial estimates as superior to the no-pooling estimates. The actual out-of-sample average lppd for the partial pooled model is the best fitting.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">map2_df</span>(<span class="kw">names</span>(fits), fits, 
     <span class="cf">function</span>(nm, fit) {
      loo &lt;-<span class="st"> </span><span class="kw">loo</span>(<span class="kw">extract_log_lik</span>(fit, <span class="st">&quot;log_lik&quot;</span>))
      ll_new &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">extract</span>(fit)[[<span class="st">&quot;log_lik_new&quot;</span>]]
      <span class="kw">tibble</span>(<span class="dt">model =</span> nm,
             <span class="dt">loo =</span> loo<span class="op">$</span>elpd_loo <span class="op">/</span><span class="st"> </span>bball1970_data<span class="op">$</span>N,
             <span class="dt">ll_out =</span> <span class="kw">mean</span>(<span class="kw">log</span>(<span class="kw">colMeans</span>(<span class="kw">exp</span>(ll_new)))))
     })
<span class="co">#&gt; # A tibble: 3 x 3</span>
<span class="co">#&gt;     model   loo ll_out</span>
<span class="co">#&gt;     &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;</span>
<span class="co">#&gt; 1  nopool -3.20  -4.60</span>
<span class="co">#&gt; 2    pool -2.58  -4.06</span>
<span class="co">#&gt; 3 partial -2.59  -4.01</span></code></pre></div>
<p>To see why this is the case, plot the average errors for each observation in- and out-of-sample. In-sample for the no-pooling model is zero, but it over-estimates (under-estimates) the players with the highest (lowest) batting averages in their first 45 at bats—this is regression to the mean. In sample, the partially pooling model shrinks the estimates towards the mean and reducing error. Out of sample, the errors of the partially pooled model are not much different than the no-pooling model, except that the extreme observations have lower errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">select</span>(bball1970,
       Player, nopool, partial, pool, BatAvg1, BatAvg2) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Player =</span> <span class="kw">as.integer</span>(<span class="kw">factor</span>(Player, <span class="dt">levels =</span> Player))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(variable, value, <span class="op">-</span>Player, <span class="op">-</span><span class="kw">matches</span>(<span class="st">&quot;BatAvg&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">In-sample Errors</span><span class="st">`</span> =<span class="st"> </span>value <span class="op">-</span><span class="st"> </span>BatAvg1,
         <span class="st">`</span><span class="dt">Out-of-sample Errors</span><span class="st">`</span> =<span class="st"> </span>value <span class="op">-</span><span class="st"> </span>BatAvg2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="kw">matches</span>(<span class="st">&quot;BatAvg&quot;</span>), <span class="op">-</span>value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(sample, error, <span class="op">-</span>variable, <span class="op">-</span>Player) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> error, <span class="dt">x =</span> Player, <span class="dt">colour =</span> variable)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">colour =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>sample, <span class="dt">ncol =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="hierarchical_files/figure-html/unnamed-chunk-16-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Extensions:</p>
<ul>
<li>Redo this analysis with the <a href="https://www.rdocumentation.org/packages/rstanarm/topics/bball2006">rstanarm</a> dataset with hits and at-bats for the entire 2006 AL season of MLB.</li>
<li>Use a beta distribution for the prior of <span class="math inline">\(\mu_i\)</span>. How would you specify the prior beta distribution so that it is uniformative?</li>
<li>If you used the beta distribution, how would you specify the beta distribution as a function of the mean?</li>
<li>The lowest batting average of the modern era is approximately 0.16 and the highest is approximately 0.4. Use this information for an informative prior distribuiton.</li>
<li>There may be some truly exceptional players. Model this by replacing the normal prior for <span class="math inline">\(\eta\)</span> with a wide tailed distribution.</li>
<li>The distribution of batting averages may be asymmetric - since there may be a few great players, but a player can only be so bad before they are relegated to the minor league. Find a skewed distribution to use as a prior.</li>
</ul>
<p>References:</p>
<ul>
<li>Albert, Jim. <a href="https://baseballwithr.wordpress.com/2016/02/15/revisiting-efron-and-morriss-baseball-study/">Revisiting Efron and Morris’s Baseball Study</a> Feb 15, 2016</li>
<li>Bob Carpenter. <a href="https://lingpipe-blog.com/2009/11/04/hierarchicalbayesian-batting-ability-with-multiple-comparisons/">Hierarchical Bayesian Batting Ability, with Multiple Comparisons</a>. November 4, 2009.</li>
<li>John Kruschke. <a href="http://doingbayesiandataanalysis.blogspot.com/2012/11/shrinkage-in-multi-level-hierarchical.html">Shrinkage in multi-level hierarchical models</a>. November 27, 2012.</li>
<li>See <span class="citation">Jensen, McShane, and Wyner (2009)</span> for an updated hierarchical model of baseball hitting</li>
</ul>
<div id="other-examples" class="section level3">
<h3><span class="header-section-number">14.1.1</span> Other Examples</h3>
<ul>
<li>Rat Tumors - BDA</li>
<li>Eight Schools - BDA</li>
</ul>
</div>
</div>
<div id="equivalent-models" class="section level2">
<h2><span class="header-section-number">14.2</span> Equivalent Models</h2>
<div id="group-varying-intercepts" class="section level3">
<h3><span class="header-section-number">14.2.1</span> Group Varying Intercepts</h3>
<p>This is a regression, with a different intercept per group: <span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_j[i] + \beta x_i, \sigma_y^2) \\
\end{aligned}
\]</span> The second level model of the group intercepts models them as distributed around a common mean, <span class="math inline">\(\mu_\alpha\)</span>, with error: <span class="math display">\[
\alpha_j = \mu_\alpha + \eta_j \\
\eta_j \sim \dnorm(0, \sigma_\alpha^2)
\]</span></p>
</div>
<div id="separate-local-regressions" class="section level3">
<h3><span class="header-section-number">14.2.2</span> Separate local regressions</h3>
<p>For each group, run a regression, <span class="math display">\[
\begin{aligned}[t]
y_i \sim \dnorm(\alpha_j + \beta x_i, \sigma_y^2) &amp; \text{for all $i$ in group $j$}
\end{aligned}
\]</span> And now model the group-level means, <span class="math display">\[
\begin{aligned}[t]
\alpha &amp;= \dgamma u_j + \eta_j \\
\eta_j &amp;\sim \dnorm(0, \sigma^2_\alpha)
\end{aligned}
\]</span></p>
</div>
<div id="modeling-the-coefficients-of-a-large-regression-model" class="section level3">
<h3><span class="header-section-number">14.2.3</span> Modeling the coefficients of a large regression model</h3>
<p>Suppose that <span class="math inline">\(X\)</span> includes all predictors and <span class="math inline">\(J\)</span> indicators for the <span class="math inline">\(J\)</span> groups.</p>
<p>We could also put the constant in the second distribution. The coefficients <span class="math inline">\(\beta\)</span> for the coefficients on the group indicators are centered around a <span class="math inline">\(\mu_\alpha\)</span>. <span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(x_i \beta, \sigma_y^2) \\
\beta_j &amp;\sim \dnorm(\mu_{\alpha}, \sigma_{\alpha}^2)
\end{aligned}
\]</span></p>
</div>
<div id="regression-with-multiple-error-terms" class="section level3">
<h3><span class="header-section-number">14.2.4</span> Regression with multiple error terms</h3>
<p><span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(x_i \beta + \eta_{j[i]}, \sigma_y^2) \\
\eta_j &amp;\sim \dnorm(0, \sigma_{\alpha}^2)
\end{aligned}
\]</span></p>
</div>
<div id="regresion-with-correlated-errors" class="section level3">
<h3><span class="header-section-number">14.2.5</span> Regresion with correlated errors</h3>
<p><span class="math display">\[
\begin{aligned}[t]
y_i &amp;= X_i \beta + \omega_i, &amp; \omega &amp;\sim \dnorm(0, \Sigma)
\end{aligned}
\]</span> The errors have an <span class="math inline">\(n \times n\)</span> covariance matrix, and are equivalent to the sum of individual and group errors. <span class="math display">\[
\omega_i = \eta_{j[i]} + \epsilon_i
\]</span></p>
<p>The variances and covariances in <span class="math inline">\(\Sigma\)</span> are:</p>
<ul>
<li>for unit <span class="math inline">\(i\)</span>: <span class="math inline">\(\Sigma_{ii} = \var(\omega_i) = \sigma_y^2 + \sigma_{\alpha}^2\)</span></li>
<li>for units <span class="math inline">\(i\)</span>, <span class="math inline">\(k\)</span> in same group <span class="math inline">\(j\)</span>: <span class="math inline">\(\Sigma_{ik} = \cov(\omega_i, \omega_k) = \sigma_{\alpha}^2\)</span></li>
<li>for units <span class="math inline">\(i\)</span>, <span class="math inline">\(k\)</span> in the same group <span class="math inline">\(j\)</span>: <span class="math inline">\(\Sigma_{ik} = 0\)</span></li>
</ul>

</div>
</div>
</div>



<span class="kw">library</span>(<span class="st">&quot;stringr&quot;</span>)</code></pre></div>
<div id="miscellaneous-mathematical-background" class="section level2">
<h2><span class="header-section-number">14.3</span> Miscellaneous Mathematical Background</h2>
<div id="location-scale-families" class="section level3">
<h3><span class="header-section-number">14.3.1</span> Location-Scale Families</h3>
<p>In a <a href="https://en.wikipedia.org/wiki/Location%E2%80%93scale_family">location-scale family</a> of distributions, if the random variable <span class="math inline">\(X\)</span> is distributed with mean 0 and standard deviation 1, then the random variable <span class="math inline">\(Y\)</span>, <span class="math display">\[
Y = \mu + \sigma X ,
\]</span> has mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p><strong>Normal distribution:</strong> Suppose <span class="math inline">\(X \sim \dnorm(0, 1)\)</span>, then <span class="math display">\[
Y = \mu + \sigma X,
\]</span> is equivalent to <span class="math inline">\(Y \sim \dnorm(\mu, \sigma)\)</span> (normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>).</p>
<p>** Student-t distribution** (including Cauchy): <span class="math display">\[
\begin{aligned}[t]
X &amp;\sim \dt{\nu}(0, 1) \\
Y &amp;= \mu + \sigma X 
\end{aligned}
\]</span> implies <span class="math display">\[
Y \sim \dt{\nu}(\mu, \sigma),
\]</span> i.e. <span class="math inline">\(Y\)</span> is distributed Student-<span class="math inline">\(t\)</span> with location <span class="math inline">\(\mu\)</span> and scale <span class="math inline">\(\sigma\)</span>.</p>
<p>In Stan, it can be useful parameterize distributions in terms of a mean 0, scale 1 parameters, and separate parameters for the locations and scales. E.g. with normal distributions,</p>
<pre><code>parameters {
  real mu;
  real&lt;lower = 0.0&gt; sigma;
  vector[n] eps;
}
transformed parameters {
  vector[n] y;
  y = mu + sigma * eps;
}
model {
  eps ~ normal(0.0, 1.0);
}</code></pre>
</div>
<div id="scale-mixtures-of-normal-distributions" class="section level3">
<h3><span class="header-section-number">14.3.2</span> Scale Mixtures of Normal Distributions</h3>
<p>Some commonly used distributions can be represented as scale mixtures of normal distributions. For formal details of scale mixtures of normal distributions see <span class="citation">West (1987)</span>. Distributions that are scale-mixtures of normal distributions can be written as, <span class="math display">\[
Y \sim \dnorm(\mu, \sigma_i^2) \\
\sigma_i \sim \pi(\sigma_i)
\]</span> As its name suggests, the individual variances (scales) themselves, have a distribution.</p>
<p>Some examples:</p>
<ul>
<li>Student-t</li>
<li>Double Exponential</li>
<li>Horseshoe or Hierarchical Shrinkage (HS)</li>
<li>Horseshoe Plus or Hierarchical Shrinkage Plus (HS+)</li>
</ul>
<p>Even when analytic forms of the distribution are available, representing them as scale mixtures of normal distributions may be convenient in modeling. In particular, it may allow for drawing samples from the distribution easily. And in HMC, it may induce a more tractable posterior density.</p>
</div>
<div id="covariance-correlation-matrix-decomposition" class="section level3">
<h3><span class="header-section-number">14.3.3</span> Covariance-Correlation Matrix Decomposition</h3>
<p>The suggested method for modeling covariance matrices in Stan is the separation strategy which decomposes a covariance matrix <span class="math inline">\(\Sigma\)</span> can be decomposed into a standard deviation vector <span class="math inline">\(\sigma\)</span>, and a correlation matrix <span class="math inline">\(R\)</span> <span class="citation">(Barnard, McCulloch, and Meng 2000)</span>, <span class="math display">\[
\Sigma = \diag(\sigma) R \diag(\sigma) .
\]</span> This is useful for setting priors on covariance because separate priors can be set for the scales of the variables via <span class="math inline">\(\sigma\)</span>, and the correlation between them, via <span class="math inline">\(R\)</span>.</p>
<p>The <a href="https://github.com/stan-dev/rstanarm/wiki/Prior-distributions">rstanarm</a> <code>decov</code> prior goes further and decomposes the covariance matrix into a correlation matrix, <span class="math inline">\(\mat{R}\)</span>, a diagonal variance matrix <span class="math inline">\(\mat{\Omega}\)</span> with trace <span class="math inline">\(n \sigma^2\)</span>, a scalar global variance <span class="math inline">\(\sigma^2\)</span>, and a simplex <span class="math inline">\(\vec{\pi}\)</span> (proportion of total variance for each variable): <span class="math display">\[
\begin{aligned}[t]
\mat{\Sigma} &amp;= \mat{\Omega} \mat{R}  \\
\diag(\mat{\Omega}) &amp;= n \vec{\pi} \sigma^2
\end{aligned}
\]</span> Separate and interpretable priors can be put on <span class="math inline">\(\mat{R}\)</span>, <span class="math inline">\(\vec{\pi}\)</span>, and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The LKJ (Lewandowski, ) distribution is a distribution over correlation coefficients, <span class="math display">\[
R \sim \dlkjcorr(\eta) ,
\]</span> where <span class="math display">\[
\dlkjcorr(\Sigma | \eta) \propto \det(\Sigma)^{(\eta - 1)} .
\]</span></p>
<p>This distribution has the following properties:</p>
<ul>
<li><span class="math inline">\(\eta = 1\)</span>: uniform correlations</li>
<li><span class="math inline">\(\eta \to \infty\)</span>: approaches the identity matrix</li>
<li><span class="math inline">\(0 &lt; \eta &lt; 1\)</span>: there is a trough at the identity matrix with higher probabilities placed on non-zero correlations.</li>
<li>For all positive <span class="math inline">\(\eta\)</span> (<span class="math inline">\(\eta &gt; 0\)</span>), <span class="math inline">\(\E(R) = \mat{I}\)</span>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lkjcorr_df &lt;-<span class="st"> </span><span class="cf">function</span>(eta, <span class="dt">n =</span> <span class="dv">2</span>) {
  out &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">rlkjcorr</span>(n, eta))
  out<span class="op">$</span>.row &lt;-<span class="st"> </span><span class="kw">seq_len</span>(<span class="kw">nrow</span>(out))
  out &lt;-<span class="st"> </span><span class="kw">gather</span>(out, .col, value, <span class="op">-</span>.row)
  out<span class="op">$</span>.col &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">str_replace</span>(out<span class="op">$</span>.col, <span class="st">&quot;^V&quot;</span>, <span class="st">&quot;&quot;</span>))
  out<span class="op">$</span>eta &lt;-<span class="st"> </span>eta
  out  
}

lkjsims &lt;-<span class="st"> </span>purrr<span class="op">::</span><span class="kw">map_df</span>(<span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">50</span>, <span class="dv">1000</span>), lkjcorr_df, <span class="dt">n =</span> <span class="dv">50</span>)</code></pre></div>
<p>This simulates a single matrix from the LKJ distribution with different values of <span class="math inline">\(\eta\)</span>. As <span class="math inline">\(\eta \to \infty\)</span>, the off-diagonal correlations tend towards 0, and the correlation matrix to the identity matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(lkjsims,
       <span class="kw">aes</span>(<span class="dt">x =</span> .row, <span class="dt">y =</span> .col, <span class="dt">fill =</span> value)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>eta, <span class="dt">ncol =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_fill_distiller</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">type =</span> <span class="st">&quot;div&quot;</span>, <span class="dt">palette =</span> <span class="st">&quot;RdYlBu&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.grid =</span> <span class="kw">element_blank</span>(), <span class="dt">axis.text =</span> <span class="kw">element_blank</span>()) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;&quot;</span>)</code></pre></div>
<p><img src="appendix_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The density of the off-diagonal correlations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lkjsims <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(.row <span class="op">&lt;</span><span class="st"> </span>.col) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">colour =</span> <span class="kw">factor</span>(eta))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>()</code></pre></div>
<p><img src="appendix_files/figure-html/unnamed-chunk-5-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>For other discussions of the LKJ correlation distribution, see these:</p>
<ul>
<li><a href="https://stats.stackexchange.com/questions/2746/how-to-efficiently-generate-random-positive-semidefinite-correlation-matrices/125017#125017" class="uri">https://stats.stackexchange.com/questions/2746/how-to-efficiently-generate-random-positive-semidefinite-correlation-matrices/125017#125017</a></li>
<li><a href="http://www.zinkov.com/posts/2015-06-09-where-priors-come-from/" class="uri">http://www.zinkov.com/posts/2015-06-09-where-priors-come-from/</a></li>
<li><a href="http://www.psychstatistics.com/2014/12/27/d-lkj-priors/" class="uri">http://www.psychstatistics.com/2014/12/27/d-lkj-priors/</a></li>
</ul>
</div>
<div id="qr-factorization" class="section level3">
<h3><span class="header-section-number">14.3.4</span> QR Factorization</h3>
<p>For a full-rank <span class="math inline">\(N \times K\)</span> matrix, the QR factorization is <span class="math display">\[
\mat{X} = \mat{Q} \mat{R} 
\]</span> where <span class="math inline">\(\mat{Q}\)</span> is an orthonormal matrix such that <span class="math inline">\(\mat{Q}\T \mat{Q}\)</span> and <span class="math inline">\(\mat{R}\)</span> is an upper triangular matrix.</p>
<p>Stan function <span class="citation">Team (2016)</span> suggest writing it is <span class="math display">\[
\begin{aligned}[t]
\mat{Q}^* = \mat{Q} \times \sqrt{N - 1} \\
\mat{R}^* = \frac{1}{\sqrt{N - 1}} \mat{R}
\end{aligned}
\]</span></p>
<p>This is used for solving linear model.</p>
<p>Suppose <span class="math inline">\(\vec{\beta}\)</span> is a <span class="math inline">\(K \times 1\)</span> vector, then <span class="math display">\[
\vec{eta} = \mat{x} \vec{\beta} = \mat{Q} \mat{R} \vec{\beta} = \mat{Q}^* \mat{R}^* \vec{\beta} .
\]</span> Suppose <span class="math inline">\(\mat{theta} = \mat{R}^* \vec{\beta}\)</span>, then <span class="math inline">\(\vec{eta} = \mat{Q}^* \mat{\theta}\)</span> and <span class="math inline">\(\vec{beta} = {\mat{R}^*}^{-1} \mat{\theta}\)</span>.</p>
<p><a href="https://cran.r-project.org/web/packages/rstanarm/vignettes/lm.html">rstanarm</a> provides a prior for a normal linear model which uses the QR decomposition to parameterize a prior in terms of <span class="math inline">\(R^2\)</span>.</p>
<p>Stan functions:</p>
<ul>
<li><code>qr_Q(matrix A)</code></li>
<li><code>qr_R(matrix A)</code></li>
</ul>
<p>See <span class="citation">Team (2016 Sec 8.2)</span></p>
</div>
<div id="cholesky-decomposition" class="section level3">
<h3><span class="header-section-number">14.3.5</span> Cholesky Decomposition</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a> of a positive definite matrix <span class="math inline">\(A\)</span> is, <span class="math display">\[
\mat{A} = \mat{L} \mat{L}\T ,
\]</span> where <span class="math inline">\(\mat{L}\)</span> is a lower-triangular matrix.</p>
<ul>
<li>It is similar to a square root for a matrix.</li>
<li><p>It often more numerically stable or efficient to work with the Cholesky decomposition, than with a covariance matrix. When working with the covariance matrix, numerical precision can result in a non positive definite matrix. However, working with <span class="math inline">\(\mat{L}\)</span> will ensure that <span class="math inline">\(\mat{A} = \mat{L} \mat{L}\T\)</span> will be positive definite.</p></li>
<li><p>In Stan</p>
<ul>
<li>Types types <code>cholesky_factor_cov</code>, and <code>cholesky_factor_corr</code> represent the Cholesky factor of covariance and correlation matrices, respectively.</li>
<li>Cholesky decomposition function is <code>cholesky_decompose(matrix A)</code></li>
</ul></li>
<li><p>Multiple functions in Stan are parameterized with Cholesky decompositions instead of or in addition to covariance matrices. Use them if possible; they are more numerically stable.</p>
<ul>
<li><code>lkj_corr_chol_lpdf</code></li>
<li><code>multi_normal_cholesky_lpdf</code></li>
</ul></li>
</ul>
<p>The Cholesky factor is used for sampling from a multivariate normal distribution using i.i.d. standard normal distributions. Suppose <span class="math inline">\(X_1, \dots, X_N\)</span> are <span class="math inline">\(N\)</span> i.i.d. standard normal distributions, <span class="math inline">\(\mat{\Omega}\)</span> is an <span class="math inline">\(N \times N\)</span> lower-triangular matrix such that <span class="math inline">\(\mat{\Omega} \mat{Omega}\T = \mat{\Sigma}\)</span>, and <span class="math inline">\(\mu\)</span> is an <span class="math inline">\(N \times 1\)</span> vector, then <span class="math display">\[
\vec{\mu} + \mat{\Omega} X \sim \dnorm(\vec{\mu}, \mat{\Sigma})
\]</span></p>
<p>See <span class="citation">Team (2016, 40, 147, 241, 246)</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shrinkage-regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="notes.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/bayesian_notes/edit/master/hierarchical.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

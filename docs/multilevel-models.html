<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>20 Multilevel Models | Updating: A Set of Bayesian Notes</title>
  <meta name="description" content="20 Multilevel Models | Updating: A Set of Bayesian Notes">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="20 Multilevel Models | Updating: A Set of Bayesian Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://jrnold.github.io/bayesian_notes" />
  
  
  <meta name="github-repo" content="jrnold/bayesian_notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="20 Multilevel Models | Updating: A Set of Bayesian Notes" />
  <meta name="twitter:site" content="@jrnld" />
  
  

<meta name="author" content="Jeffrey B. Arnold">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="shrinkage-and-regularized-regression.html">
<link rel="next" href="appendix.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<script src="libs/grViz-binding-1.0.0/grViz.js"></script>



<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Bayesian Notes</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>1</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-analysis"><i class="fa fa-check"></i><b>1.1</b> Bayesian Analysis</a></li>
<li class="chapter" data-level="1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>1.2</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="part"><span><b>I Theory</b></span></li>
<li class="chapter" data-level="2" data-path="bayes-theorem.html"><a href="bayes-theorem.html"><i class="fa fa-check"></i><b>2</b> Bayes Theorem</a><ul>
<li class="chapter" data-level="" data-path="bayes-theorem.html"><a href="bayes-theorem.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="2.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#introduction-to-bayes-theorem"><i class="fa fa-check"></i><b>2.1</b> Introduction to Bayes’ Theorem</a></li>
<li class="chapter" data-level="2.2" data-path="bayes-theorem.html"><a href="bayes-theorem.html#examples"><i class="fa fa-check"></i><b>2.2</b> Examples</a><ul>
<li class="chapter" data-level="2.2.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#taxi-cab-problem"><i class="fa fa-check"></i><b>2.2.1</b> Taxi-Cab Problem</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html#why-most-research-findings-are-false"><i class="fa fa-check"></i><b>2.3</b> Why most research findings are false</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#questions"><i class="fa fa-check"></i><b>2.3.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="bayes-theorem.html"><a href="bayes-theorem.html#measurement-error-and-rare-events-in-surveys"><i class="fa fa-check"></i><b>2.4</b> Measurement Error and Rare Events in Surveys</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html"><i class="fa fa-check"></i><b>3</b> Example: Predicting Names from Ages</a><ul>
<li class="chapter" data-level="" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#prerequisites-1"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="3.1" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#statement-of-the-problem"><i class="fa fa-check"></i><b>3.1</b> Statement of the problem</a></li>
<li class="chapter" data-level="3.2" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#data-wrangling"><i class="fa fa-check"></i><b>3.2</b> Data Wrangling</a></li>
<li class="chapter" data-level="3.3" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#probability-of-age-given-name-and-sex"><i class="fa fa-check"></i><b>3.3</b> Probability of age given name and sex</a><ul>
<li class="chapter" data-level="3.3.1" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#questions-1"><i class="fa fa-check"></i><b>3.3.1</b> Questions</a></li>
<li class="chapter" data-level="3.3.2" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#references"><i class="fa fa-check"></i><b>3.3.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>4</b> Naive Bayes</a><ul>
<li class="chapter" data-level="" data-path="naive-bayes.html"><a href="naive-bayes.html#prerequisites-2"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="4.1" data-path="naive-bayes.html"><a href="naive-bayes.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="naive-bayes.html"><a href="naive-bayes.html#examples-1"><i class="fa fa-check"></i><b>4.2</b> Examples</a><ul>
<li class="chapter" data-level="4.2.1" data-path="naive-bayes.html"><a href="naive-bayes.html#federalist-papers"><i class="fa fa-check"></i><b>4.2.1</b> Federalist Papers</a></li>
<li class="chapter" data-level="4.2.2" data-path="naive-bayes.html"><a href="naive-bayes.html#extensions"><i class="fa fa-check"></i><b>4.2.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="naive-bayes.html"><a href="naive-bayes.html#details"><i class="fa fa-check"></i><b>4.3</b> Details</a><ul>
<li class="chapter" data-level="4.3.1" data-path="naive-bayes.html"><a href="naive-bayes.html#generative-vs.-discriminative-models"><i class="fa fa-check"></i><b>4.3.1</b> Generative vs. Discriminative Models</a></li>
<li class="chapter" data-level="4.3.2" data-path="naive-bayes.html"><a href="naive-bayes.html#estimation"><i class="fa fa-check"></i><b>4.3.2</b> Estimation</a></li>
<li class="chapter" data-level="4.3.3" data-path="naive-bayes.html"><a href="naive-bayes.html#prediction"><i class="fa fa-check"></i><b>4.3.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="naive-bayes.html"><a href="naive-bayes.html#references-1"><i class="fa fa-check"></i><b>4.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="priors.html"><a href="priors.html"><i class="fa fa-check"></i><b>5</b> Priors</a><ul>
<li class="chapter" data-level="5.1" data-path="priors.html"><a href="priors.html#levels-of-priors"><i class="fa fa-check"></i><b>5.1</b> Levels of Priors</a></li>
<li class="chapter" data-level="5.2" data-path="priors.html"><a href="priors.html#conjugate-priors"><i class="fa fa-check"></i><b>5.2</b> Conjugate Priors</a><ul>
<li class="chapter" data-level="5.2.1" data-path="priors.html"><a href="priors.html#binomial-beta"><i class="fa fa-check"></i><b>5.2.1</b> Binomial-Beta</a></li>
<li class="chapter" data-level="5.2.2" data-path="priors.html"><a href="priors.html#categorical-dirichlet"><i class="fa fa-check"></i><b>5.2.2</b> Categorical-Dirichlet</a></li>
<li class="chapter" data-level="5.2.3" data-path="priors.html"><a href="priors.html#poisson-gamma"><i class="fa fa-check"></i><b>5.2.3</b> Poisson-Gamma</a></li>
<li class="chapter" data-level="5.2.4" data-path="priors.html"><a href="priors.html#normal-with-known-variance"><i class="fa fa-check"></i><b>5.2.4</b> Normal with known variance</a></li>
<li class="chapter" data-level="5.2.5" data-path="priors.html"><a href="priors.html#exponential-family"><i class="fa fa-check"></i><b>5.2.5</b> Exponential Family</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="priors.html"><a href="priors.html#improper-priors"><i class="fa fa-check"></i><b>5.3</b> Improper Priors</a></li>
<li class="chapter" data-level="5.4" data-path="priors.html"><a href="priors.html#cromwells-rule"><i class="fa fa-check"></i><b>5.4</b> Cromwell’s Rule</a></li>
<li class="chapter" data-level="5.5" data-path="priors.html"><a href="priors.html#asymptotics"><i class="fa fa-check"></i><b>5.5</b> Asymptotics</a></li>
<li class="chapter" data-level="5.6" data-path="priors.html"><a href="priors.html#proper-and-improper-priors"><i class="fa fa-check"></i><b>5.6</b> Proper and Improper Priors</a></li>
<li class="chapter" data-level="5.7" data-path="priors.html"><a href="priors.html#hyperpriors-and-hyperparameters"><i class="fa fa-check"></i><b>5.7</b> Hyperpriors and Hyperparameters</a></li>
<li class="chapter" data-level="5.8" data-path="priors.html"><a href="priors.html#references-2"><i class="fa fa-check"></i><b>5.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>6</b> Estimation</a><ul>
<li class="chapter" data-level="6.1" data-path="estimation-1.html"><a href="estimation-1.html#point-estimates"><i class="fa fa-check"></i><b>6.1</b> Point Estimates</a></li>
<li class="chapter" data-level="6.2" data-path="estimation-1.html"><a href="estimation-1.html#credible-intervals"><i class="fa fa-check"></i><b>6.2</b> Credible Intervals</a><ul>
<li class="chapter" data-level="6.2.1" data-path="estimation-1.html"><a href="estimation-1.html#compared-to-confidence-intervals"><i class="fa fa-check"></i><b>6.2.1</b> Compared to confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimation-1.html"><a href="estimation-1.html#bayesian-decision-theory"><i class="fa fa-check"></i><b>6.3</b> Bayesian Decision Theory</a></li>
</ul></li>
<li class="part"><span><b>II Computation</b></span></li>
<li class="chapter" data-level="7" data-path="bayesian-computation.html"><a href="bayesian-computation.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#how-to-calculate-a-posterior"><i class="fa fa-check"></i><b>7.1</b> How to calculate a posterior?</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#example-globe-tossing-model"><i class="fa fa-check"></i><b>7.2</b> Example: Globe-tossing model</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#quadrature"><i class="fa fa-check"></i><b>7.3</b> Quadrature</a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#grid-approximation"><i class="fa fa-check"></i><b>7.3.1</b> Grid approximation</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian-computation.html"><a href="bayesian-computation.html#functional-approximations"><i class="fa fa-check"></i><b>7.4</b> Functional Approximations</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#maximum-a-posteriori"><i class="fa fa-check"></i><b>7.4.1</b> Maximum A Posteriori</a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#laplace-approximation"><i class="fa fa-check"></i><b>7.4.2</b> Laplace Approximation</a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#variational-inference"><i class="fa fa-check"></i><b>7.4.3</b> Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian-computation.html"><a href="bayesian-computation.html#sampling-methods"><i class="fa fa-check"></i><b>7.5</b> Sampling Methods</a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#numerical-integration"><i class="fa fa-check"></i><b>7.5.1</b> Numerical Integration</a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>7.5.2</b> Inverse transform sampling</a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#direct-approximation"><i class="fa fa-check"></i><b>7.5.3</b> Direct approximation</a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian-computation.html"><a href="bayesian-computation.html#rejection-sampling"><i class="fa fa-check"></i><b>7.5.4</b> Rejection sampling</a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian-computation.html"><a href="bayesian-computation.html#importance-sampling"><i class="fa fa-check"></i><b>7.5.5</b> Importance Sampling</a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian-computation.html"><a href="bayesian-computation.html#mcmc-methods"><i class="fa fa-check"></i><b>7.5.6</b> MCMC Methods</a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian-computation.html"><a href="bayesian-computation.html#discarding-early-iterations"><i class="fa fa-check"></i><b>7.5.7</b> Discarding early iterations</a></li>
<li class="chapter" data-level="7.5.8" data-path="bayesian-computation.html"><a href="bayesian-computation.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>7.5.8</b> Monte Carlo Sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>8</b> MCMC Diagnostics</a><ul>
<li class="chapter" data-level="" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#prerequisites-3"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="8.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#reparameterize-models"><i class="fa fa-check"></i><b>8.1</b> Reparameterize Models</a></li>
<li class="chapter" data-level="8.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#convergence-diagnostics"><i class="fa fa-check"></i><b>8.2</b> Convergence Diagnostics</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#potential-scale-reduction-hatr"><i class="fa fa-check"></i><b>8.2.1</b> Potential Scale Reduction (<span class="math inline">\(\hat{R}\)</span>)</a></li>
<li class="chapter" data-level="8.2.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#references-3"><i class="fa fa-check"></i><b>8.2.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation-effective-sample-size-and-mcse"><i class="fa fa-check"></i><b>8.3</b> Autocorrelation, Effective Sample Size, and MCSE</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>8.3.1</b> Effective Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#thinning"><i class="fa fa-check"></i><b>8.4</b> Thinning</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#traceplots"><i class="fa fa-check"></i><b>8.4.1</b> Traceplots</a></li>
<li class="chapter" data-level="8.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#monte-carlo-standard-error-mcse"><i class="fa fa-check"></i><b>8.4.2</b> Monte Carlo Standard Error (MCSE)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#hmc-nut-specific-diagnostics"><i class="fa fa-check"></i><b>8.5</b> HMC-NUT Specific Diagnostics</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#divergent-transitions"><i class="fa fa-check"></i><b>8.5.1</b> Divergent transitions</a></li>
<li class="chapter" data-level="8.5.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#maximum-tree-depth"><i class="fa fa-check"></i><b>8.5.2</b> Maximum Tree-depth</a></li>
<li class="chapter" data-level="8.5.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#bayesian-fraction-of-missing-information"><i class="fa fa-check"></i><b>8.5.3</b> Bayesian Fraction of Missing Information</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#debugging-bayesian-computing"><i class="fa fa-check"></i><b>8.6</b> Debugging Bayesian Computing</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>9</b> Model Checking</a><ul>
<li class="chapter" data-level="9.1" data-path="model-checking.html"><a href="model-checking.html#why-check-models"><i class="fa fa-check"></i><b>9.1</b> Why check models?</a></li>
<li class="chapter" data-level="9.2" data-path="model-checking.html"><a href="model-checking.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>9.2</b> Posterior Predictive Checks</a><ul>
<li class="chapter" data-level="9.2.1" data-path="model-checking.html"><a href="model-checking.html#bayesian-p-values"><i class="fa fa-check"></i><b>9.2.1</b> Bayesian p-values</a></li>
<li class="chapter" data-level="9.2.2" data-path="model-checking.html"><a href="model-checking.html#test-quantities"><i class="fa fa-check"></i><b>9.2.2</b> Test quantities</a></li>
<li class="chapter" data-level="9.2.3" data-path="model-checking.html"><a href="model-checking.html#p-values-vs.-u-values"><i class="fa fa-check"></i><b>9.2.3</b> p-values vs. u-values</a></li>
<li class="chapter" data-level="9.2.4" data-path="model-checking.html"><a href="model-checking.html#marginal-predictive-checks"><i class="fa fa-check"></i><b>9.2.4</b> Marginal predictive checks</a></li>
<li class="chapter" data-level="9.2.5" data-path="model-checking.html"><a href="model-checking.html#outliers"><i class="fa fa-check"></i><b>9.2.5</b> Outliers</a></li>
<li class="chapter" data-level="9.2.6" data-path="model-checking.html"><a href="model-checking.html#graphical-posterior-predictive-checks"><i class="fa fa-check"></i><b>9.2.6</b> Graphical Posterior Predictive Checks</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="model-checking.html"><a href="model-checking.html#references-4"><i class="fa fa-check"></i><b>9.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>10</b> Model Comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="model-comparison.html"><a href="model-comparison.html#models"><i class="fa fa-check"></i><b>10.1</b> Models</a></li>
<li class="chapter" data-level="10.2" data-path="model-comparison.html"><a href="model-comparison.html#classes-of-model-spaces"><i class="fa fa-check"></i><b>10.2</b> Classes of Model Spaces</a></li>
<li class="chapter" data-level="10.3" data-path="model-comparison.html"><a href="model-comparison.html#continuous-model-expansion"><i class="fa fa-check"></i><b>10.3</b> Continuous model expansion</a></li>
<li class="chapter" data-level="10.4" data-path="model-comparison.html"><a href="model-comparison.html#discrete-model-expansion"><i class="fa fa-check"></i><b>10.4</b> Discrete Model Expansion</a></li>
<li class="chapter" data-level="10.5" data-path="model-comparison.html"><a href="model-comparison.html#out-of-sample-predictive-accuracy"><i class="fa fa-check"></i><b>10.5</b> Out-of-sample predictive accuracy</a></li>
<li class="chapter" data-level="10.6" data-path="model-comparison.html"><a href="model-comparison.html#stacking"><i class="fa fa-check"></i><b>10.6</b> Stacking</a></li>
<li class="chapter" data-level="10.7" data-path="model-comparison.html"><a href="model-comparison.html#posterior-predictive-criteria"><i class="fa fa-check"></i><b>10.7</b> Posterior Predictive Criteria</a><ul>
<li class="chapter" data-level="10.7.1" data-path="model-comparison.html"><a href="model-comparison.html#summary-and-advice"><i class="fa fa-check"></i><b>10.7.1</b> Summary and Advice</a></li>
<li class="chapter" data-level="10.7.2" data-path="model-comparison.html"><a href="model-comparison.html#expected-log-predictive-density"><i class="fa fa-check"></i><b>10.7.2</b> Expected Log Predictive Density</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="model-comparison.html"><a href="model-comparison.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>10.8</b> Bayesian Model Averaging</a></li>
<li class="chapter" data-level="10.9" data-path="model-comparison.html"><a href="model-comparison.html#pseudo-bma"><i class="fa fa-check"></i><b>10.9</b> Pseudo-BMA</a></li>
<li class="chapter" data-level="10.10" data-path="model-comparison.html"><a href="model-comparison.html#loo-cv-via-importance-sampling"><i class="fa fa-check"></i><b>10.10</b> LOO-CV via importance sampling</a></li>
<li class="chapter" data-level="10.11" data-path="model-comparison.html"><a href="model-comparison.html#selection-induced-bias"><i class="fa fa-check"></i><b>10.11</b> Selection induced Bias</a></li>
</ul></li>
<li class="part"><span><b>III Models</b></span></li>
<li class="chapter" data-level="11" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Introduction to Stan and Linear Regression</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#prerequisites-4"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="11.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#ols-and-mle-linear-regression"><i class="fa fa-check"></i><b>11.1</b> OLS and MLE Linear Regression</a><ul>
<li class="chapter" data-level="11.1.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#bayesian-model-with-improper-priors"><i class="fa fa-check"></i><b>11.1.1</b> Bayesian Model with Improper priors</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#stan-model"><i class="fa fa-check"></i><b>11.2</b> Stan Model</a></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling-model-with-stan"><i class="fa fa-check"></i><b>11.3</b> Sampling Model with Stan</a><ul>
<li class="chapter" data-level="11.3.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling"><i class="fa fa-check"></i><b>11.3.1</b> Sampling</a></li>
<li class="chapter" data-level="11.3.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#convergence-diagnostics-and-model-fit"><i class="fa fa-check"></i><b>11.3.2</b> Convergence Diagnostics and Model Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#prerequisites-5"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="12.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#introduction-1"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-models"><i class="fa fa-check"></i><b>12.2</b> Count Models</a><ul>
<li class="chapter" data-level="12.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson"><i class="fa fa-check"></i><b>12.2.1</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example-3"><i class="fa fa-check"></i><b>12.3</b> Example</a></li>
<li class="chapter" data-level="12.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#negative-binomial"><i class="fa fa-check"></i><b>12.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="12.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#multinomial-categorical-models"><i class="fa fa-check"></i><b>12.5</b> Multinomial / Categorical Models</a></li>
<li class="chapter" data-level="12.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#gamma-regression"><i class="fa fa-check"></i><b>12.6</b> Gamma Regression</a></li>
<li class="chapter" data-level="12.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#beta-regression"><i class="fa fa-check"></i><b>12.7</b> Beta Regression</a></li>
<li class="chapter" data-level="12.8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-5"><i class="fa fa-check"></i><b>12.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="binomial-models.html"><a href="binomial-models.html"><i class="fa fa-check"></i><b>13</b> Binomial Models</a><ul>
<li class="chapter" data-level="" data-path="binomial-models.html"><a href="binomial-models.html#prerequisites-6"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="13.1" data-path="binomial-models.html"><a href="binomial-models.html#introduction-2"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="binomial-models.html"><a href="binomial-models.html#link-functions-link-function"><i class="fa fa-check"></i><b>13.2</b> Link Functions {link-function}</a><ul>
<li class="chapter" data-level="13.2.1" data-path="binomial-models.html"><a href="binomial-models.html#stan"><i class="fa fa-check"></i><b>13.2.1</b> Stan</a></li>
<li class="chapter" data-level="13.2.2" data-path="binomial-models.html"><a href="binomial-models.html#example-vote-turnout"><i class="fa fa-check"></i><b>13.2.2</b> Example: Vote Turnout</a></li>
<li class="chapter" data-level="13.2.3" data-path="binomial-models.html"><a href="binomial-models.html#stan-1"><i class="fa fa-check"></i><b>13.2.3</b> Stan</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="binomial-models.html"><a href="binomial-models.html#references-6"><i class="fa fa-check"></i><b>13.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="separtion.html"><a href="separtion.html"><i class="fa fa-check"></i><b>14</b> Separation</a><ul>
<li class="chapter" data-level="" data-path="separtion.html"><a href="separtion.html#prerequisites-7"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="14.1" data-path="separtion.html"><a href="separtion.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="separtion.html"><a href="separtion.html#complete-separation"><i class="fa fa-check"></i><b>14.2</b> Complete Separation</a></li>
<li class="chapter" data-level="14.3" data-path="separtion.html"><a href="separtion.html#quasi-separation"><i class="fa fa-check"></i><b>14.3</b> Quasi-Separation</a></li>
<li class="chapter" data-level="14.4" data-path="separtion.html"><a href="separtion.html#weak-priors"><i class="fa fa-check"></i><b>14.4</b> Weak Priors</a></li>
<li class="chapter" data-level="14.5" data-path="separtion.html"><a href="separtion.html#example-support-of-aca-medicaid-expansion"><i class="fa fa-check"></i><b>14.5</b> Example: Support of ACA Medicaid Expansion</a></li>
<li class="chapter" data-level="14.6" data-path="separtion.html"><a href="separtion.html#questions-2"><i class="fa fa-check"></i><b>14.6</b> Questions</a></li>
<li class="chapter" data-level="14.7" data-path="separtion.html"><a href="separtion.html#references-7"><i class="fa fa-check"></i><b>14.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>15</b> Robust Regression</a><ul>
<li class="chapter" data-level="" data-path="robust-regression.html"><a href="robust-regression.html#prerequisites-8"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="15.1" data-path="robust-regression.html"><a href="robust-regression.html#wide-tailed-distributions"><i class="fa fa-check"></i><b>15.1</b> Wide Tailed Distributions</a></li>
<li class="chapter" data-level="15.2" data-path="robust-regression.html"><a href="robust-regression.html#student-t-distribution"><i class="fa fa-check"></i><b>15.2</b> Student-t distribution</a><ul>
<li class="chapter" data-level="15.2.1" data-path="robust-regression.html"><a href="robust-regression.html#examples-2"><i class="fa fa-check"></i><b>15.2.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="robust-regression.html"><a href="robust-regression.html#robit"><i class="fa fa-check"></i><b>15.3</b> Robit</a></li>
<li class="chapter" data-level="15.4" data-path="robust-regression.html"><a href="robust-regression.html#quantile-regression"><i class="fa fa-check"></i><b>15.4</b> Quantile regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="robust-regression.html"><a href="robust-regression.html#questions-3"><i class="fa fa-check"></i><b>15.4.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="robust-regression.html"><a href="robust-regression.html#references-8"><i class="fa fa-check"></i><b>15.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html"><i class="fa fa-check"></i><b>16</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#prerequisites-9"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="16.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#introduction-4"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#weighted-regression"><i class="fa fa-check"></i><b>16.2</b> Weighted Regression</a></li>
<li class="chapter" data-level="16.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#modeling-the-scale-with-covariates"><i class="fa fa-check"></i><b>16.3</b> Modeling the Scale with Covariates</a></li>
<li class="chapter" data-level="16.4" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#prior-distributions"><i class="fa fa-check"></i><b>16.4</b> Prior Distributions</a><ul>
<li class="chapter" data-level="16.4.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#examples-duncan"><i class="fa fa-check"></i><b>16.4.1</b> Examples: Duncan</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#exercises"><i class="fa fa-check"></i><b>16.5</b> Exercises</a></li>
<li class="chapter" data-level="16.6" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#references-9"><i class="fa fa-check"></i><b>16.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="rare-events.html"><a href="rare-events.html"><i class="fa fa-check"></i><b>17</b> Rare Events</a><ul>
<li class="chapter" data-level="" data-path="rare-events.html"><a href="rare-events.html#prerequisites-10"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="17.1" data-path="rare-events.html"><a href="rare-events.html#introduction-5"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="rare-events.html"><a href="rare-events.html#finite-sample-bias"><i class="fa fa-check"></i><b>17.2</b> Finite-Sample Bias</a></li>
<li class="chapter" data-level="17.3" data-path="rare-events.html"><a href="rare-events.html#case-control"><i class="fa fa-check"></i><b>17.3</b> Case Control</a></li>
<li class="chapter" data-level="17.4" data-path="rare-events.html"><a href="rare-events.html#questions-4"><i class="fa fa-check"></i><b>17.4</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html"><i class="fa fa-check"></i><b>18</b> Shrinkage and Hierarchical Models</a><ul>
<li class="chapter" data-level="18.1" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#hierarchical-models"><i class="fa fa-check"></i><b>18.1</b> Hierarchical Models</a></li>
<li class="chapter" data-level="18.2" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#baseball-hits"><i class="fa fa-check"></i><b>18.2</b> Baseball Hits</a><ul>
<li class="chapter" data-level="18.2.1" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#references-10"><i class="fa fa-check"></i><b>18.2.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html"><i class="fa fa-check"></i><b>19</b> Shrinkage and Regularized Regression</a><ul>
<li class="chapter" data-level="" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#prerequisites-11"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="19.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#introduction-6"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#shrinkage-estimators"><i class="fa fa-check"></i><b>19.2</b> Shrinkage Estimators</a><ul>
<li class="chapter" data-level="19.2.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#penalized-maximum-likelihood-regression"><i class="fa fa-check"></i><b>19.2.1</b> Penalized Maximum Likelihood Regression</a></li>
<li class="chapter" data-level="19.2.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#bayesian-shrinkage"><i class="fa fa-check"></i><b>19.2.2</b> Bayesian Shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#sparse-shrinkage"><i class="fa fa-check"></i><b>19.3</b> Sparse Shrinkage</a><ul>
<li class="chapter" data-level="19.3.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#penalized-likelihood"><i class="fa fa-check"></i><b>19.3.1</b> Penalized Likelihood</a></li>
<li class="chapter" data-level="19.3.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#bayesian-sparse-shrinkage-models"><i class="fa fa-check"></i><b>19.3.2</b> Bayesian Sparse Shrinkage Models</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#section"><i class="fa fa-check"></i><b>19.4</b> </a><ul>
<li class="chapter" data-level="19.4.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#shrinkage-factor"><i class="fa fa-check"></i><b>19.4.1</b> Shrinkage Factor</a></li>
<li class="chapter" data-level="19.4.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#prior-on-the-global-scale"><i class="fa fa-check"></i><b>19.4.2</b> Prior on the Global Scale</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#differences-between-bayesian-and-penalized-ml"><i class="fa fa-check"></i><b>19.5</b> Differences between Bayesian and Penalized ML</a></li>
<li class="chapter" data-level="19.6" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#examples-3"><i class="fa fa-check"></i><b>19.6</b> Examples</a><ul>
<li class="chapter" data-level="19.6.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#diabetes"><i class="fa fa-check"></i><b>19.6.1</b> Diabetes</a></li>
<li class="chapter" data-level="19.6.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#example-4"><i class="fa fa-check"></i><b>19.6.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="19.7" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#shrinkage-with-correlated-variables"><i class="fa fa-check"></i><b>19.7</b> Shrinkage with Correlated Variables</a></li>
<li class="chapter" data-level="19.8" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#variable-selection"><i class="fa fa-check"></i><b>19.8</b> Variable Selection</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="multilevel-models.html"><a href="multilevel-models.html"><i class="fa fa-check"></i><b>20</b> Multilevel Models</a><ul>
<li class="chapter" data-level="20.1" data-path="multilevel-models.html"><a href="multilevel-models.html#terminology"><i class="fa fa-check"></i><b>20.1</b> Terminology</a></li>
<li class="chapter" data-level="20.2" data-path="multilevel-models.html"><a href="multilevel-models.html#normal"><i class="fa fa-check"></i><b>20.2</b> Normal</a></li>
<li class="chapter" data-level="20.3" data-path="multilevel-models.html"><a href="multilevel-models.html#example-radon"><i class="fa fa-check"></i><b>20.3</b> Example: Radon</a><ul>
<li class="chapter" data-level="20.3.1" data-path="multilevel-models.html"><a href="multilevel-models.html#data"><i class="fa fa-check"></i><b>20.3.1</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="multilevel-models.html"><a href="multilevel-models.html#radon-example"><i class="fa fa-check"></i><b>20.4</b> Radon Example</a></li>
<li class="chapter" data-level="20.5" data-path="multilevel-models.html"><a href="multilevel-models.html#with-individual-covariates"><i class="fa fa-check"></i><b>20.5</b> With Individual Covariates</a></li>
<li class="chapter" data-level="20.6" data-path="multilevel-models.html"><a href="multilevel-models.html#with-group-level-covariates"><i class="fa fa-check"></i><b>20.6</b> With Group-Level Covariates</a></li>
<li class="chapter" data-level="20.7" data-path="multilevel-models.html"><a href="multilevel-models.html#pooling-of-hierarchical-parameters"><i class="fa fa-check"></i><b>20.7</b> Pooling of Hierarchical Parameters</a></li>
<li class="chapter" data-level="20.8" data-path="multilevel-models.html"><a href="multilevel-models.html#lme4"><i class="fa fa-check"></i><b>20.8</b> lme4</a></li>
<li class="chapter" data-level="20.9" data-path="multilevel-models.html"><a href="multilevel-models.html#covariance-priors"><i class="fa fa-check"></i><b>20.9</b> Priors for Covariances</a></li>
<li class="chapter" data-level="20.10" data-path="multilevel-models.html"><a href="multilevel-models.html#cetered-and-non-centered-parameterizations"><i class="fa fa-check"></i><b>20.10</b> Cetered and Non-centered Parameterizations</a></li>
<li class="chapter" data-level="20.11" data-path="multilevel-models.html"><a href="multilevel-models.html#extensions-1"><i class="fa fa-check"></i><b>20.11</b> Extensions</a></li>
<li class="chapter" data-level="20.12" data-path="multilevel-models.html"><a href="multilevel-models.html#miscellaneous"><i class="fa fa-check"></i><b>20.12</b> Miscellaneous</a><ul>
<li class="chapter" data-level="20.12.1" data-path="multilevel-models.html"><a href="multilevel-models.html#how-many-groups"><i class="fa fa-check"></i><b>20.12.1</b> How many groups?</a></li>
<li class="chapter" data-level="20.12.2" data-path="multilevel-models.html"><a href="multilevel-models.html#correlation-between-predictors-and-errors"><i class="fa fa-check"></i><b>20.12.2</b> Correlation between Predictors and Errors</a></li>
</ul></li>
<li class="chapter" data-level="20.13" data-path="multilevel-models.html"><a href="multilevel-models.html#references-11"><i class="fa fa-check"></i><b>20.13</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#prerequisites-12"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="20.14" data-path="appendix.html"><a href="appendix.html#parameters"><i class="fa fa-check"></i><b>20.14</b> Parameters</a></li>
<li class="chapter" data-level="20.15" data-path="appendix.html"><a href="appendix.html#miscellaneous-mathematical-background"><i class="fa fa-check"></i><b>20.15</b> Miscellaneous Mathematical Background</a><ul>
<li class="chapter" data-level="20.15.1" data-path="appendix.html"><a href="appendix.html#location-scale-families"><i class="fa fa-check"></i><b>20.15.1</b> Location-Scale Families</a></li>
<li class="chapter" data-level="20.15.2" data-path="appendix.html"><a href="appendix.html#scale-mixtures-of-normal-distributions"><i class="fa fa-check"></i><b>20.15.2</b> Scale Mixtures of Normal Distributions</a></li>
<li class="chapter" data-level="20.15.3" data-path="appendix.html"><a href="appendix.html#covariance-correlation-matrix-decomposition"><i class="fa fa-check"></i><b>20.15.3</b> Covariance-Correlation Matrix Decomposition</a></li>
<li class="chapter" data-level="20.15.4" data-path="appendix.html"><a href="appendix.html#qr-factorization"><i class="fa fa-check"></i><b>20.15.4</b> QR Factorization</a></li>
<li class="chapter" data-level="20.15.5" data-path="appendix.html"><a href="appendix.html#cholesky-decomposition"><i class="fa fa-check"></i><b>20.15.5</b> Cholesky Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="20.16" data-path="appendix.html"><a href="appendix.html#scaled-and-unscaled-variables"><i class="fa fa-check"></i><b>20.16</b> Scaled and Unscaled Variables</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>21</b> Distributions</a></li>
<li class="chapter" data-level="22" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html"><i class="fa fa-check"></i><b>22</b> Annotated Bibliography</a><ul>
<li class="chapter" data-level="22.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#textbooks"><i class="fa fa-check"></i><b>22.1</b> Textbooks</a></li>
<li class="chapter" data-level="22.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#syllabi"><i class="fa fa-check"></i><b>22.2</b> Syllabi</a></li>
<li class="chapter" data-level="22.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#topics"><i class="fa fa-check"></i><b>22.3</b> Topics</a></li>
<li class="chapter" data-level="22.4" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayes-theorem-1"><i class="fa fa-check"></i><b>22.4</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="22.5" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#article-length-introductions-to-bayesian-statistics"><i class="fa fa-check"></i><b>22.5</b> Article Length Introductions to Bayesian Statistics</a><ul>
<li class="chapter" data-level="22.5.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#why-bayesian"><i class="fa fa-check"></i><b>22.5.1</b> Why Bayesian</a></li>
<li class="chapter" data-level="22.5.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#modern-statistical-workflow"><i class="fa fa-check"></i><b>22.5.2</b> Modern Statistical Workflow</a></li>
<li class="chapter" data-level="22.5.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-philosophy"><i class="fa fa-check"></i><b>22.5.3</b> Bayesian Philosophy</a></li>
<li class="chapter" data-level="22.5.4" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>22.5.4</b> Bayesian Hypothesis Testing</a></li>
<li class="chapter" data-level="22.5.5" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-frequentist-debates"><i class="fa fa-check"></i><b>22.5.5</b> Bayesian Frequentist Debates</a></li>
<li class="chapter" data-level="22.5.6" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#categorical"><i class="fa fa-check"></i><b>22.5.6</b> Categorical</a></li>
<li class="chapter" data-level="22.5.7" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#variable-selection-1"><i class="fa fa-check"></i><b>22.5.7</b> Variable Selection</a></li>
<li class="chapter" data-level="22.5.8" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#multiple-testing"><i class="fa fa-check"></i><b>22.5.8</b> Multiple Testing</a></li>
<li class="chapter" data-level="22.5.9" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#rare-events-1"><i class="fa fa-check"></i><b>22.5.9</b> Rare Events</a></li>
<li class="chapter" data-level="22.5.10" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#identifiability"><i class="fa fa-check"></i><b>22.5.10</b> Identifiability</a></li>
<li class="chapter" data-level="22.5.11" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#shrinkage"><i class="fa fa-check"></i><b>22.5.11</b> Shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="22.6" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#software"><i class="fa fa-check"></i><b>22.6</b> Software</a><ul>
<li class="chapter" data-level="22.6.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#stan-2"><i class="fa fa-check"></i><b>22.6.1</b> Stan</a></li>
<li class="chapter" data-level="22.6.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#diagrams"><i class="fa fa-check"></i><b>22.6.2</b> Diagrams</a></li>
<li class="chapter" data-level="22.6.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#priors-1"><i class="fa fa-check"></i><b>22.6.3</b> Priors</a></li>
</ul></li>
<li class="chapter" data-level="22.7" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-model-averaging-1"><i class="fa fa-check"></i><b>22.7</b> Bayesian Model Averaging</a></li>
<li class="chapter" data-level="22.8" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#multilevel-modeling"><i class="fa fa-check"></i><b>22.8</b> Multilevel Modeling</a></li>
<li class="chapter" data-level="22.9" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#mixture-models-1"><i class="fa fa-check"></i><b>22.9</b> Mixture Models</a></li>
<li class="chapter" data-level="22.10" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#inference"><i class="fa fa-check"></i><b>22.10</b> Inference</a><ul>
<li class="chapter" data-level="22.10.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#discussion-of-bayesian-inference"><i class="fa fa-check"></i><b>22.10.1</b> Discussion of Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#model-checking-1"><i class="fa fa-check"></i><b>22.11</b> Model Checking</a><ul>
<li class="chapter" data-level="22.11.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#posterior-predictive-checks-1"><i class="fa fa-check"></i><b>22.11.1</b> Posterior Predictive Checks</a></li>
<li class="chapter" data-level="22.11.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#prediction-criteria"><i class="fa fa-check"></i><b>22.11.2</b> Prediction Criteria</a></li>
<li class="chapter" data-level="22.11.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#software-validation"><i class="fa fa-check"></i><b>22.11.3</b> Software Validation</a></li>
</ul></li>
<li class="chapter" data-level="22.12" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#hierarchical-modeling"><i class="fa fa-check"></i><b>22.12</b> Hierarchical Modeling</a></li>
<li class="chapter" data-level="22.13" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#shrinkageregularization"><i class="fa fa-check"></i><b>22.13</b> Shrinkage/Regularization</a></li>
<li class="chapter" data-level="22.14" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#empirical-bayes"><i class="fa fa-check"></i><b>22.14</b> Empirical Bayes</a></li>
<li class="chapter" data-level="22.15" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#history-of-bayesian-statistics"><i class="fa fa-check"></i><b>22.15</b> History of Bayesian Statistics</a></li>
<li class="chapter" data-level="22.16" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#sampling-difficulties"><i class="fa fa-check"></i><b>22.16</b> Sampling Difficulties</a></li>
<li class="chapter" data-level="22.17" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#complicated-estimation-and-testing"><i class="fa fa-check"></i><b>22.17</b> Complicated Estimation and Testing</a></li>
<li class="chapter" data-level="22.18" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#pooling-polls"><i class="fa fa-check"></i><b>22.18</b> Pooling Polls</a></li>
<li class="chapter" data-level="22.19" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#visualizing-mcmc-methods"><i class="fa fa-check"></i><b>22.19</b> Visualizing MCMC Methods</a></li>
<li class="chapter" data-level="22.20" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-point-estimation-decision"><i class="fa fa-check"></i><b>22.20</b> Bayesian point estimation / Decision</a></li>
<li class="chapter" data-level="22.21" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#stan-modeling-language"><i class="fa fa-check"></i><b>22.21</b> Stan Modeling Language</a></li>
<li class="chapter" data-level="22.22" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayes-factors"><i class="fa fa-check"></i><b>22.22</b> Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-12.html"><a href="references-12.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Updating: A Set of Bayesian Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\median}{median}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\logistic}{Logistic}
\DeclareMathOperator{\logit}{Logit}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

% This follows BDA
\newcommand{\dunif}{\mathsf{Uniform}}
\newcommand{\dnorm}{\mathsf{Normal}}
\newcommand{\dhalfnorm}{\mathrm{HalfNormal}}
\newcommand{\dlnorm}{\mathsf{LogNormal}}
\newcommand{\dmvnorm}{\mathsf{Normal}}
\newcommand{\dgamma}{\mathsf{Gamma}}
\newcommand{\dinvgamma}{\mathsf{InvGamma}}
\newcommand{\dchisq}{\mathsf{ChiSquared}}
\newcommand{\dinvchisq}{\mathsf{InvChiSquared}}
\newcommand{\dexp}{\mathsf{Exponential}}
\newcommand{\dlaplace}{\mathsf{Laplace}}
\newcommand{\dweibull}{\mathsf{Weibull}}
\newcommand{\dwishart}{\mathsf{Wishart}}
\newcommand{\dinvwishart}{\mathsf{InvWishart}}
\newcommand{\dlkj}{\mathsf{LkjCorr}}
\newcommand{\dt}{\mathsf{StudentT}}
\newcommand{\dhalft}{\mathsf{HalfStudentT}}
\newcommand{\dbeta}{\mathsf{Beta}}
\newcommand{\ddirichlet}{\mathsf{Dirichlet}}
\newcommand{\dlogistic}{\mathsf{Logistic}}
\newcommand{\dllogistic}{\mathsf{LogLogistic}}
\newcommand{\dpois}{\mathsf{Poisson}}
\newcommand{\dBinom}{\mathsf{Binomial}}
\newcommand{\dmultinom}{\mathsf{Multinom}}
\newcommand{\dnbinom}{\mathsf{NegativeBinomial}}
\newcommand{\dnbinomalt}{\mathsf{NegativeBinomial2}}
\newcommand{\dbetabinom}{\mathsf{BetaBinomial}}
\newcommand{\dcauchy}{\mathsf{Cauchy}}
\newcommand{\dhalfcauchy}{\mathsf{HalfCauchy}}
\newcommand{\dbernoulli}{\mathsf{Bernoulli}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Reals}{\R}
\newcommand{\RealPos}{\R^{+}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Nats}{\N}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}

\DeclareMathOperator{\invlogit}{Inv-Logit}
\DeclareMathOperator{\logit}{Logit}
\DeclareMathOperator{\diag}{diag}

\]
<div id="multilevel-models" class="section level1">
<h1><span class="header-section-number">20</span> Multilevel Models</h1>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;rstan&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;rstanarm&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)</code></pre>
<p>Multilevel models are a commonly used hierarchical model.
They extend (generalized) linear models to include coefficients that vary by discrete groups.</p>
<div id="terminology" class="section level2">
<h2><span class="header-section-number">20.1</span> Terminology</h2>
<p>These models go by different names in different literatures:
<em>hierarchical (generalized) linear models</em>, <em>nested data models</em>,
<em>mixed models</em>, <em>random coefficients</em>, <em>random-effects</em>,
<em>random parameter models</em>, <em>split-plot designs</em>.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>
There are further names for specific types of these models including varying-intercept, varying-slope,rando etc.</p>
<p>In classical statistics there two main approaches to these models:</p>
<ol style="list-style-type: decimal">
<li>Maximum likelihood (and variations): See the <strong>lme4</strong> package. This
is more common in statistics and many social sciences (except economics).</li>
<li>OLS and adjusted standard errors: See the <strong>plm</strong> packge. This is the
stanard econometric approach.</li>
</ol>
<p>See <a href="https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf">“Some false friends”</a> in the <strong>plm</strong> vignette and <span class="citation">A. Gelman and Hill (2007, 245, fn. 5)</span> for discussion of the differences between these approaches.</p>
</div>
<div id="normal" class="section level2">
<h2><span class="header-section-number">20.2</span> Normal</h2>
<p>Notation:</p>
<ul>
<li>Units: <span class="math inline">\(i = 1, \dots, n\)</span>.</li>
<li>Outcomes: <span class="math inline">\(y = (y_1, \dots, y_n)\)</span></li>
<li>Predictors: <span class="math inline">\(X\)</span> is a <span class="math inline">\(n \times p\)</span> matrix; <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(K \times 1\)</span>
column vector of individual <span class="math inline">\(i\)</span>’s predictors.</li>
<li>Groups: <span class="math inline">\(j = 1, \dots, m\)</span>. Each observation is in a group.
Let <span class="math inline">\(j[i]\)</span> indicate the group that observation <span class="math inline">\(i\)</span> is in.</li>
</ul>
<p>Suppose we want to estimate a regression for these data, where thet data generating process is <span class="math inline">\(y_i = \alpha_i + X_i \beta_i\)</span> with normal errors.</p>
<p>Depending on how we model differences between groups we can approach this in
three ways</p>
<ol style="list-style-type: decimal">
<li>Pooled model</li>
<li>Individual models</li>
<li>Partially pooled model</li>
</ol>
<p>In the <em>pooled model</em> we ignore differences between groups and use common
coefficients.
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha + x_i&#39; \beta, \sigma) \\
\end{aligned}
\]</span>
The pooled model estimates one model, with three parameters: <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>,
and <span class="math inline">\(\sigma\)</span>.</p>
<p>In an <em>individual</em> or <em>non-pooled model</em>, a sepearate model is estimated for each group.
For each <span class="math inline">\(j&#39; \in 1, \dots, m\)</span>, estimate the model
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]} + x_i&#39; \beta_{j[i]}, \sigma_{j[i]}) &amp; \text{ where } j[i] = j&#39; .
\end{aligned}
\]</span>
The non-pooled model estimates <span class="math inline">\(m\)</span> separate models with <span class="math inline">\(m \times 3\)</span> parameters, one <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span> for each group.
Note that in this model, no information is shared between groups.
The data from another group and its estimates of <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span> do not help in the estimation of another group.
However, if these are in any way similar problems we may think that the values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> differ between groups, but that they should be similar.
We should think that there must be a way to share that information via a prior.
This insight will lead to the partially pooled model.</p>
<p>In a <em>partially pooled</em> model, different groups have different parameters, but
these group parameters share common hyperpriors—priors in which the distribution is a function of parameters estimated from the data.
This is an example of a partially pooled model,
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]} + \beta_{j[i]} x_i, \sigma_{j[i]}) \\
  \begin{bmatrix}
  \alpha_j \\
  \beta_j
  \end{bmatrix}
&amp; \sim
\dnorm
\left(
  \begin{bmatrix}
  \mu_\alpha \\
  \mu_\beta
  \end{bmatrix},
\Omega
\right) \\
\sigma_{j[i]} &amp;\sim p(.)
\end{aligned} .
\]</span>
The imporant feature of this model, is that the priors for the parameters <span class="math inline">\(\alpha_j\)</span>, <span class="math inline">\(\beta_j\)</span> are functions of other parameters, often called <em>hyperparmeters</em>, <span class="math inline">\(\mu_{\alpha}\)</span>, <span class="math inline">\(\mu_{\beta}\)</span>, and <span class="math inline">\(\Omega\)</span>.
This is allows for sharing information between groups.
The observations from one group help to estimate <span class="math inline">\(\mu_{\alpha}\)</span>, <span class="math inline">\(\mu_{\beta}\)</span>,
and <span class="math inline">\(\Sigma\)</span>, which in turn help to estimate the <span class="math inline">\(\alpha_j\)</span> and <span class="math inline">\(\beta_j\)</span> in another group.
If the parameters of the prior distribution of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> were fixed (data) and not themselves parameters, this sharing of information would not occur.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="multilevel_files/figure-html/unnamed-chunk-4-1.png" alt="Visual representation of hierarchical models" width="70%" />
<p class="caption">
Figure 20.1: Visual representation of hierarchical models
</p>
</div>
<p>We can also allow some parameters to vary between groups, but pool other parameters.</p>
<p><em>Group-heteroskedastic</em>: Assume that the intercept (<span class="math inline">\(\alpha\)</span>) and slope (<span class="math inline">\(\beta\)</span>) are the same across groups, but allow the scale of the regression error to vary between groups.
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha + x&#39;_i \beta, \sigma_{j[i]}) \\
\log \sigma_j &amp;\sim \dnorm(\tau, \psi)
\end{aligned}
\]</span>
The prior on the <span class="math inline">\(\log \sigma_j\)</span> has two parameters, <span class="math inline">\(\tau\)</span> (the average log-standard deviation of the errors) and <span class="math inline">\(\psi\)</span>, which determines the level of heteroskedasticity.</p>
<p>In a <em>varying-intercept</em> model keep the slope coefficients (<span class="math inline">\(\beta\)</span>) common between groups, but allow the intercepts (<span class="math inline">\(\alpha_j\)</span>) to vary by group.
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]} + \beta x_i, \sigma) \\
\alpha_{j} &amp;\sim \dnorm(\mu_\alpha, \omega_{\alpha})
\end{aligned}
\]</span></p>
<p>In a <em>varying-slope model</em>, the groups share a common intercept, <span class="math inline">\(\alpha\)</span>, but the slope coefficient (<span class="math inline">\(\beta\)</span>), varies between groups.</p>
<p><span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha + \beta_{j[i]} x_i, \sigma^2) \\
\beta_{j} &amp; \sim \dnorm(\mu_{\beta}, \omega_{\beta})
\end{aligned}
\]</span>
This is less common since it is hard to think of cases when it is appropriate.
More often, if the slope coefficient is allowed to vary between groups,
the the intercepts should as well.
This is the more-general <em>varying-slope varying-intercept</em> model,
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha + \beta_{j[i]} x_i, \sigma^2) \\
  \begin{bmatrix}
  \alpha_j \\
  \beta_j
  \end{bmatrix}
&amp; \sim
\dnorm
\left(
  \begin{bmatrix}
  \mu_\alpha \\
  \mu_\beta
  \end{bmatrix},
\Omega
\right)
\end{aligned}
\]</span></p>
</div>
<div id="example-radon" class="section level2">
<h2><span class="header-section-number">20.3</span> Example: Radon</h2>
<p>This example models the presence of radon in houses in Minnesota which appears in <span class="citation">A. Gelman and Hill (2007)</span> and <span class="citation">A. Gelman, Carlin, et al. (2013)</span>.
This is partly derived from a <a href="http://mc-stan.org/documentation/case-studies/radon.html">Stan Case Study</a>, which uses <code>PyStan</code> instead of <strong>rstan</strong>.</p>
<div id="data" class="section level3">
<h3><span class="header-section-number">20.3.1</span> Data</h3>
<p>The <a href="https://www.rdocumentation.org/packages/radon/topics/rstanarm">radon</a> data is included in the <strong><a href="https://cran.r-project.org/package=rstanarm">rstanarm</a></strong> package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;radon&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;rstanarm&quot;</span>)
<span class="kw">glimpse</span>(radon)
<span class="co">#&gt; Observations: 919</span>
<span class="co">#&gt; Variables: 4</span>
<span class="co">#&gt; $ floor       &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</span>
<span class="co">#&gt; $ county      &lt;fct&gt; AITKIN, AITKIN, AITKIN, AITKIN, ANOKA, ANOKA, ANOK...</span>
<span class="co">#&gt; $ log_radon   &lt;dbl&gt; 0.8329, 0.8329, 1.0986, 0.0953, 1.1632, 0.9555, 0....</span>
<span class="co">#&gt; $ log_uranium &lt;dbl&gt; -0.689, -0.689, -0.689, -0.689, -0.847, -0.847, -0...</span></code></pre>
<p>The data consist of 919 observations of radon levels of houses from 85 counties.</p>
<pre class="sourceCode r"><code class="sourceCode r">radon_county &lt;-<span class="st"> </span>radon <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(county) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">log_radon_mean =</span> <span class="kw">mean</span>(log_radon),
            <span class="dt">log_radon_sd =</span> <span class="kw">sd</span>(log_radon),
            <span class="dt">log_uranium =</span> <span class="kw">mean</span>(log_uranium),
            <span class="dt">n =</span> <span class="kw">length</span>(county)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">log_radon_se =</span> log_radon_sd <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(n))</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">data =</span> radon,
               <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">y =</span> log_radon,
                             <span class="dt">x =</span> <span class="kw">fct_reorder</span>(county, log_radon, mean)),
               <span class="dt">colour =</span> <span class="st">&quot;gray&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> radon,
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">y =</span> log_radon,
                           <span class="dt">x =</span> <span class="kw">fct_reorder</span>(county, log_radon, mean)),
             <span class="dt">colour =</span> <span class="st">&quot;gray&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> radon_county,
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">fct_reorder</span>(county, log_radon_mean),
                           <span class="dt">y =</span> log_radon_mean),
             <span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;log(radon)&quot;</span>, <span class="dt">x =</span> <span class="st">&quot;&quot;</span>)</code></pre>
<p><img src="multilevel_files/figure-html/unnamed-chunk-6-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The observations in counties vary in size,</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(radon_county, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">log2</span>(n))) <span class="op">+</span>
<span class="st">         </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">         </span><span class="kw">geom_rug</span>()</code></pre>
<p><img src="multilevel_files/figure-html/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Unsurprisingly, there is more variation in county means among counties with smaller numbers
of observations.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(radon_county, <span class="kw">aes</span>(<span class="dt">y =</span> log_radon_mean,
                         <span class="dt">x =</span> <span class="kw">log2</span>(n))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre>
<p><img src="multilevel_files/figure-html/unnamed-chunk-8-1.png" width="70%" style="display: block; margin: auto;" />
Much of this can be explained simply by sampling error.</p>
</div>
</div>
<div id="radon-example" class="section level2">
<h2><span class="header-section-number">20.4</span> Radon Example</h2>
<p>In this example we want to model the amount of radon (log scale) in a home.
Let <span class="math inline">\(y_i\)</span> be the <em>centered</em> and <em>scaled</em> log amount of radon in home <span class="math inline">\(i\)</span>.</p>
<p>The simplest model is a pooled model, where all homes are modeled as iid draws
from a common distribution.
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha, \sigma) &amp; \text{for } i \in 1, \dots, n \\
\alpha &amp;\sim \dnorm(0, 10) \\
\sigma &amp;\sim \dexp(1)
\end{aligned}
\]</span>
where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma\)</span> are given weakly informative priors.
We will estimate these models <strong>rstanarm</strong> functions <code>stan_glm</code> and <code>stan_glmer</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_radon_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(log_radon <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> radon, <span class="dt">refresh =</span> <span class="dv">-1</span>)</code></pre>
<p>We can extend this to a no-pooling model with a different mean for each county.
Let <span class="math inline">\(j[i] \in 1, \dots, m\)</span> be the county of observation <span class="math inline">\(i\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]}, \sigma) &amp; \text{for } i \in 1, \dots, n \\
\alpha_j &amp;\sim \dnorm(0, 10) &amp; \text{for } j \in 1, \dots, m
\end{aligned}
\]</span>
The <span class="math inline">\(\alpha_j\)</span> are all drawn from weakly informative prior distributions.
And although the <span class="math inline">\(\alpha_j\)</span> are drawn from the same prior distribution,
it has fixed parameters and thus no information is shared between observations in different counties.</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_radon_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(log_radon <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span>county, <span class="dt">data =</span> radon, <span class="dt">refresh =</span> <span class="dv">-1</span>)</code></pre>
<p>Finally, consider a partially pooled model.
Like the previous model, each county has its own mean value.
However, now these county-means share a prior which has its own parameters.
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]}, \sigma) &amp; \text{for } i \in 1, \dots, n \\
\alpha_j &amp;\sim \dnorm(\mu, \tau) &amp; \text{for } j \in 1, \dots, m
\end{aligned}
\]</span>
We could also write the model with the country-level average in the
mean equation for <span class="math inline">\(y\)</span>, and the <span class="math inline">\(\alpha_j\)</span> values distributed around the county
level average, <span class="math inline">\(\gamma\)</span>.
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]}, \sigma) &amp; \text{for } i \in 1, \dots, n \\
\alpha_j &amp;\sim \dnorm(\gamma, \tau) &amp; \text{for } j \in 1, \dots, m \\
\tau &amp;\sim \dexp(1)
\end{aligned}
\]</span></p>
<p>Hierarchical/multi-level generalized linear models can be estimated with
<code>stan_glmer</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_radon_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">stan_glmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>floor <span class="op">|</span><span class="st"> </span>county), <span class="dt">data =</span> radon,
                          <span class="dt">refresh =</span> <span class="dv">-1</span>)</code></pre>
<p>For each of these models extract the county means.</p>
<pre class="sourceCode r"><code class="sourceCode r">alpha_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">tidyMCMC</span>(fit_radon_<span class="dv">1</span>, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;(Intercept)&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># add county</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">county =</span> <span class="kw">list</span>(<span class="kw">unique</span>(<span class="kw">as.character</span>(radon[[<span class="st">&quot;county&quot;</span>]])))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(county) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="st">&quot;Complete&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">alpha_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">tidyMCMC</span>(fit_radon_<span class="dv">2</span>, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">str_detect</span>(term, <span class="st">&quot;^county&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">county =</span> <span class="kw">str_replace</span>(term, <span class="st">&quot;^county&quot;</span>, <span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="st">&quot;No&quot;</span>)</code></pre>
<p>See this <a href="http://mc-stan.org/rstanarm/articles/pooling.html">vignette</a> for
extracting the random intercepts.</p>
<pre class="sourceCode r"><code class="sourceCode r">alphas &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(fit_radon_<span class="dv">3</span>, <span class="dt">regex_pars =</span> <span class="st">&quot;^b</span><span class="ch">\\</span><span class="st">[&quot;</span>)
alpha_mean &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(fit_radon_<span class="dv">3</span>, <span class="dt">pars =</span> <span class="st">&quot;(Intercept)&quot;</span>)
alpha_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">sweep</span>(alphas, <span class="dv">1</span>, alpha_mean, <span class="dt">FUN =</span> <span class="st">&quot;+&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(term, value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">estimate =</span> <span class="kw">mean</span>(value), <span class="dt">conf.low =</span> <span class="kw">quantile</span>(value, <span class="fl">0.025</span>),
            <span class="dt">conf.high =</span> <span class="kw">quantile</span>(value, <span class="fl">0.975</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">county =</span> <span class="kw">str_match</span>(term, <span class="st">&quot;county:(.*)</span><span class="ch">\\</span><span class="st">]&quot;</span>)[ , <span class="dv">2</span>]) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="st">&quot;Partial&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">all_models &lt;-
<span class="st">  </span><span class="kw">bind_rows</span>(alpha_<span class="dv">1</span>, alpha_<span class="dv">2</span>, alpha_<span class="dv">3</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># reorder county by size</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">county =</span> <span class="kw">fct_reorder</span>(county, estimate, mean))</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(all_models, <span class="kw">aes</span>(<span class="dt">x =</span> county, <span class="dt">y =</span> estimate, <span class="dt">ymin =</span> conf.low, <span class="dt">ymax =</span> conf.high,
             <span class="dt">color =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="dt">position =</span> <span class="kw">position_dodge</span>(<span class="dt">width =</span> <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="multilevel_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="with-individual-covariates" class="section level2">
<h2><span class="header-section-number">20.5</span> With Individual Covariates</h2>
<p>Individual covariates can also be added.
<span class="math display">\[
\begin{aligned}
y_i &amp;\sim  N(\mu_i, \sigma^2) \\
\mu_i &amp;= \alpha_{j[i]} + \beta~\mathtt{floor}_i  \\
\alpha_j &amp;\sim \dnorm(\gamma, \tau) &amp; \text{for } j \in 1, \dots, m \\
\beta &amp;\sim \dnorm(0, 2.5)\\
\tau &amp;\sim \dexp(1) \\
\gamma &amp;\sim \dnorm(0, 10)
\end{aligned}
\]</span>
This is often called a <em>varying intercept</em> model or the <em>random effects</em> model.
Each county has a different intercept, but a common value of <span class="math inline">\(\beta\)</span>.
Note that the prior on <span class="math inline">\(\beta\)</span> assumes that <span class="math inline">\(y\)</span> and <span class="math inline">\(\mathtt{floor}\)</span> are centered and scaled.</p>
<p>We can estimate this with <code>stan_glmer</code> as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_radon_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">stan_glmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>county) <span class="op">+</span><span class="st"> </span>floor,
                          <span class="dt">data =</span> radon,
                          <span class="dt">refresh =</span> <span class="dv">-1</span>)</code></pre>
<p>We could also allow the parameter of <span class="math inline">\(\beta\)</span> to vary between counties and give it a prior distribution.
<span class="math display">\[
\begin{aligned}
y_i &amp;\sim  N(\mu_i, \sigma^2) \\
\mu_i &amp;= \alpha_{j[i]} + \beta_{j[i]}~\mathtt{floor}_i
\end{aligned}
\]</span>
Both <span class="math inline">\(\alpha_j\)</span> and <span class="math inline">\(\beta_j\)</span> need to be given hierarchical priors.
Since there are features common to each group, it is common to specify a prior
that allows these parameters to be correlated, such as a multivariate normal
distribution with mean <span class="math inline">\(\gamma\)</span> and covariance <span class="math inline">\(\Sigma\)</span>,
<span class="math display">\[
\begin{aligned}[t]
\begin{bmatrix}
\alpha_j \\
\beta_j
\end{bmatrix} &amp;\sim
\dnorm\left(\begin{bmatrix}\gamma_{\alpha} \\ \gamma_{\beta} \end{bmatrix}, \Sigma\right) \\
\end{aligned}
\]</span>
The vector of locations of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> can be given weakly informative
priors, which can be the same as the pooled parameters would be given,
<span class="math display">\[
\begin{aligned}
\gamma_{\alpha} &amp;\sim \dnorm(0, 10) \\
\gamma_{\beta} &amp;\sim \dnorm(0, 2.5) \\
\end{aligned}
\]</span>
The prior distribution for the covariance matrix is more complicated.
See the section <a href="multilevel-models.html#covariance-priors">Priors for Covariances</a> for a discussion of appropriate prior
distirbutions.</p>
<p>If instead, <span class="math inline">\(\alpha_j\)</span> and <span class="math inline">\(\beta_j\)</span> were given inproper priors, or
alternatively, <span class="math inline">\(\Sigma \to \tau I\)</span> where <span class="math inline">\(\tau \to \infty\)</span>, then this model
would be equivalent to an MLE model in with county fixed effects, and with
<code>county</code> interacted with <code>floor</code>, e.g. <code>log_radon ~ county * floor</code>.</p>
<p>This model is often called a <em>(varying-intercept) varying-slope</em> model, or
a <em>random coefficients</em> model.
It can be estimated as:</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_radon_<span class="dv">5</span> &lt;-<span class="st"> </span><span class="kw">stan_glmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>floor <span class="op">|</span><span class="st"> </span>county),
                          <span class="dt">data =</span> radon,
                          <span class="dt">refresh =</span> <span class="dv">-1</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit_radon_<span class="dv">5</span>, <span class="dt">regex_pars =</span> <span class="kw">c</span>(<span class="st">&quot;^b</span><span class="ch">\\</span><span class="st">[floor&quot;</span>))</code></pre>
<p><img src="multilevel_files/figure-html/unnamed-chunk-18-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="with-group-level-covariates" class="section level2">
<h2><span class="header-section-number">20.6</span> With Group-Level Covariates</h2>
<p>Group-level covariates could also be added as predictors.
<span class="math display">\[
\begin{aligned}
y_i &amp;\sim  N(\mu_i, \sigma^2) \\
\mu_i &amp;= \alpha_{j[i]} + \beta~\mathtt{log\_uranium}_{j[i]}  \\
\alpha_j &amp;\sim \dnorm(\gamma, \tau) &amp; \text{for } j \in 1, \dots, m \\
\beta &amp;\sim \dnorm(0, 2.5)\\
\tau &amp;\sim \dexp(1) \\
\gamma &amp;\sim \dnorm(0, 10)
\end{aligned}
\]</span></p>
<p>We could also write that model with group-level predictors included in the prior fthe group level intercepts as
<span class="math display">\[
\begin{aligned}
y_i &amp;\sim  N(\mu_i, \sigma^2) \\
\mu_i &amp;= \alpha_{j[i]} \\
\alpha_j &amp;\sim \dnorm(\gamma +  \beta~\mathtt{log\_uranium}_j, \tau) &amp; \text{for } j \in 1, \dots, m \\
\end{aligned}
\]</span>
These two models are equivalent.
However, they may have different sampling efficiencies.
Generally, in estimation, the first form in which the group-level predictors are included in the top level is used.</p>
<p>To estimate this model with <code>stan_glmer</code> run the following.</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_radon_<span class="dv">6</span> &lt;-<span class="st"> </span><span class="kw">stan_glmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>county) <span class="op">+</span><span class="st"> </span>log_uranium,
                          <span class="dt">data =</span> radon,
                          <span class="dt">refresh =</span> <span class="dv">-1</span>)</code></pre>
</div>
<div id="pooling-of-hierarchical-parameters" class="section level2">
<h2><span class="header-section-number">20.7</span> Pooling of Hierarchical Parameters</h2>
<p>What does partial pooling do?<br />
This is easiest to understand in a simple mean-model with normal errors.
<span class="math display">\[
\begin{aligned}[t]
y &amp;\sim \dnorm(\mu_{j[i]}, \sigma) \\
\mu_{j} &amp;\sim \dnorm(\gamma, \tau) .
\end{aligned}
\]</span>
If the hyperparameters were known, the posterior of <span class="math inline">\(\mu_j\)</span> is
<span class="math display">\[
\mu_j | y, \gamma, \sigma, \tau \sim \dnorm(\hat{\mu}_j, V_j)
\]</span>
where
<span class="math display">\[
\begin{aligned}[t]
\hat{\mu}_j &amp;= \frac{\frac{n_j}{\sigma^2} \bar{y}_j + \frac{1}{\tau^2} \gamma}{\frac{n_j}{\sigma^2} + \frac{1}{\tau^2}} \\
V_j &amp;= \frac{1}{\frac{n_j}{\sigma^2} + \frac{1}{\tau^2}}
\end{aligned}
\]</span></p>
<p>How does this vary depending on the size of the group, <span class="math inline">\(n_j\)</span>?</p>
<table>
<colgroup>
<col width="32%" />
<col width="67%" />
</colgroup>
<thead>
<tr class="header">
<th>Sample size, <span class="math inline">\(n_j\)</span></th>
<th>Estimate of <span class="math inline">\(\hat{\mu}_j\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(n_j = 0\)</span></td>
<td><span class="math inline">\(\hat{\mu}_j = \gamma\)</span> (complete pooling)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_j &lt; \frac{\sigma^2}{\tau^2}\)</span></td>
<td><span class="math inline">\(\hat{\mu}_j\)</span> closer to <span class="math inline">\(\gamma\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_j = \frac{\sigma^2}{\tau^2}\)</span></td>
<td><span class="math inline">\(\hat{\mu}_j = \frac{1}{2} \bar{y}_j + \frac{1}{2} \gamma\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_j &gt; \frac{\sigma^2}{\tau^2}\)</span></td>
<td><span class="math inline">\(\hat{\mu}_j\)</span> closer to <span class="math inline">\(\bar{y}_j\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_j = \infty\)</span></td>
<td><span class="math inline">\(\hat{\mu}_j = \bar{y}_j\)</span> (no pooling)</td>
</tr>
</tbody>
</table>
<p>When the sample size of the group is large, the prior data from other groups provides little additional informiation and the posterior is dominated by the observations in that group.
However, when the sample size of the group is small, there is little information coming from the groups own observations and prior data from other groups can have a big influence on the posterior.
We even have posterior estimates of groups and observations from groups we have not seen before due to the estimated distribution of group means.</p>
<p>This is a good reason to use partial pooling.
If there is enough data it will converge to the no-pooling case, but if there is not it will degrade to the full-pooling case.</p>
<p>If instead the group means <span class="math inline">\(\mu_j\)</span> were known, we can calculate some crude estimates.
The <em>data variance</em>, <span class="math inline">\(\sigma^2\)</span>, is the residual variance,
<span class="math display">\[
\E(\sigma^2 | y, \mu)  = \frac{1}{n} \sum_{i = 1}^n (y - \mu_{j[i]})^2 .
\]</span>
The global mean is approximately the average of the group-level means,
<span class="math display">\[
\begin{aligned}[t]
\E(\gamma | y, \mu) &amp;= \frac{1}{J} \sum_{i = 1}^n \mu_j \\
\Var(\gamma | y, \mu) &amp;= \frac{1}{J} \tau^2
\end{aligned}
\]</span>
The group level variance is <span class="math inline">\(\tau^2\)</span> is,
<span class="math display">\[
\E(\tau^2 | y, \mu) = \frac{1}{J} \sum_{j = 1}^J (\mu_j - \gamma)^2
\]</span></p>
</div>
<div id="lme4" class="section level2">
<h2><span class="header-section-number">20.8</span> lme4</h2>
<p>In R, the most widely used package to estimate mixed-effects models is <strong>lme4</strong>.
This estimates models using maximum likelihood or restricted maximum likelihood methods (REML).
This will be faster than using full-Bayesian methods but also underestimate the uncertainty, as well as being a worse approximation of the posterior.
Additionally, in frequentist inference, the meaning of the random effects is different; they are nuisance parameters and not given standard errors.</p>
<p>See <span class="citation">Bates (2010)</span> and <span class="citation">Bates et al. (2014)</span> for introductions to mixed-effects models with <strong>lme4</strong>.
These are also good introductions to classical approaches to mixed effects models.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;lme4&quot;</span>)</code></pre>
<p>Complete pooling</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_pooled &lt;-<span class="st"> </span><span class="kw">lm</span>(log_radon <span class="op">~</span><span class="st"> </span>county <span class="op">+</span><span class="st"> </span>floor, <span class="dt">data =</span> radon)</code></pre>
<p>County-varying intercepts with no-pooling</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_intercept_nopool &lt;-<span class="st"> </span><span class="kw">lm</span>(log_radon <span class="op">~</span><span class="st"> </span>floor, <span class="dt">data =</span> radon)</code></pre>
<p>County-varying intercepts with partial-pooling</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_intercept_partial &lt;-<span class="st"> </span><span class="kw">lmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>county) <span class="op">+</span><span class="st"> </span>floor, <span class="dt">data =</span> radon)</code></pre>
<p>Varying slopes with no pooling:</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_slope_nopool &lt;-<span class="st"> </span><span class="kw">lm</span>(log_radon <span class="op">~</span><span class="st"> </span>county <span class="op">*</span><span class="st"> </span>floor, <span class="dt">data =</span> radon)</code></pre>
<p>Varying slopes with partial pooling:</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_slope_partial &lt;-<span class="st"> </span><span class="kw">lmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>floor <span class="op">|</span><span class="st"> </span>county), <span class="dt">data =</span> radon)</code></pre>
<p>We can also include a county-level variable (<code>log_uranium</code>) in various models:
With no-pooling,</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_slope_partial &lt;-<span class="st"> </span><span class="kw">lm</span>(log_radon <span class="op">~</span><span class="st"> </span>floor <span class="op">+</span><span class="st"> </span>log_uranium, <span class="dt">data =</span> radon)</code></pre>
<p>With varying-intercepts</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_slope_partial &lt;-<span class="st"> </span><span class="kw">lmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>county) <span class="op">+</span><span class="st"> </span>floor <span class="op">+</span><span class="st"> </span>log_uranium,
                          <span class="dt">data =</span> radon)</code></pre>
<p>With varying-intercepts and slopes,</p>
<pre class="sourceCode r"><code class="sourceCode r">fit_slope_partial &lt;-<span class="st"> </span><span class="kw">lmer</span>(log_radon <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>floor <span class="op">|</span><span class="st"> </span>county) <span class="op">+</span><span class="st">  </span>log_uranium,
                          <span class="dt">data =</span> radon)</code></pre>
</div>
<div id="covariance-priors" class="section level2">
<h2><span class="header-section-number">20.9</span> Priors for Covariances</h2>
<p>Traditinally the Wishart and inverse Wishart distributions have been used for modeling covariance distributions. This was largely because they were in some cases conjugate
distributions. However, they have several poor properties.
See <a href="https://github.com/pymc-devs/pymc3/issues/538#issuecomment-94153586" class="uri">https://github.com/pymc-devs/pymc3/issues/538#issuecomment-94153586</a></p>
<p>A preferred approach is to first decompose the covariance matrix (<span class="math inline">\(\Sigma\)</span>) into
a vector of standard deviations (<span class="math inline">\(\omega\)</span>) and a correlation matrix (<span class="math inline">\(R\)</span>),
<span class="math display">\[
\Sigma = \diag(\omega) R \diag(\omega) ,
\]</span>
and then place priors on each of those.</p>
<p>The preferred distribution for a correlation distribution is
the LKJ distribution.
This distribution is proportional to a product of the determinant of the correlation matrix.
The LKJ distribution takes a single parameter <span class="math inline">\(\eta &gt; 0\)</span>.
It can be interpreted similarly to the the shape parameter of a symmetric Beta distribution.
When <span class="math inline">\(\eta = 1\)</span>, then there is a uniform distribution over all correlation matrices.
As <span class="math inline">\(\eta \to 1\)</span>, the correlation matrix approaches the identity matrix.
As <span class="math inline">\(\eta \to 0\)</span>, the density has a trough at the identity matrix.
<span class="math display">\[
\mathsf{LKJ}(R | \eta) \propto |R|^{\eta - 1}
\]</span></p>
<p>There are a few ways to assign prior distributions to the standard deviation vector, <span class="math inline">\(\omega\)</span>.</p>
<p>The <code>lkj()</code> prior used by <code>stan_mvmer()</code> and <code>stan_jm()</code> assigns independent Half Student-t priors, with degrees of freedom <span class="math inline">\(d\)</span>, and scale, <span class="math inline">\(s_j\)</span>,
<span class="math display">\[
\omega_j \sim \HalfStudentT(d, 0, s_j) .
\]</span></p>
<p>The <code>deconv()</code> prior used by <code>stan_glmer</code> decomposes the standard deviation vector further.
It notes that the trace of a covariance matrix is equal to the sum of the variances.
This suggests decomposing the variance vector (<span class="math inline">\(\omega^2\)</span>) into the product of the
trace (<span class="math inline">\(\psi^2\)</span>) and a simplex (<span class="math inline">\(\pi\)</span>) which allocates how much of the total variance
is associated with each element.
<span class="math display">\[
\begin{aligned}
\omega &amp;= \psi \sqrt{\pi}
\end{aligned}
\]</span>
The simplex is given a symmetric Dirichlet prior with concentration parameter <span class="math inline">\(k\)</span>,
<span class="math display">\[
\pi &amp;\sim \ddirchlet(k, 1 / p).
\]</span>
The default for <code>deconv()</code> is 1, for a uniform distribution over the space of simplex vectors of that size.</p>
<p>The square root of the trace (<span class="math inline">\(\psi\)</span>) is given a gamma prior with shape parameter <span class="math inline">\(a\)</span>
and scale parameter <span class="math inline">\(b\)</span>,
<span class="math display">\[
\psi &amp;\sim \dgamma(a, b) .
\]</span>
The default for <code>deconv()</code> is <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 1\)</span>, which is equal to <span class="math inline">\(\dexp(\psi | 1)\)</span>.</p>
</div>
<div id="cetered-and-non-centered-parameterizations" class="section level2">
<h2><span class="header-section-number">20.10</span> Cetered and Non-centered Parameterizations</h2>
<p>The natural way to write a hiearchical model is with a centered parameterization:
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]}, \sigma) \\
\alpha_{j} &amp;\sim \dnorm(\mu_\alpha, \omega_{\alpha})
\end{aligned}
\]</span>
However, this can be difficult to sample from. In particular, by construction,
the values of <span class="math inline">\(\mu_{\alpha}\)</span>, <span class="math inline">\(\omega_{\alpha}\)</span>, and <span class="math inline">\(\alpha_j\)</span> are going to be highly correlated.
This results in a posterior pattern called a “funnel” which is difficult for HMC
to sample due to the changes in the posterior curvature.</p>
<p>The centered paramerization centers and scales the distribution of the intercepts.
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha_{j[i]}, \sigma) \\
\alpha_{j} &amp;\sim \mu_{\alpha} + \omega_{\alpha} \alpha^* \\
\alpha^*_{j} &amp;\sim \dnorm(0, 1)
\end{aligned}
\]</span></p>
<p>These are two ways of writing the <em>same</em> model.
However, they change the parameters that the HMC algorithm is actively sampling
and thus can have different sampling performance.</p>
<p>However, neither is universally better.</p>
<ul>
<li>If much data, the non-centered parameterization works better</li>
<li>If less data, the centered parameterization works better</li>
</ul>
<p>And there is currently no ex-ante way to know which will work better, and at
what amount of “data” that the performance of one or the other is better.
However, one other reason to use the centered parameterization (if it is also scaled),
is that the Stan HMC implementation tends to be more efficient if all parameters
are on the scale.</p>
<p>See also</p>
<ul>
<li>Stan Ref Ch 9 “Regression” has sections on hierarchical/multi-level models</li>
<li>Stan Ref Ch 22 “Reparameterizations &amp; Change of Variables”</li>
<li><a href="http://twiecki.github.io/blog/2017/02/08/bayesian-hierchical-non-centered/" class="uri">http://twiecki.github.io/blog/2017/02/08/bayesian-hierchical-non-centered/</a></li>
<li><a href="https://arxiv.org/pdf/1312.0906.pdf" class="uri">https://arxiv.org/pdf/1312.0906.pdf</a></li>
<li>&lt;<a href="https://www.youtube.com/watch?v=pHsuIaPbNbY&amp;t=8s" class="uri">https://www.youtube.com/watch?v=pHsuIaPbNbY&amp;t=8s</a></li>
</ul>
</div>
<div id="extensions-1" class="section level2">
<h2><span class="header-section-number">20.11</span> Extensions</h2>
<ul>
<li><p>Including group-level covariates</p></li>
<li><p>Prior distributions for hierarchical scale</p></li>
<li><p>Prediction. Hierarchical models can be use</p>
<ul>
<li>new obs in existing groups</li>
<li>new group</li>
<li>new obs in new group</li>
</ul></li>
<li><p>Modeling correlation between intercept and slopes</p></li>
<li><p>Non-nested models</p></li>
</ul>
</div>
<div id="miscellaneous" class="section level2">
<h2><span class="header-section-number">20.12</span> Miscellaneous</h2>
<div id="how-many-groups" class="section level3">
<h3><span class="header-section-number">20.12.1</span> How many groups?</h3>
<p>In classical discussions of multi-level or hierarchical models,
a common question is how many groups are required to be able to use random effects vs. fixed effects.</p>
<p>As noted earlier, random effects estimates the variance between group means. If there are few groups, there is not much information available to estimate this variance.
As such, random effects is not much different than fixed effects.</p>
<p>This literature provides many different rules of thumb for the number of groups necessary to be able to use random effects: 8, 10, 30, 50, or 100 <span class="citation">(Stegmueller 2013, 749)</span>.</p>
<p><span class="citation">Stegmueller (2013)</span> finds that Bayesian method produces better multi-level-models than maximum likelihood methods for all numbers of groups.
ML methods do not suffer severe bias above 10-15 groups.
Bayesian point estimates are biased for smaller numbers of groups, but less than the ML.
Additionally, the Bayesian methods have better frequentist coverage than ML methods.</p>
<p><span class="citation">Beck and Katz (2007)</span> show that ML random coefficient models are superior in terms of efficiency to many types of pooled and un-pooled estimators in small samples.</p>
</div>
<div id="correlation-between-predictors-and-errors" class="section level3">
<h3><span class="header-section-number">20.12.2</span> Correlation between Predictors and Errors</h3>
<p><span class="citation">Bafumi and Gelman (2006)</span> analyze this case.</p>
<p>The standard suggestion in frequentist literature is to use a Hausman test
where the null hypothesis is that random effects are consistent. However,
<span class="citation">Clark and Linzer (2014)</span> note that in small samples this is likely to fail to reject
random effects; and in large samples, random effects behave like fixed effects
anyways.</p>
</div>
</div>
<div id="references-11" class="section level2">
<h2><span class="header-section-number">20.13</span> References</h2>
<p>Texts and chapters on multi-level analysis:</p>
<ul>
<li><p>Bayesian</p>
<ul>
<li><span class="citation">A. Gelman and Hill (2007 Ch. 11-17)</span>.</li>
<li><span class="citation">A. Gelman, Carlin, et al. (2013 Ch 5)</span> “Hierarchical Models”</li>
<li><span class="citation">A. Gelman, Carlin, et al. (2013 Ch 15)</span> “Hierarchical Linear Models”</li>
<li><span class="citation">Jackman (2009 CHh. 7)</span></li>
<li><span class="citation">Draper (2008)</span></li>
</ul></li>
<li><p>Frequentist</p>
<ul>
<li><span class="citation">Goldstein (2011)</span></li>
<li><span class="citation">Snijders and Bosker (2011)</span></li>
<li><span class="citation">Rabe-Hesketh and Skrondal (2012)</span></li>
<li><span class="citation">Jiang (2007)</span></li>
</ul></li>
</ul>
<p>Stan model examples:</p>
<ul>
<li>Stan models for <a href="https://github.com/stan-dev/example-models/wiki/ARM-Models">ARM</a></li>
<li><a href="http://mc-stan.org/documentation/case-studies/radon.html" class="uri">http://mc-stan.org/documentation/case-studies/radon.html</a></li>
<li><a href="https://biologyforfun.wordpress.com/2016/12/08/crossed-and-nested-hierarchical-models-with-stan-and-r/" class="uri">https://biologyforfun.wordpress.com/2016/12/08/crossed-and-nested-hierarchical-models-with-stan-and-r/</a></li>
</ul>
<p>Examples of multilevel models</p>
<ul>
<li><p><span class="citation">Western (1998)</span>: economic growth for OECD countries</p></li>
<li><p><span class="citation">Gelman and King (1993)</span>: US election polling</p></li>
<li><p><span class="citation">Park, Gelman, and Bafumi (2004)</span>: multi-level models of opinion polls combined with post-stratification to extrapolate national opinion surveys to regions.</p></li>
<li><p><span class="citation">Steenbergen and Jones (2002)</span>: mostly an intro/review of MLM, but uses the
cross-country Eurobarometer to model support for the EU</p></li>
<li><p><span class="citation">A. Gelman, Shor, et al. (2007)</span>: state-level opinion polls</p></li>
<li><p><span class="citation">Raudenbush and Bryk (2001)</span>: student performance with student and school-level indicators</p></li>
<li><p><span class="citation">Gilardi (2010)</span>: policy diffusion</p></li>
<li><p><span class="citation">O’Rourke and Sinnott (2006)</span>: attitudes toward immigration</p></li>
<li><p><span class="citation">Andersen and Fetner (2008)</span>: ethnic and social tolerance</p></li>
<li><p><span class="citation">Weldon (2006)</span>: ethnic and social tolerance</p></li>
<li><p><span class="citation">Arzheimer (2009)</span>: right-wing voting</p></li>
<li><p><span class="citation">Hooghe et al. (2009)</span>: social and political trust</p></li>
<li><p><span class="citation">Anderson and Singer (2008)</span>: satisfaction with democracy</p></li>
<li><p><span class="citation">Meer, Deth, and Scheepers (2009)</span>: political participation</p></li>
<li><p><span class="citation">Iversen and Rosenbluth (2006)</span>: political economy of the gender wage gap</p></li>
<li><p><span class="citation">Hooghe and Marks (2004)</span>: support for European integration</p></li>
<li><p><span class="citation">Lax and Phillips (2009)</span>: American politics using states and neighborhoods</p></li>
<li><p><span class="citation">Voeten (2008)</span>: judicial decision making</p></li>
<li><p><span class="citation">Franchino and Høyland (2009)</span>: legislative politics</p></li>
<li><p><span class="citation">Denisova et al. (2009)</span>: politics of economic reforms</p></li>
<li><p><span class="citation">Aitkin and Longford (1986)</span>, <span class="citation">Goldstein et al. (2000)</span>,
<span class="citation">Goldstein et al. (1993)</span>: education</p></li>
<li><p><span class="citation">Goldstein et al. (2000)</span>: medicine</p></li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="14">
<li id="fn14"><p><a href="https://en.wikipedia.org/wiki/Multilevel_model" class="uri">https://en.wikipedia.org/wiki/Multilevel_model</a><a href="multilevel-models.html#fnref14" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shrinkage-and-regularized-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/bayesian_notes/edit/master/multilevel.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Updating: A Set of Bayesian Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Updating: A Set of Bayesian Notes">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Updating: A Set of Bayesian Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://jrnold.github.io/bayesian_notes" />
  
  
  <meta name="github-repo" content="jrnold/bayesian_notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Updating: A Set of Bayesian Notes" />
  <meta name="twitter:site" content="@jrnld" />
  
  

<meta name="author" content="Jeffrey B. Arnold">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction-to-stan-and-linear-regression.html">
<link rel="next" href="generalized-linear-models.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>

\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\median}{median}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\logistic}{Logistic}
\DeclareMathOperator{\logit}{Logit}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

% This follows BDA
\newcommand{\dunif}{\mathrm{U}}
\newcommand{\dnorm}{\mathrm{N}}
\newcommand{\dlnorm}{\mathrm{lognormal}}
\newcommand{\dmvnorm}{\mathrm{N}}
\newcommand{\dgamma}{\mathrm{Gamma}}
\newcommand{\dinvgamma}{\mathrm{Inv-Gamma}}
\newcommand{\dchisq}[1]{\chi^2_{#1}}
\newcommand{\dinvchisq}[1]{\mathrm{Inv-}\chi^2_{#1}}
\newcommand{\dexp}{\mathrm{Expon}}
\newcommand{\dlaplace}{\mathrm{Laplace}}
\newcommand{\dweibull}{\mathrm{Weibull}}
\newcommand{\dwishart}[1]{\mathrm{Wishart}_{#1}}
\newcommand{\dinvwishart}[1]{\mathrm{Inv-Wishart}_{#1}}
\newcommand{\dlkj}{\mathrm{LkjCorr}}
\newcommand{\dt}[1]{t_{#1}}
\newcommand{\dbeta}{\mathrm{Beta}}
\newcommand{\ddirichlet}{\mathrm{Dirichlet}}
\newcommand{\dlogistic}{\mathrm{Logistic}}
\newcommand{\dllogistic}{\mathrm{Log-logistic}}
\newcommand{\dpois}{\mathrm{Poisson}}
\newcommand{\dbin}{\mathrm{Bin}}
\newcommand{\dmultinom}{\mathrm{Multinom}}
\newcommand{\dnbinom}{\mathrm{Neg-bin}}
\newcommand{\dnbinomalt}{\mathrm{Neg-bin2}}
\newcommand{\dbetabinom}{\mathrm{Beta-bin}}
\newcommand{\dcauchy}{\mathrm{Cauchy}}
\newcommand{\dhalfcauchy}{\mathrm{Cauchy}^{+}}
\newcommand{\dlkjcorr}{\mathrm{LKJ}^{+}}



\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Bayesian Notes</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Theory</b></span></li>
<li class="chapter" data-level="1" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>1</b> Bayesian Inference</a></li>
<li class="part"><span><b>II Computation</b></span></li>
<li class="chapter" data-level="2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>2</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>2.1</b> Monte Carlo Sampling</a></li>
<li class="chapter" data-level="2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo-sampling"><i class="fa fa-check"></i><b>2.2</b> Markov Chain Monte Carlo Sampling</a></li>
<li class="chapter" data-level="2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#references"><i class="fa fa-check"></i><b>2.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>3</b> MCMC Diagnostics</a><ul>
<li class="chapter" data-level="3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#reparameterize-models"><i class="fa fa-check"></i><b>3.1</b> Reparameterize Models</a></li>
<li class="chapter" data-level="3.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#convergence-diagnostics"><i class="fa fa-check"></i><b>3.2</b> Convergence Diagnostics</a><ul>
<li class="chapter" data-level="3.2.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#potential-scale-reduction-hatr"><i class="fa fa-check"></i><b>3.2.1</b> Potential Scale Reduction (<span class="math inline">\(\hat{R}\)</span>)</a></li>
<li class="chapter" data-level="3.2.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#references-1"><i class="fa fa-check"></i><b>3.2.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation-effective-sample-size-and-mcse"><i class="fa fa-check"></i><b>3.3</b> Autocorrelation, Effective Sample Size, and MCSE</a><ul>
<li class="chapter" data-level="3.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>3.3.1</b> Effective Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#thinning"><i class="fa fa-check"></i><b>3.4</b> Thinning</a><ul>
<li class="chapter" data-level="3.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#traceplots"><i class="fa fa-check"></i><b>3.4.1</b> Traceplots</a></li>
<li class="chapter" data-level="3.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#monte-carlo-standard-error-mcse"><i class="fa fa-check"></i><b>3.4.2</b> Monte Carlo Standard Error (MCSE)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#hmc-nut-specific-diagnostics"><i class="fa fa-check"></i><b>3.5</b> HMC-NUT Specific Diagnostics</a><ul>
<li class="chapter" data-level="3.5.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#divergent-transitions"><i class="fa fa-check"></i><b>3.5.1</b> Divergent transitions</a></li>
<li class="chapter" data-level="3.5.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#maximum-treedepth"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Treedepth</a></li>
<li class="chapter" data-level="3.5.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#bayesian-fraction-of-missing-information"><i class="fa fa-check"></i><b>3.5.3</b> Bayesian Fraction of Missing Information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="posterior-inference.html"><a href="posterior-inference.html"><i class="fa fa-check"></i><b>4</b> Posterior Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="posterior-inference.html"><a href="posterior-inference.html#prerequisites"><i class="fa fa-check"></i><b>4.1</b> Prerequisites</a></li>
<li class="chapter" data-level="4.2" data-path="posterior-inference.html"><a href="posterior-inference.html#introduction"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="posterior-inference.html"><a href="posterior-inference.html#functions-of-the-posterior-distribution"><i class="fa fa-check"></i><b>4.3</b> Functions of the Posterior Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="posterior-inference.html"><a href="posterior-inference.html#marginal-effects"><i class="fa fa-check"></i><b>4.4</b> Marginal Effects</a><ul>
<li class="chapter" data-level="4.4.1" data-path="posterior-inference.html"><a href="posterior-inference.html#example-marginal-effect-plot-for-x"><i class="fa fa-check"></i><b>4.4.1</b> Example: Marginal Effect Plot for X</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>5</b> Model Checking</a><ul>
<li class="chapter" data-level="5.1" data-path="model-checking.html"><a href="model-checking.html#why-check-models"><i class="fa fa-check"></i><b>5.1</b> Why check models?</a></li>
<li class="chapter" data-level="5.2" data-path="model-checking.html"><a href="model-checking.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>5.2</b> Posterior Predictive Checks</a><ul>
<li class="chapter" data-level="5.2.1" data-path="model-checking.html"><a href="model-checking.html#bayesian-p-values"><i class="fa fa-check"></i><b>5.2.1</b> Bayesian p-values</a></li>
<li class="chapter" data-level="5.2.2" data-path="model-checking.html"><a href="model-checking.html#test-quantities"><i class="fa fa-check"></i><b>5.2.2</b> Test quantities</a></li>
<li class="chapter" data-level="5.2.3" data-path="model-checking.html"><a href="model-checking.html#p-values-vs.u-values"><i class="fa fa-check"></i><b>5.2.3</b> p-values vs. u-values</a></li>
<li class="chapter" data-level="5.2.4" data-path="model-checking.html"><a href="model-checking.html#marginal-predictive-checks"><i class="fa fa-check"></i><b>5.2.4</b> Marginal predictive checks</a></li>
<li class="chapter" data-level="5.2.5" data-path="model-checking.html"><a href="model-checking.html#outliers"><i class="fa fa-check"></i><b>5.2.5</b> Outliers</a></li>
<li class="chapter" data-level="5.2.6" data-path="model-checking.html"><a href="model-checking.html#grapical-posterior-predictive-checks"><i class="fa fa-check"></i><b>5.2.6</b> Grapical Posterior Predictive Checks</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="model-checking.html"><a href="model-checking.html#sources"><i class="fa fa-check"></i><b>5.3</b> Sources</a></li>
</ul></li>
<li class="part"><span><b>III Models</b></span></li>
<li class="chapter" data-level="6" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Introduction to Stan and Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#prerequites"><i class="fa fa-check"></i><b>6.1</b> Prerequites</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#the-statistical-model"><i class="fa fa-check"></i><b>6.2</b> The Statistical Model</a><ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling"><i class="fa fa-check"></i><b>6.2.1</b> Sampling</a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#convergence-diagnostics-and-model-fit"><i class="fa fa-check"></i><b>6.2.2</b> Convergence Diagnostics and Model Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html"><i class="fa fa-check"></i><b>7</b> Heteroskedasticity and Robust Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#linear-regression-with-student-t-distributed-errors"><i class="fa fa-check"></i><b>7.2</b> Linear Regression with Student t distributed errors</a><ul>
<li class="chapter" data-level="7.2.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#double-exponential-laplace-errors"><i class="fa fa-check"></i><b>7.2.1</b> Double Exponential (Laplace) Errors</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#heteroskedasticity"><i class="fa fa-check"></i><b>7.3</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="7.3.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#covariates"><i class="fa fa-check"></i><b>7.3.1</b> Covariates</a></li>
<li class="chapter" data-level="7.3.2" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#student-t-error"><i class="fa fa-check"></i><b>7.3.2</b> Student-t Error</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#references-2"><i class="fa fa-check"></i><b>7.4</b> References</a><ul>
<li class="chapter" data-level="7.4.1" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#robust-regression"><i class="fa fa-check"></i><b>7.4.1</b> Robust regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#heteroskedasticity-1"><i class="fa fa-check"></i><b>7.4.2</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="7.4.3" data-path="heteroskedasticity-and-robust-regression.html"><a href="heteroskedasticity-and-robust-regression.html#qunatile-regression"><i class="fa fa-check"></i><b>7.4.3</b> Qunatile regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>8</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="8.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#generalized-linear-models-1"><i class="fa fa-check"></i><b>8.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="8.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-models"><i class="fa fa-check"></i><b>8.2</b> Count Models</a><ul>
<li class="chapter" data-level="8.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson"><i class="fa fa-check"></i><b>8.2.1</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example"><i class="fa fa-check"></i><b>8.3</b> Example</a></li>
<li class="chapter" data-level="8.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#negative-binomial"><i class="fa fa-check"></i><b>8.4</b> Negative Binomial</a><ul>
<li class="chapter" data-level="8.4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-3"><i class="fa fa-check"></i><b>8.4.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#multinomial-categorical-models"><i class="fa fa-check"></i><b>8.5</b> Multinomial / Categorical Models</a></li>
<li class="chapter" data-level="8.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#gamma-regression"><i class="fa fa-check"></i><b>8.6</b> Gamma Regression</a></li>
<li class="chapter" data-level="8.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#beta-regression"><i class="fa fa-check"></i><b>8.7</b> Beta Regression</a></li>
<li class="chapter" data-level="8.8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#ordered-logistic"><i class="fa fa-check"></i><b>8.8</b> Ordered Logistic</a></li>
<li class="chapter" data-level="8.9" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-4"><i class="fa fa-check"></i><b>8.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="binomial-models.html"><a href="binomial-models.html"><i class="fa fa-check"></i><b>9</b> Binomial Models</a><ul>
<li class="chapter" data-level="9.0.1" data-path="binomial-models.html"><a href="binomial-models.html#link-functions-link-function"><i class="fa fa-check"></i><b>9.0.1</b> Link Functions {link-function}</a></li>
<li class="chapter" data-level="9.0.2" data-path="binomial-models.html"><a href="binomial-models.html#stan"><i class="fa fa-check"></i><b>9.0.2</b> Stan</a></li>
<li class="chapter" data-level="9.0.3" data-path="binomial-models.html"><a href="binomial-models.html#example-vote-turnout"><i class="fa fa-check"></i><b>9.0.3</b> Example: Vote Turnout</a></li>
<li class="chapter" data-level="9.0.4" data-path="binomial-models.html"><a href="binomial-models.html#separation"><i class="fa fa-check"></i><b>9.0.4</b> Separation</a></li>
<li class="chapter" data-level="9.1" data-path="binomial-models.html"><a href="binomial-models.html#rare-events-logit"><i class="fa fa-check"></i><b>9.1</b> Rare Events Logit</a></li>
<li class="chapter" data-level="9.2" data-path="binomial-models.html"><a href="binomial-models.html#case-control"><i class="fa fa-check"></i><b>9.2</b> Case Control</a><ul>
<li class="chapter" data-level="9.2.1" data-path="binomial-models.html"><a href="binomial-models.html#references-6"><i class="fa fa-check"></i><b>9.2.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html"><i class="fa fa-check"></i><b>10</b> Unbounded Count Models</a><ul>
<li class="chapter" data-level="10.1" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#poisson-1"><i class="fa fa-check"></i><b>10.1</b> Poisson</a></li>
<li class="chapter" data-level="10.2" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#negative-binomial-1"><i class="fa fa-check"></i><b>10.2</b> Negative Binomial</a><ul>
<li class="chapter" data-level="10.2.1" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#stan-1"><i class="fa fa-check"></i><b>10.2.1</b> Stan</a></li>
<li class="chapter" data-level="10.2.2" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#example-number-of-number-o"><i class="fa fa-check"></i><b>10.2.2</b> Example: Number of Number o</a></li>
<li class="chapter" data-level="10.2.3" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#references-7"><i class="fa fa-check"></i><b>10.2.3</b> References</a></li>
<li class="chapter" data-level="10.2.4" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#link-functions"><i class="fa fa-check"></i><b>10.2.4</b> Link functions</a></li>
<li class="chapter" data-level="10.2.5" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#stan-2"><i class="fa fa-check"></i><b>10.2.5</b> Stan</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#example-bilateral-sanctions"><i class="fa fa-check"></i><b>10.3</b> Example: Bilateral Sanctions</a></li>
<li class="chapter" data-level="10.4" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#negative-binomial-2"><i class="fa fa-check"></i><b>10.4</b> Negative Binomial</a><ul>
<li class="chapter" data-level="10.4.1" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#example-economic-sanctions-ii-ex-econ-sanctions-2"><i class="fa fa-check"></i><b>10.4.1</b> Example: Economic Sanctions II {ex-econ-sanctions-2}</a></li>
<li class="chapter" data-level="10.4.2" data-path="unbounded-count-models.html"><a href="unbounded-count-models.html#references-8"><i class="fa fa-check"></i><b>10.4.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="categorical-variables.html"><a href="categorical-variables.html"><i class="fa fa-check"></i><b>11</b> Categorical Variables</a><ul>
<li class="chapter" data-level="11.1" data-path="categorical-variables.html"><a href="categorical-variables.html#example-mexico-vote-choice"><i class="fa fa-check"></i><b>11.1</b> Example: Mexico Vote Choice</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ordered-categorical-outcomes.html"><a href="ordered-categorical-outcomes.html"><i class="fa fa-check"></i><b>12</b> Ordered Categorical Outcomes</a></li>
<li class="chapter" data-level="13" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html"><i class="fa fa-check"></i><b>13</b> Shrinkage and Regularization</a><ul>
<li class="chapter" data-level="13.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#normal-linear-regression-model"><i class="fa fa-check"></i><b>13.1</b> Normal Linear Regression Model</a></li>
<li class="chapter" data-level="13.2" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#penalized-regression"><i class="fa fa-check"></i><b>13.2</b> Penalized Regression</a><ul>
<li class="chapter" data-level="13.2.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#ridge-regression"><i class="fa fa-check"></i><b>13.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="13.2.2" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#lasso"><i class="fa fa-check"></i><b>13.2.2</b> Lasso</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#bayesian-shrinkage-priors"><i class="fa fa-check"></i><b>13.3</b> Bayesian Shrinkage Priors</a></li>
<li class="chapter" data-level="13.4" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#differences-between-bayesian-shrinkage-and-penalized-likelihood"><i class="fa fa-check"></i><b>13.4</b> Differences between Bayesian Shrinkage and Penalized Likelihood</a></li>
<li class="chapter" data-level="13.5" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#hierarchical-shrinkage-priors"><i class="fa fa-check"></i><b>13.5</b> Hierarchical Shrinkage Priors</a></li>
<li class="chapter" data-level="13.6" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#example-1"><i class="fa fa-check"></i><b>13.6</b> Example</a><ul>
<li class="chapter" data-level="13.6.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#double-exponential-laplace-prior"><i class="fa fa-check"></i><b>13.6.1</b> Double Exponential (Laplace) Prior</a></li>
<li class="chapter" data-level="13.6.2" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#hierarchical-prior-hs"><i class="fa fa-check"></i><b>13.6.2</b> Hierarchical Prior (HS)</a></li>
<li class="chapter" data-level="13.6.3" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#comparison"><i class="fa fa-check"></i><b>13.6.3</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#shrinkage-parameters"><i class="fa fa-check"></i><b>13.7</b> Shrinkage Parameters</a></li>
<li class="chapter" data-level="13.8" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#choice-of-hyperparameter-on-tau"><i class="fa fa-check"></i><b>13.8</b> Choice of Hyperparameter on <span class="math inline">\(\tau\)</span></a></li>
<li class="chapter" data-level="13.9" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#r-implementations"><i class="fa fa-check"></i><b>13.9</b> R Implementations</a></li>
<li class="chapter" data-level="13.10" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>13.10</b> Bayesian Model Averaging</a><ul>
<li class="chapter" data-level="13.10.1" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#zellners-g-prior"><i class="fa fa-check"></i><b>13.10.1</b> Zellner’s g-prior</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#slab-and-spike-priors"><i class="fa fa-check"></i><b>13.11</b> Slab and Spike Priors</a></li>
<li class="chapter" data-level="13.12" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#technical-notes"><i class="fa fa-check"></i><b>13.12</b> Technical Notes</a></li>
<li class="chapter" data-level="13.13" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#multiple-comparisons-and-thresholding-rules"><i class="fa fa-check"></i><b>13.13</b> Multiple Comparisons and Thresholding rules</a></li>
<li class="chapter" data-level="13.14" data-path="shrinkage-regularization.html"><a href="shrinkage-regularization.html#examples-of-applications-of-sensitivity-analysis"><i class="fa fa-check"></i><b>13.14</b> Examples of Applications of Sensitivity Analysis</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hierarchical-models.html"><a href="hierarchical-models.html"><i class="fa fa-check"></i><b>14</b> Hierarchical Models</a><ul>
<li class="chapter" data-level="14.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#example-baseball-hits"><i class="fa fa-check"></i><b>14.1</b> Example: Baseball Hits</a><ul>
<li class="chapter" data-level="14.1.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#other-examples"><i class="fa fa-check"></i><b>14.1.1</b> Other Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="multilevel-models.html"><a href="multilevel-models.html"><i class="fa fa-check"></i><b>15</b> Multilevel Models</a><ul>
<li class="chapter" data-level="15.1" data-path="multilevel-models.html"><a href="multilevel-models.html#example-radon"><i class="fa fa-check"></i><b>15.1</b> Example: Radon</a><ul>
<li class="chapter" data-level="15.1.1" data-path="multilevel-models.html"><a href="multilevel-models.html#data"><i class="fa fa-check"></i><b>15.1.1</b> Data</a></li>
<li class="chapter" data-level="15.1.2" data-path="multilevel-models.html"><a href="multilevel-models.html#varying-intercepts-models"><i class="fa fa-check"></i><b>15.1.2</b> Varying Intercepts Models</a></li>
<li class="chapter" data-level="15.1.3" data-path="multilevel-models.html"><a href="multilevel-models.html#varying-intercept-model"><i class="fa fa-check"></i><b>15.1.3</b> Varying Intercept Model</a></li>
<li class="chapter" data-level="15.1.4" data-path="multilevel-models.html"><a href="multilevel-models.html#varying-slope-model"><i class="fa fa-check"></i><b>15.1.4</b> Varying Slope Model</a></li>
<li class="chapter" data-level="15.1.5" data-path="multilevel-models.html"><a href="multilevel-models.html#group-level-predictors"><i class="fa fa-check"></i><b>15.1.5</b> Group Level Predictors</a></li>
<li class="chapter" data-level="15.1.6" data-path="multilevel-models.html"><a href="multilevel-models.html#lme4"><i class="fa fa-check"></i><b>15.1.6</b> lme4</a></li>
<li class="chapter" data-level="15.1.7" data-path="multilevel-models.html"><a href="multilevel-models.html#rstanarm"><i class="fa fa-check"></i><b>15.1.7</b> rstanarm</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="multilevel-models.html"><a href="multilevel-models.html#pooling-of-hierarchical-parameters"><i class="fa fa-check"></i><b>15.2</b> Pooling of Hierarchical Parameters</a></li>
<li class="chapter" data-level="15.3" data-path="multilevel-models.html"><a href="multilevel-models.html#anova"><i class="fa fa-check"></i><b>15.3</b> ANOVA</a></li>
<li class="chapter" data-level="15.4" data-path="multilevel-models.html"><a href="multilevel-models.html#time-series-cross-section"><i class="fa fa-check"></i><b>15.4</b> Time-Series Cross Section</a></li>
<li class="chapter" data-level="15.5" data-path="multilevel-models.html"><a href="multilevel-models.html#extensions"><i class="fa fa-check"></i><b>15.5</b> Extensions</a></li>
<li class="chapter" data-level="15.6" data-path="multilevel-models.html"><a href="multilevel-models.html#miscellaneous"><i class="fa fa-check"></i><b>15.6</b> Miscellaneous</a><ul>
<li class="chapter" data-level="15.6.1" data-path="multilevel-models.html"><a href="multilevel-models.html#how-many-groups"><i class="fa fa-check"></i><b>15.6.1</b> How many groups?</a></li>
<li class="chapter" data-level="15.6.2" data-path="multilevel-models.html"><a href="multilevel-models.html#correlation-between-predictors-and-errors"><i class="fa fa-check"></i><b>15.6.2</b> Correlation between Predictors and Errors</a></li>
<li class="chapter" data-level="15.6.3" data-path="multilevel-models.html"><a href="multilevel-models.html#comparison-to-other-panel-methods"><i class="fa fa-check"></i><b>15.6.3</b> Comparison to Other Panel Methods</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="multilevel-models.html"><a href="multilevel-models.html#references-9"><i class="fa fa-check"></i><b>15.7</b> References</a></li>
</ul></li>
<li class="part"><span><b>IV Appendix</b></span><ul>
<li class="chapter" data-level="15.8" data-path="multilevel-models.html"><a href="multilevel-models.html#parameters"><i class="fa fa-check"></i><b>15.8</b> Parameters</a></li>
<li class="chapter" data-level="15.9" data-path="multilevel-models.html"><a href="multilevel-models.html#miscellaneous-mathematical-background"><i class="fa fa-check"></i><b>15.9</b> Miscellaneous Mathematical Background</a><ul>
<li class="chapter" data-level="15.9.1" data-path="multilevel-models.html"><a href="multilevel-models.html#location-scale-families"><i class="fa fa-check"></i><b>15.9.1</b> Location-Scale Families</a></li>
<li class="chapter" data-level="15.9.2" data-path="multilevel-models.html"><a href="multilevel-models.html#scale-mixtures-of-normal-distributions"><i class="fa fa-check"></i><b>15.9.2</b> Scale Mixtures of Normal Distributions</a></li>
<li class="chapter" data-level="15.9.3" data-path="multilevel-models.html"><a href="multilevel-models.html#covariance-correlation-matrix-decomposition"><i class="fa fa-check"></i><b>15.9.3</b> Covariance-Correlation Matrix Decomposition</a></li>
<li class="chapter" data-level="15.9.4" data-path="multilevel-models.html"><a href="multilevel-models.html#qr-factorization"><i class="fa fa-check"></i><b>15.9.4</b> QR Factorization</a></li>
<li class="chapter" data-level="15.9.5" data-path="multilevel-models.html"><a href="multilevel-models.html#cholesky-decomposition"><i class="fa fa-check"></i><b>15.9.5</b> Cholesky Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>16</b> Notes</a><ul>
<li class="chapter" data-level="16.1" data-path="notes.html"><a href="notes.html#syllabi"><i class="fa fa-check"></i><b>16.1</b> Syllabi</a></li>
<li class="chapter" data-level="16.2" data-path="notes.html"><a href="notes.html#textbooks"><i class="fa fa-check"></i><b>16.2</b> Textbooks</a></li>
<li class="chapter" data-level="16.3" data-path="notes.html"><a href="notes.html#topics"><i class="fa fa-check"></i><b>16.3</b> Topics</a><ul>
<li class="chapter" data-level="16.3.1" data-path="notes.html"><a href="notes.html#overviews"><i class="fa fa-check"></i><b>16.3.1</b> Overviews</a></li>
<li class="chapter" data-level="16.3.2" data-path="notes.html"><a href="notes.html#bayesian-philosophy"><i class="fa fa-check"></i><b>16.3.2</b> Bayesian Philosophy</a></li>
<li class="chapter" data-level="16.3.3" data-path="notes.html"><a href="notes.html#bayesian-frequentist-debates"><i class="fa fa-check"></i><b>16.3.3</b> Bayesian Frequentist Debates</a></li>
<li class="chapter" data-level="16.3.4" data-path="notes.html"><a href="notes.html#categorical"><i class="fa fa-check"></i><b>16.3.4</b> Categorical</a></li>
<li class="chapter" data-level="16.3.5" data-path="notes.html"><a href="notes.html#identifiability"><i class="fa fa-check"></i><b>16.3.5</b> Identifiability</a></li>
<li class="chapter" data-level="16.3.6" data-path="notes.html"><a href="notes.html#time-series"><i class="fa fa-check"></i><b>16.3.6</b> Time Series</a></li>
<li class="chapter" data-level="16.3.7" data-path="notes.html"><a href="notes.html#topic-models"><i class="fa fa-check"></i><b>16.3.7</b> Topic Models</a></li>
<li class="chapter" data-level="16.3.8" data-path="notes.html"><a href="notes.html#nonparametric-bayesian-methods"><i class="fa fa-check"></i><b>16.3.8</b> Nonparametric Bayesian Methods</a></li>
<li class="chapter" data-level="16.3.9" data-path="notes.html"><a href="notes.html#prior-elicitation"><i class="fa fa-check"></i><b>16.3.9</b> Prior Elicitation</a></li>
<li class="chapter" data-level="16.3.10" data-path="notes.html"><a href="notes.html#variable-selection"><i class="fa fa-check"></i><b>16.3.10</b> Variable Selection</a></li>
<li class="chapter" data-level="16.3.11" data-path="notes.html"><a href="notes.html#shrinkage"><i class="fa fa-check"></i><b>16.3.11</b> Shrinkage</a></li>
<li class="chapter" data-level="16.3.12" data-path="notes.html"><a href="notes.html#applied-bayes-rule"><i class="fa fa-check"></i><b>16.3.12</b> Applied Bayes Rule</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="notes.html"><a href="notes.html#computation-methods"><i class="fa fa-check"></i><b>16.4</b> Computation Methods</a><ul>
<li class="chapter" data-level="16.4.1" data-path="notes.html"><a href="notes.html#software"><i class="fa fa-check"></i><b>16.4.1</b> Software</a></li>
<li class="chapter" data-level="16.4.2" data-path="notes.html"><a href="notes.html#stan-3"><i class="fa fa-check"></i><b>16.4.2</b> Stan</a></li>
<li class="chapter" data-level="16.4.3" data-path="notes.html"><a href="notes.html#diagrams"><i class="fa fa-check"></i><b>16.4.3</b> Diagrams</a></li>
<li class="chapter" data-level="16.4.4" data-path="notes.html"><a href="notes.html#political-science-bayesian-works"><i class="fa fa-check"></i><b>16.4.4</b> Political Science Bayesian Works</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="notes.html"><a href="notes.html#model-checking-1"><i class="fa fa-check"></i><b>16.5</b> Model Checking</a></li>
<li class="chapter" data-level="16.6" data-path="notes.html"><a href="notes.html#general-applications-and-models"><i class="fa fa-check"></i><b>16.6</b> General Applications and Models</a><ul>
<li class="chapter" data-level="16.6.1" data-path="notes.html"><a href="notes.html#mixed-methods-and-qualitative-research"><i class="fa fa-check"></i><b>16.6.1</b> Mixed Methods and Qualitative Research</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="notes.html"><a href="notes.html#hierarchical-modeling"><i class="fa fa-check"></i><b>16.7</b> Hierarchical Modeling</a></li>
<li class="chapter" data-level="16.8" data-path="notes.html"><a href="notes.html#shrinkageregularization"><i class="fa fa-check"></i><b>16.8</b> Shrinkage/Regularization</a><ul>
<li class="chapter" data-level="16.8.1" data-path="notes.html"><a href="notes.html#examples"><i class="fa fa-check"></i><b>16.8.1</b> Examples</a></li>
<li class="chapter" data-level="16.8.2" data-path="notes.html"><a href="notes.html#latent-variable-models"><i class="fa fa-check"></i><b>16.8.2</b> Latent Variable Models</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="notes.html"><a href="notes.html#bayes-theorem-examples"><i class="fa fa-check"></i><b>16.9</b> Bayes Theorem Examples</a><ul>
<li class="chapter" data-level="16.9.1" data-path="notes.html"><a href="notes.html#miscallaneous"><i class="fa fa-check"></i><b>16.9.1</b> Miscallaneous</a></li>
<li class="chapter" data-level="16.9.2" data-path="notes.html"><a href="notes.html#german-tank-problem"><i class="fa fa-check"></i><b>16.9.2</b> German Tank Problem</a></li>
</ul></li>
<li class="chapter" data-level="16.10" data-path="notes.html"><a href="notes.html#good-turing-estimator"><i class="fa fa-check"></i><b>16.10</b> Good-Turing Estimator</a></li>
<li class="chapter" data-level="16.11" data-path="notes.html"><a href="notes.html#reproducibility"><i class="fa fa-check"></i><b>16.11</b> Reproducibility</a><ul>
<li class="chapter" data-level="16.11.1" data-path="notes.html"><a href="notes.html#uncategorized"><i class="fa fa-check"></i><b>16.11.1</b> Uncategorized</a></li>
</ul></li>
<li class="chapter" data-level="16.12" data-path="notes.html"><a href="notes.html#empirical-bayes"><i class="fa fa-check"></i><b>16.12</b> Empirical Bayes</a></li>
<li class="chapter" data-level="16.13" data-path="notes.html"><a href="notes.html#things-to-cover"><i class="fa fa-check"></i><b>16.13</b> Things to cover</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-10.html"><a href="references-10.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Updating: A Set of Bayesian Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="heteroskedasticity-and-robust-regression" class="section level1">
<h1><span class="header-section-number">7</span> Heteroskedasticity and Robust Regression</h1>
<div id="prerequisites-1" class="section level2">
<h2><span class="header-section-number">7.1</span> Prerequisites</h2>
<p><strong><a href="https://cran.r-project.org/package=VGAM">VGAM</a></strong> is needed for the Laplace distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;VGAM&quot;</span>)</code></pre></div>
</div>
<div id="linear-regression-with-student-t-distributed-errors" class="section level2">
<h2><span class="header-section-number">7.2</span> Linear Regression with Student t distributed errors</h2>
<p>Like OLS, Bayesian linear regression with normally distributed errors is sensitive to outliers. The normal distribution has narrow tail probabilities.</p>
<p>This plots the normal, Double Exponential (Laplace), and Student-t (df = 4) distributions all with mean 0 and scale 1, and the surprise (<span class="math inline">\(- log(p)\)</span>) at each point. Higher surprise is a lower log-likelihood. Both the Student-t and Double Exponential distributions have surprise values well below the normal in the ranges (-6, 6).<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> This means that outliers impose less of a penalty on the log-posterior models using these distributions, and the regression line would need to move less to incorporate those observations since the error distribution will not consider them as unusual.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dt">length.out =</span> <span class="dv">100</span>)
<span class="kw">bind_rows</span>(
  <span class="kw">tibble</span>(<span class="dt">z =</span> z,
         <span class="dt">p =</span> <span class="kw">dnorm</span>(z, <span class="dv">0</span>, <span class="dv">1</span>),
         <span class="dt">distr =</span> <span class="st">&quot;Normal&quot;</span>),
  <span class="kw">tibble</span>(<span class="dt">z =</span> z,
         <span class="dt">p =</span> <span class="kw">dt</span>(z, <span class="dv">4</span>),
         <span class="dt">distr =</span> <span class="st">&quot;Student-t (df = 4)&quot;</span>),
  <span class="kw">tibble</span>(<span class="dt">z =</span> z,
         <span class="dt">p =</span> VGAM<span class="op">::</span><span class="kw">dlaplace</span>(z, <span class="dv">0</span>, <span class="dv">1</span>),
         <span class="dt">distr =</span> <span class="st">&quot;Double Exponential&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">-log(p)</span><span class="st">`</span> =<span class="st"> </span><span class="op">-</span><span class="kw">log</span>(p)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> z, <span class="dt">y =</span> <span class="st">`</span><span class="dt">-log(p)</span><span class="st">`</span>, <span class="dt">colour =</span> distr)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>()
      </code></pre></div>
<p><img src="robust_files/figure-html/unnamed-chunk-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dt">length.out =</span> <span class="dv">100</span>)
<span class="kw">bind_rows</span>(
  <span class="kw">tibble</span>(<span class="dt">z =</span> z,
         <span class="dt">p =</span> <span class="kw">dnorm</span>(z, <span class="dv">0</span>, <span class="dv">1</span>),
         <span class="dt">distr =</span> <span class="st">&quot;Normal&quot;</span>),
  <span class="kw">tibble</span>(<span class="dt">z =</span> z,
         <span class="dt">p =</span> <span class="kw">dt</span>(z, <span class="dv">4</span>),
         <span class="dt">distr =</span> <span class="st">&quot;Student-t (df = 4)&quot;</span>),
  <span class="kw">tibble</span>(<span class="dt">z =</span> z,
         <span class="dt">p =</span> VGAM<span class="op">::</span><span class="kw">dlaplace</span>(z, <span class="dv">0</span>, <span class="dv">1</span>),
         <span class="dt">distr =</span> <span class="st">&quot;Double Exponential&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">-log(p)</span><span class="st">`</span> =<span class="st"> </span><span class="op">-</span><span class="kw">log</span>(p)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> z, <span class="dt">y =</span> p, <span class="dt">colour =</span> distr)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>()
      </code></pre></div>
<p><img src="robust_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_t</code></pre></div>
<pre>
  <code class="stan">data {
  // number of observations
  int n;
  // response vector
  vector[n] y;
  // number of columns in the design matrix X
  int k;
  // design matrix X
  matrix [n, k] X;
  // beta prior
  real b_loc;
  real<lower = 0.0> b_scale;
  // sigma prior
  real sigma_scale;
}
parameters {
  // regression coefficient vector
  vector[k] b;
  // scale of the regression errors
  real<lower = 0.0> sigma;
  real<lower = 1.0> nu;
}
transformed parameters {
  // mu is the observation fitted/predicted value
  // also called yhat
  vector[n] mu;
  mu = X * b;
}
model {
  // priors
  b ~ normal(b_loc, b_scale);
  sigma ~ cauchy(0, sigma_scale);
  nu ~ gamma(2, 0.1);
  // likelihood
  y ~ student_t(nu, mu, sigma);
}
generated quantities {
  // simulate data from the posterior
  vector[n] y_rep;
  // log-likelihood values
  vector[n] log_lik;
  for (i in 1:n) {
    y_rep[i] = student_t_rng(nu, mu[i], sigma);
    log_lik[i] = student_t_lpdf(y[i] | nu, mu[i], sigma);
  }

}</code>
</pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">unionization &lt;-<span class="st"> </span><span class="kw">read_tsv</span>(<span class="st">&quot;data/western1995/unionization.tsv&quot;</span>,
         <span class="dt">col_types =</span> <span class="kw">cols</span>(
              <span class="dt">country =</span> <span class="kw">col_character</span>(),
              <span class="dt">union_density =</span> <span class="kw">col_double</span>(),
              <span class="dt">left_government =</span> <span class="kw">col_double</span>(),
              <span class="dt">labor_force_size =</span> <span class="kw">col_number</span>(),
              <span class="dt">econ_conc =</span> <span class="kw">col_double</span>()
            ))
mod_data &lt;-<span class="st"> </span><span class="kw">lm_preprocess</span>(union_density <span class="op">~</span><span class="st"> </span>left_government <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(labor_force_size) <span class="op">+</span><span class="st"> </span>econ_conc, <span class="dt">data =</span> unionization)
                                   
mod_data &lt;-<span class="st"> </span><span class="kw">within</span>(mod_data, {
  b_loc &lt;-<span class="st"> </span><span class="dv">0</span>
  b_scale &lt;-<span class="st"> </span><span class="dv">1000</span>
  sigma_scale &lt;-<span class="st"> </span><span class="kw">sd</span>(y)
})</code></pre></div>
<p>The <code>max_treedepth</code> parameter needed to be increased because in some runs it was hitting the maximum tree depth. This is likely due to the wide tails of the Student t distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_t_fit &lt;-<span class="st"> </span><span class="kw">sampling</span>(mod_t, <span class="dt">data =</span> mod_data, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">max_treedepth =</span> <span class="dv">11</span>))
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 1).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 3.7e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.916274 seconds (Warm-up)</span>
<span class="co">#&gt;                0.793757 seconds (Sampling)</span>
<span class="co">#&gt;                1.71003 seconds (Total)</span>
<span class="co">#&gt; The following numerical problems occurred the indicated number of times on chain 1</span>
<span class="co">#&gt;                                                                                          count</span>
<span class="co">#&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite!     1</span>
<span class="co">#&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected.</span>
<span class="co">#&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</span>
<span class="co">#&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 2).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 1.5e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.895086 seconds (Warm-up)</span>
<span class="co">#&gt;                0.851101 seconds (Sampling)</span>
<span class="co">#&gt;                1.74619 seconds (Total)</span>
<span class="co">#&gt; The following numerical problems occurred the indicated number of times on chain 2</span>
<span class="co">#&gt;                                                                                          count</span>
<span class="co">#&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite!     1</span>
<span class="co">#&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected.</span>
<span class="co">#&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</span>
<span class="co">#&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 3).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 1.4e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.959192 seconds (Warm-up)</span>
<span class="co">#&gt;                0.806953 seconds (Sampling)</span>
<span class="co">#&gt;                1.76615 seconds (Total)</span>
<span class="co">#&gt; The following numerical problems occurred the indicated number of times on chain 3</span>
<span class="co">#&gt;                                                                                     count</span>
<span class="co">#&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is 0, but must be &gt; 0!     1</span>
<span class="co">#&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected.</span>
<span class="co">#&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</span>
<span class="co">#&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 4).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 1.4e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.883576 seconds (Warm-up)</span>
<span class="co">#&gt;                0.731602 seconds (Sampling)</span>
<span class="co">#&gt;                1.61518 seconds (Total)</span>
<span class="co">#&gt; The following numerical problems occurred the indicated number of times on chain 4</span>
<span class="co">#&gt;                                                                                          count</span>
<span class="co">#&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite!     1</span>
<span class="co">#&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected.</span>
<span class="co">#&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</span>
<span class="co">#&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users.</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod_t_fit, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b&quot;</span>))<span class="op">$</span>summary
<span class="co">#&gt;        mean se_mean     sd    2.5%    25%    50%     75%  97.5% n_eff Rhat</span>
<span class="co">#&gt; b[1] 90.924 2.19841 66.781 -44.196  47.81 91.762 133.164 223.22   923    1</span>
<span class="co">#&gt; b[2]  0.273 0.00162  0.083   0.103   0.22  0.275   0.328   0.43  2626    1</span>
<span class="co">#&gt; b[3] -6.082 0.13953  4.322 -14.791  -8.92 -6.101  -3.263   2.57   959    1</span>
<span class="co">#&gt; b[4]  2.763 0.74224 22.668 -43.434 -11.60  2.445  17.292  48.50   933    1</span></code></pre></div>
<p>Compare those results when using a model with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_normal</code></pre></div>
<pre>
  <code class="stan">data {
  // number of observations
  int n;
  // response vector
  vector[n] y;
  // number of columns in the design matrix X
  int k;
  // design matrix X
  matrix [n, k] X;
  // beta prior
  real b_loc;
  real<lower = 0.0> b_scale;
  // sigma prior
  real sigma_scale;
}
parameters {
  // regression coefficient vector
  vector[k] b;
  // scale of the regression errors
  real<lower = 0.0> sigma;
}
transformed parameters {
  // mu is the observation fitted/predicted value
  // also called yhat
  vector[n] mu;
  mu = X * b;
}
model {
  // priors
  b ~ normal(b_loc, b_scale);
  sigma ~ cauchy(0, sigma_scale);
  // likelihood
  y ~ normal(mu, sigma);
}
generated quantities {
  // simulate data from the posterior
  vector[n] y_rep;
  // log-likelihood posterior
  vector[n] log_lik;
  for (i in 1:n) {
    y_rep[i] = normal_rng(mu[i], sigma);
    log_lik[i] = normal_lpdf(y[i] | mu[i], sigma);
  }
}</code>
</pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_normal_fit &lt;-<span class="st"> </span><span class="kw">sampling</span>(mod_normal, <span class="dt">data =</span> mod_data)
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 1).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 2.7e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.57835 seconds (Warm-up)</span>
<span class="co">#&gt;                0.504448 seconds (Sampling)</span>
<span class="co">#&gt;                1.0828 seconds (Total)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 2).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 1.2e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.533096 seconds (Warm-up)</span>
<span class="co">#&gt;                0.469512 seconds (Sampling)</span>
<span class="co">#&gt;                1.00261 seconds (Total)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 3).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 1.1e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.516238 seconds (Warm-up)</span>
<span class="co">#&gt;                0.602928 seconds (Sampling)</span>
<span class="co">#&gt;                1.11917 seconds (Total)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 4).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Gradient evaluation took 1.9e-05 seconds</span>
<span class="co">#&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.</span>
<span class="co">#&gt; Adjust your expectations accordingly!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="co">#&gt; Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="co">#&gt; Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="co">#&gt; Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Elapsed Time: 0.525881 seconds (Warm-up)</span>
<span class="co">#&gt;                0.422686 seconds (Sampling)</span>
<span class="co">#&gt;                0.948567 seconds (Total)</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod_normal_fit, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b&quot;</span>))<span class="op">$</span>summary
<span class="co">#&gt;        mean se_mean      sd     2.5%     25%    50%     75%   97.5% n_eff</span>
<span class="co">#&gt; b[1] 95.987 2.41105 63.9174 -27.7095  53.147 95.592 137.966 223.008   703</span>
<span class="co">#&gt; b[2]  0.270 0.00194  0.0842   0.0979   0.217  0.273   0.326   0.434  1891</span>
<span class="co">#&gt; b[3] -6.356 0.15521  4.2031 -14.8646  -9.116 -6.339  -3.532   1.768   733</span>
<span class="co">#&gt; b[4]  0.858 0.80219 21.5103 -41.1619 -13.309  0.972  15.252  43.658   719</span>
<span class="co">#&gt;      Rhat</span>
<span class="co">#&gt; b[1] 1.01</span>
<span class="co">#&gt; b[2] 1.00</span>
<span class="co">#&gt; b[3] 1.01</span>
<span class="co">#&gt; b[4] 1.01</span></code></pre></div>
<div id="double-exponential-laplace-errors" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Double Exponential (Laplace) Errors</h3>
<p>An alternative form of “robust” regression is to use the Double Exponential (Laplace) distributions for the errors.</p>
<p>This is the equivalent to least median regression, where the regression line is the median (50% quantile)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_dbl_exp</code></pre></div>
<pre>
  <code class="stan">data {
  // number of observations
  int n;
  // response vector
  vector[n] y;
  // number of columns in the design matrix X
  int k;
  // design matrix X
  matrix [n, k] X;
  // beta prior
  real b_loc;
  real<lower = 0.0> b_scale;
  // sigma prior
  real sigma_scale;
}
parameters {
  // regression coefficient vector
  vector[k] b;
  // scale of the regression errors
  real<lower = 0.0> sigma;
}
transformed parameters {
  // mu is the observation fitted/predicted value
  vector[n] mu;
  // tau are obs-level scale params
  mu = X * b;
}
model {
  // priors
  b ~ normal(b_loc, b_scale);
  sigma ~ cauchy(0, sigma_scale);
  // likelihood
  y ~ double_exponential(mu, sigma);
}
generated quantities {
  // simulate data from the posterior
  vector[n] y_rep;
  // log-likelihood values
  vector[n] log_lik;
  // use a single loop since both y_rep and log_lik are elementwise
  for (i in 1:n) {
    y_rep[i] = double_exponential_rng(mu[i], sigma);
    log_lik[i] = double_exponential_lpdf(y[i] | mu[i], sigma);
  }

}</code>
</pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod_dbl_exp_fit, <span class="dt">par =</span> <span class="kw">c</span>(<span class="st">&quot;b&quot;</span>))<span class="op">$</span>summary
<span class="co">#&gt;        mean se_mean      sd    2.5%    25%    50%     75%   97.5% n_eff</span>
<span class="co">#&gt; b[1] 60.671 2.58294 72.5324 -84.418 15.896 56.321 105.938 207.022   789</span>
<span class="co">#&gt; b[2]  0.298 0.00217  0.0815   0.126  0.248  0.304   0.354   0.442  1415</span>
<span class="co">#&gt; b[3] -4.303 0.15685  4.4911 -13.482 -7.160 -4.100  -1.555   4.641   820</span>
<span class="co">#&gt; b[4] 13.381 0.91373 25.6984 -38.570 -2.215 14.588  29.270  64.348   791</span>
<span class="co">#&gt;      Rhat</span>
<span class="co">#&gt; b[1]    1</span>
<span class="co">#&gt; b[2]    1</span>
<span class="co">#&gt; b[3]    1</span>
<span class="co">#&gt; b[4]    1</span></code></pre></div>
<p>Model comparison</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loo_t &lt;-<span class="st"> </span><span class="kw">loo</span>(<span class="kw">extract_log_lik</span>(mod_normal_fit, <span class="st">&quot;log_lik&quot;</span>))
<span class="co">#&gt; Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k-</span>
<span class="co">#&gt; diagnostic&#39;) for details.</span>
loo_normal &lt;-<span class="st"> </span><span class="kw">loo</span>(<span class="kw">extract_log_lik</span>(mod_t_fit, <span class="st">&quot;log_lik&quot;</span>))
<span class="co">#&gt; Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k-</span>
<span class="co">#&gt; diagnostic&#39;) for details.</span>
loo_dbl_exp &lt;-<span class="st"> </span><span class="kw">loo</span>(<span class="kw">extract_log_lik</span>(mod_dbl_exp_fit, <span class="st">&quot;log_lik&quot;</span>))</code></pre></div>
</div>
</div>
<div id="heteroskedasticity" class="section level2">
<h2><span class="header-section-number">7.3</span> Heteroskedasticity</h2>
<p>In applied regression, heteroskedasticity consistent (HC) or robust standard errors are often used.</p>
<p>However, there is straightforwardly direct translation of HC standard error to regression model this in a Bayesian setting. The sandwich method of estimating HC errors uses the same point estimates for the regression coefficients as OLS, but estimates the standard errors of those coefficients in a second stage from the OLS residuals. Disregarding differences in frequentist vs. Bayesian inference, it is clear that a direct translation of that method could not be fully Bayesian since the coefficients and errors are not estimated jointly.</p>
<p>In a linear normal regression model with heteroskedasticity, each observation has its own scale parameter, <span class="math inline">\(\sigma_i\)</span>, <span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(X \beta, \sigma_i) .
\end{aligned}
\]</span> It should be clear that without proper priors this model is not identified, meaning that the posterior distribution is improper. To estimate this model we have to apply some model to the scale terms, <span class="math inline">\(\sigma_i\)</span>. In fact, you can think of homoskedasticity as the simplest such model; assuming that all <span class="math inline">\(\sigma_i = \sigma\)</span>. A more general model of <span class="math inline">\(\sigma_i\)</span> should encode any information the analyst has about the scale terms. This can be a distribution or functions of covariates for how we think observations may have different values.</p>
<div id="covariates" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Covariates</h3>
<p>A simple model of heteroskedasticity is if the observations can be split into groups. Suppose the observations are partitioned into <span class="math inline">\(k = 1, \dots, K\)</span> groups, and <span class="math inline">\(k[i]\)</span> is the group of observation <span class="math inline">\(i\)</span>, <span class="math display">\[
\sigma_i = \sigma_{k[i]}
\]</span></p>
<p>Another choice would be to model the scale term with a regression model, for example, <span class="math display">\[
\log(\sigma_i) \sim \dnorm(X \gamma, \tau)
\]</span></p>
</div>
<div id="student-t-error" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Student-t Error</h3>
<p>The Student-t distribution of error terms from the <a href="heteroskedasticity-and-robust-regression.html#robust-regression">Robust Regression</a> chapter is also model of heteroskedasticity.</p>
<p>A reparameterization that will be used quite often is to rewrite a normal distributions with unequal scale parameters as the product of a common global scale parameter (<span class="math inline">\(\sigma\)</span>), and observation specific local scale parameters, <span class="math inline">\(\lambda_i\)</span>,[^globalmixture] <span class="math display">\[
y_i \sim \dnorm(X\beta, \lambda_i \sigma) .
\]</span> If the local variance parameters are distributed inverse-gamma, <span class="math display">\[
\lambda^2 \sim \dinvgamma(\nu / 2, \nu / 2)
\]</span> then the above is equivalent to a regression with errors distributed Student-t errors with <span class="math inline">\(\nu\)</span> degrees of freedom, <span class="math display">\[
y_i \sim \dt{\nu}(X \beta, \sigma) .
\]</span></p>
<p>[^globalmixture] See <a href="http://www.sumsar.net/blog/2013/12/t-as-a-mixture-of-normals/">this</a> for a visualization of a Student-t distribution a mixture of Normal distributions, and <a href="https://www.johndcook.com/t_normal_mixture.pdf">this</a> for a derivation of the Student t distribution as a mixture of normal distributions. This scale mixture of normal representation will also be used with shrinkage priors on the regression coefficients.</p>
<p><strong>Example:</strong> Simulate Student-t distribution with <span class="math inline">\(\nu\)</span> degrees of freedom as a scale mixture of normal. For *s in 1:S$,</p>
<ol style="list-style-type: decimal">
<li>Simulate <span class="math inline">\(z_s \sim \dgamma(\nu / 2, \nu / 2)\)</span></li>
<li><span class="math inline">\(x_s = 1 / \sqrt{z_s}2\)</span> is draw from <span class="math inline">\(\dt{\nu}(0, 1)\)</span>.</li>
</ol>
<p>When using R, ensure that you are using the correct parameterization of the gamma distribution. <strong>Left to reader</strong></p>
</div>
</div>
<div id="references-2" class="section level2">
<h2><span class="header-section-number">7.4</span> References</h2>
<div id="robust-regression" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Robust regression</h3>
<ul>
<li>See <span class="citation">Gelman and Hill (2007 sec 6.6)</span>, <span class="citation">Gelman et al. (2013 ch 17)</span></li>
<li><span class="citation">Team (2016 Sec 8.4)</span> for the Stan example using a Student-t distribution</li>
</ul>
</div>
<div id="heteroskedasticity-1" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Heteroskedasticity</h3>
<ul>
<li><span class="citation">Gelman et al. (2013 Sec. 14.7)</span> for models with unequal variances and correlations.</li>
<li><span class="citation">Team (2016)</span> reparameterizes the Student t distribution as a mixture of gamma distributions in Stan.</li>
</ul>
</div>
<div id="qunatile-regression" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Qunatile regression</h3>
<ul>
<li><span class="citation">Benoit and Poel (2017)</span></li>
<li><span class="citation">Yu and Zhang (2005)</span> for the three-parameter asymmetric Laplace distribution</li>
</ul>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>The Double Exponential distribution still has a thinner tail than the Student-t at higher values.<a href="heteroskedasticity-and-robust-regression.html#fnref3">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-stan-and-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-linear-models.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/bayesian_notes/edit/master/robust.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

[
["index.html", "Updating: A Set of Bayesian Notes Preface", " Updating: A Set of Bayesian Notes Jeffrey B. Arnold Preface Notes on Bayesian methods - written to supplement CS&amp;SS/STAT 564: Bayesian Statistics for the Social Sciences. These notes largely focus on the application and theory necessary for quantitative social scientists to successfully apply Bayesian statistical methods. I also don’t hesitate to link to those who have already explained things well, and focus my efforts on places where I haven’t found good explanations (or explanations I understand), or places where I need to write notes to deepen my own understanding. All these chapters will use the rstan package library(&quot;rstan&quot;) "],
["bayesian-inference.html", "1 Bayesian Inference", " 1 Bayesian Inference "],
["markov-chain-monte-carlo.html", "2 Markov Chain Monte Carlo 2.1 Monte Carlo Sampling 2.2 Markov Chain Monte Carlo Sampling 2.3 References", " 2 Markov Chain Monte Carlo 2.1 Monte Carlo Sampling Monte Carlo methods are used to numerically approximate integrals, when the integral function is not tractable but the function being integrated is. In Bayesian stats, the mean of a probability density \\(p(\\theta)\\) is \\[ \\mu = \\int_{\\Theta} \\theta p(\\theta) \\, d \\theta . \\] Except for cases in which the distribution \\(p(\\theta)\\) has a known form (not the case for most applied models) for functional form of the integral isn’t known, but \\(p(\\theta)\\) is The Monte Carlo estimate of \\(\\mu\\) is. Draw \\(N\\) independent samples, \\(\\theta^{(1)}, \\dots, \\theta^{(N)}\\), from \\(p(\\theta)\\) Estimate \\(\\hat{\\mu}\\) with, \\[ \\hat{\\mu} = \\frac{1}{N} \\sum_{n = 1}^N \\theta^{(N)} . \\] If \\(p(\\theta)\\) has finite mean and variance, the law of large numbers ensures that the Monte Carlo estimate converges to the true value \\[ \\lim_{N \\to \\infty} \\hat\\mu \\to \\mu \\] and the estimation error is governed by the CLT, \\[ | \\mu - \\hat{\\mu} | \\propto \\frac{1}{\\sqrt{N}} \\] Example The mean of \\(Y = X^2\\) where \\(X \\sim \\dnorm(0, 1)\\). Draw a sample from \\(Y\\), x &lt;- rnorm(1024, 0, 1) ^ 2 The Monte Carlo estimates of the mean is mean(x) #&gt; [1] 0.977 with standard error, sd(x) / sqrt(length(x)) #&gt; [1] 0.042 2.2 Markov Chain Monte Carlo Sampling Problem: Monte Carlo sampling requires the samples to be independent. But what if you cannot draw independent samples? Solution: Markov Chain Monte Carlo are a class of algorithms to sample from a distribution when independent samples cannot be drawn. However, the samples in MCMC will be dependent. 2.3 References Stan Development Team (2016 Ch. 28) "],
["mcmc-diagnostics.html", "3 MCMC Diagnostics 3.1 Convergence Diagnostics 3.2 Autocorrelation, Effective Sample Size, and MCSE 3.3 HMC Specific Diagnostics 3.4 References", " 3 MCMC Diagnostics There are two parts of checking a Bayesian model: diagnostics: Is the sampler working? Is it adequately approximating the specified posterior distribution: \\(p(\\theta | D)\\). model fit: Does the model adequately represent the data? 3.1 Convergence Diagnostics Under certain conditions, MCMC algorithms will draw a sample from the target posterior distribution after it has converged to equilbrium. However, since in practice, any sample is finite, there is no guarantee about whether its converged, or is close enough to the posterior distribution. In general there is no way to prove that the sampler has converged.1 However, there are several statistics that indicate that a sampler has not converged. 3.1.1 Potential Scale Reduction (\\(\\hat{R}\\)) In equilibrium, the distribution of samples from chains should be the same regardless of the initial starting values of the chains (Stan Development Team 2016, Sec 28.2). One way to check this is to compare the distributions of multiple chains—in equilibrium they should all have the same mean. Additionally, the split \\(\\hat{R}\\) tests for convergence by splitting the chain in half, and testing the hypothesis that the means are the same in each half. This tests for non-stationarity within a chain. See Stan Development Team (2016 Sec 28.2) for the equations to calculate these. TODO: Examples of passing and non-passing Rhat chains using fake data generated from known functions with a given autocorrelation. Rule of Thumb: The rule of thumb is that R-hat values for all less than 1.1 source. Note that all parameters must show convergence. This is a necessary but not sufficient condition for convergence. 3.2 Autocorrelation, Effective Sample Size, and MCSE MCMC samples are dependent. This does not effect the validity of inference on the posterior if the samplers has time to explore the posterior distribution, but it does affect the efficiency of the sampler. In other words, highly correlated MCMC samplers requires more samples to produce the same level of Monte Carlo error for an estimate. 3.2.1 Autocorrelation The effective sample size (ESS) measures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample. Suppose that the \\(\\rho^2_t\\) is the ACF function of a sample of size \\(N\\), the effective sample size, \\(N_eff\\), is \\[ N_{eff} = \\frac{N}{\\sum_{t = -\\infty}^\\infty \\rho_t} = \\frac{N}{1 + 2 \\sum_{t = -\\infty}^\\infty \\rho_t}. \\] TODO show that if \\(\\rho_t = 1\\) for all \\(t\\) then \\(N_eff = 1\\), and if \\(\\rho_t = 0\\) for all \\(t\\) then \\(N_eff = N\\). See also Stan Development Team (2016 Sec 28.4), Geyer (2011), and Gelman et al. (2013). Thinning Since the autocorrelation tends to decrease as the lag increases, thinning samples will reduce the final autocorrelation in the sample while also reducing the total number of samples saved. Due to the autocorrelation, the reduction in the number of effective samples will often be less than number of samples removed in thinning. Both of these will produce 1,000 samples from the poserior, but effective sample size of \\(B\\) will be greater than the effective sample size of \\(A\\), since after thinnin g the autocorrelation in \\(B\\) will be lower. A Generating 1,000 samples after convergence and save all of them B Generating 10,000 samples after convergence and save every 10th sample In this case, A produces 10,000 samples, and B produces 1,000. The effective sample size of A will be higher than B. However, due to autocorrelation, the proportional reduction in the effective sample size in B will be less than the thinning: \\(N_{eff}(A) / N_{eff}(B) &lt; 10\\). A Generating 10,000 samples after convergence and save all of them B Generating 10,000 samples after convergence and save every 10th sample Thinning trades off sample size for memory, and due to autocorrelation in samples, loss in effective sample size is less than the loss in sample size. Example: Comparison of the effective sample sizes for data generated with various levels of autocorrelation. The package rstan does not directly expose the function it uses to calculate ESS, so this ess function does so (for a single chain). ess &lt;- function(x) { N &lt;- length(x) V &lt;- map_dbl(seq_len(N - 1), function(t) { mean(diff(x, lag = t) ^ 2, na.rm = TRUE) }) rho &lt;- head_while(1 - V / var(x), `&gt;`, y = 0) N / (1 + sum(rho)) } n &lt;- 1024 ess(rnorm(n)) #&gt; [1] 1024 ess(arima.sim(list(ar = 0.5), n)) #&gt; [1] 1024 ess(arima.sim(list(ar = 0.75), n)) #&gt; [1] 653 ess(arima.sim(list(ar = 0.875), n)) #&gt; [1] 328 ess(arima.sim(list(ar = 0.99), n)) #&gt; [1] 67 3.2.2 Monte Carlo Standard Error (MCSE) The Monte Carlo standard error is the uncertainty about a statistic in the sample due to sampling error. With a independent sample of size \\(N\\), the MCSE for the sample mean is \\[ MCSE(\\bar{\\theta}) = \\frac{s}{\\sqrt{N}} \\] where \\(s\\) is the sample standard deviation. However, MCMC are generally not independent, and the MCSE will be higher than that of an independent sample. One way to calculate the MCSE with autocorrelated samples is to use the effective sample size instead of the sample size, \\[ MCSE(\\bar{\\theta}) = \\frac{s}{\\sqrt{N_{eff}}} \\] MCSE for common values: the mean, and any posterior probabilities: mean \\(s_\\theta / \\sqrt{S}\\) probability \\(\\sqrt{p (1 - p) / S}\\) The estimation of standard errors for quantiles, as would be used in is more complicated. See the package mcmcse for Monte Carlo standard errors of quantiles (though calculated in a different method than rstan). See Gelman et al. (2013 Sec. 10.5). 3.3 HMC Specific Diagnostics HMC produces several diagnostics that indicate that the sampler is breaking and, thus, not sampling from the posterior distribution. This is unusual, as most Bayesian sampling methods do not give indication of whether they are working well, and all that can be checked are the properties of the samples themselves with methods like \\(\\hat{R}\\). The two diagnostics that HMC provides are divergent transitions maximum treedepth The HMC sampler has two tuning parameters Stepsize: Length of the steps to take Treedepth: Number of steps to take Stan chooses intelligent defaults for these values. However, this does not always work, and the divergent transitions and maximum treedepth tuning parameters indicate that these parameters should be adjusted. 3.3.1 Divergent transitions The problem: The details of the HMC are technical and can be found TODO. The gist of the problem is that Stan is using a discrete approximation of a continuous function when integrating. If the step sizes are too large, the discrete approximation does not work. Helpfully, when the approximation is poor it does not fail without any indication but will produce “divergent transitions”. If there are too many divergent transitions, then the sampler is not drawing samples from the entire posterior and inferences will be biased The solution: Reduce the step size. This can be done by increasing the the adapt_delta parameter. This is the target average proposal acceptance probability in the adaptation, which is used to determine the step size during warmup. A higher desired acceptance probability (closer to 1) reduces the the step size. A smaller step size means that it will require more steps to explore the posterior distribution. See Stan Development Team (2016, 380) 3.3.2 Maximum Treedepth The problem: NUTS is an intelligent method to select the number of steps to take in each iteration. However, there is still a maximum number of steps that NUTS will try. If the sampler is often hitting the maximum number of steps, it means that the optimal number of steps to take in each iteration is higher than the maximum. While divergent transitions bias inference, a too-small maximum treedepth only affects efficiency. The sampler is still exploring the posterior distribution, but the exploration will be slower and the autocorrelation higher (effective sample size lower) than if the maximum treedepth were set higher. The solution: Increase the maximum treedepth. 3.4 References see Gelman et al. (2013, 267) Stan2016a [Ch 28.] for how Stan calculates Rhat, autocorrelations, and ESS. See Flegal, Haran, and Jones (2008) and the mcmcse for methods to calculate MCMC standard errors and an argument for using ESS as a stopping rule for Bayesian inference. Talk by Geyer on MCSE This is also the case in optimization with non-convex objective functions.↩ "],
["posterior-inference.html", "4 Posterior Inference 4.1 Prerequisites", " 4 Posterior Inference 4.1 Prerequisites The haven package is used to read Stata .dta files. library(&quot;rubbish&quot;) library(&quot;haven&quot;) 4.1.1 Introduction The posterior distribution is the probability distribution \\(\\Pr(\\theta | y)\\). One we have the posterior distribution, or more often a sample from the posterior distribution, it is relatively easy to perform inference on any function of the posterior. Common means to summarize the post mean: \\(\\E(p(\\theta | y)) \\approx \\frac{1}{S} \\sum_{i = 1}^S \\theta^{(s)}\\) median: \\(\\median(p(\\theta | y)) \\approx \\median \\theta^{(s)}\\) quantiles: 2.5%, 5%, 25%, 50%, 75%, 95%, 97.5% credible interval: central credible interval: the interval between the p/2% and 1 - p/2% quantiles highest posterior density interval: the narrowest interval containing p% of distribution 4.1.2 Functions of the Posterior Distribution It is also easy to conduct inference on functions of the posterior distribution. Suppose \\(\\theta^{(1)}, \\dots, \\theta^{(S)}\\) are a sample from \\(p(\\theta | y)\\), the \\(f(\\theta^{(1)}), \\dots, f(\\theta^{(S)})\\) are a sample from \\(p(f(\\theta) | y)\\). This is not easy for methods like MLE that produce point estimates. Even with MLE Even in OLS, non-linear functions coefficients generally require either the Delta method or bootstrapping to calculate confidence intervals. Berry, Golder, and Milton (2012), Golder (2017),Brambor, Clark, and Golder (2006) discuss calculating confidence intervals See Rainey (2016) on “transformation induced bias” See Carpenter (2016) on how reparameterization affects point estimates; this is a Stan Case study with working code 4.1.3 Marginal Effects 4.1.3.1 Exmample: Marginal Effect Plot for X This example from Matt Golder’s Interactions page constructs a marginal effect plot for \\(X\\), where there is an interaction between \\(X\\) and \\(Z\\). \\[ Y = \\beta_0 + \\beta_x + \\beta_z + \\beta_{xz} X Z + \\epsilon \\] alexseev &lt;- read_dta(&quot;data/alexseev.dta&quot;) The regression that is run mod_f &lt;- xenovote ~ slavicshare * changenonslav + inc9903 + eduhi02 + unemp02 + apt9200 + vsall03 + brdcont lm(mod_f, data = alexseev) #&gt; #&gt; Call: #&gt; lm(formula = mod_f, data = alexseev) #&gt; #&gt; Coefficients: #&gt; (Intercept) slavicshare #&gt; 8.942878 0.031486 #&gt; changenonslav inc9903 #&gt; -0.851108 0.000234 #&gt; eduhi02 unemp02 #&gt; -0.039512 1.432013 #&gt; apt9200 vsall03 #&gt; 0.030125 0.661163 #&gt; brdcont slavicshare:changenonslav #&gt; 2.103688 0.008226 Use the lm_preprocess function in the rubbish package to turn the model formula into a list with relevant data. mod_data &lt;- lm_preprocess(mod_f, data = alexseev)[c(&quot;X&quot;, &quot;y&quot;)] mod_data &lt;- within(mod_data, { n &lt;- nrow(X) k &lt;- ncol(X) # indices of relevant coefficients M &lt;- 100 changenonslav &lt;- seq(min(X[ , &quot;changenonslav&quot;]), max(X[ , &quot;changenonslav&quot;]), length.out = M) idx_b_slavicshare &lt;- which(colnames(X) == &quot;slavicshare&quot;) idx_b_slavicshare_changenonslav &lt;- which(colnames(X) == &quot;slavicshare:changenonslav&quot;) b_loc &lt;- 0 # data appropriate prior b_scale &lt;- max(apply(X, 2, sd)) * 3 sigma_scale &lt;- sd(y) }) Get the mean of dydx dydx &lt;- get_posterior_mean(mod_fit, pars = &quot;dydx&quot;) ggplot(tibble(changenonslav = mod_data$changenonslav, dydx = dydx[ , &quot;mean-all chains&quot;]), aes(x = changenonslav, y = dydx)) + geom_line() + ylab(&quot;Marginal effect of slavic share&quot;) + xlab(paste(expression(Delta, &quot;non-Slavic Share&quot;))) Plotting each iteration as a line: dydx_all &lt;- rstan::extract(mod_fit, pars = &quot;dydx&quot;)$dydx %&gt;% as.tibble() %&gt;% mutate(.iter = row_number()) %&gt;% # keep only a few iter gather(param, value, -.iter) %&gt;% left_join(tibble(param = paste0(&quot;V&quot;, seq_along(mod_data$changenonslav)), changenonslav = mod_data$changenonslav), by = &quot;param&quot;) dydx_all %&gt;% filter(.iter %in% sample(unique(.iter), 2 ^ 8)) %&gt;% ggplot(aes(x = changenonslav, y = value, group = .iter)) + geom_line(alpha = 0.3) + ylab(&quot;Marginal effect of slavic share&quot;) + xlab(paste(expression(Delta, &quot;non-Slavic Share&quot;))) Summarize the marginal effects with mean, 50% central credible interval, and 90% central credible intervals: dydx_all %&gt;% group_by(changenonslav) %&gt;% summarise(mean = mean(value), q5 = quantile(value, 0.05), q25 = quantile(value, 0.25), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95)) %&gt;% ggplot(aes(x = changenonslav, y = mean)) + geom_ribbon(aes(ymin = q5, ymax = q95), alpha = 0.2) + geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.2) + geom_line(colour = &quot;blue&quot;) + ylab(&quot;Marginal effect of slavic share&quot;) + xlab(expression(paste(Delta, &quot;non-Slavic Share&quot;))) "],
["model-checking.html", "5 Model Checking 5.1 Why check models? 5.2 Posterior Predictive Checks 5.3 Sources", " 5 Model Checking 5.1 Why check models? In theory—Bayesian model should include all relevant substantive knowledge and subsume all possible theories. In practice—It won’t. Need to check how the model fits data. The question is not whether a model is “true”; it isn’t (Box 1976). But is it good enough for the purposes of the analysis. See Gelman, Meng, and Stern (1996), Gelman (2007), Gelman (2009), Gelman et al. (2013 Ch. 6), Gelman and Shalizi (2012b), Kruschke (2013), Gelman and Shalizi (2012a), Gelman (2014) for more discussion of the motivation and use of posterior predictive checks. 5.2 Posterior Predictive Checks One way to evaluate the fit of a model is posterior predictive checks Fit the model to the data to get the posterior distribution of the parameters: \\(p(\\theta | D)\\) Simulate data from the fitted model: \\(p(\\tilde{D} | \\theta, D)\\) Compare the simulated data (or a statistic thereof) to the observed data and a statistic thereof. The comparison between data simulated from the model can be formal or visual. Within a Stan function, this is done in the generated quantities block using a _rng distribution functions: generated quantities { vector[n] yrep; for (i in 1:n) { yrep[i] ~ } } The package bayesplot includes multiple functions for posterior predictive checks; see the help for PPC-overview for a summary of these functions. 5.2.1 Bayesian p-values A posterior predictive p-value is a the tail posterior probability for a statistic generated from the model compared to the statistic observed in the data. Let \\(y = (y_1, \\dots, y_n)\\) be the observed data. Suppose the model has been fit and there is a set of simulation \\(\\theta^(s)\\), \\(s = 1, \\dots, n_sims\\). In replicated dataset, \\(y^{rep(s)\\), has been generated from the predictive distribution of the data, \\(p(y^{(rep)} | \\theta = \\theta^{(s)}\\). Then the ensemble of simulated datasets, \\((y^{rep(s)}, \\dots, y^{rep(nsims)})\\), is a sample from the posterior predictive distribution, \\(p(y^{(rep)} | y)\\) The model can be tested by means of discrepancy statistics, which are some function of the data and parameters, \\(T(y, \\theta)\\). If \\(\\theta\\) was known, then compare discrepancy by \\(T(y^{(rep)}, \\theta)\\). The statistical significance is \\(p = \\Pr(T(y^{(rep)}, \\theta) &gt; T(y, \\theta) | y, \\theta)\\). If \\(\\theta\\) is unknown, then average over the posterior distribution of \\(\\theta\\), \\[ \\begin{aligned}[t] p &amp;= \\Pr(T(y^{(rep)}, \\theta) &gt; T(y, \\theta) | y) \\\\ &amp;= \\int Pr(T(y^{(rep)}, \\theta) &gt; T(y, \\theta) | y, \\theta) p(\\theta | y) d\\,\\theta , \\end{aligned} \\] which is easily estimated from the MCMC samples as, \\[ p = \\frac{1}{n_{sims}}\\sum_{s = 1}^{n_{sims}} 1( T(y^{rep(s)}, \\theta(s)) &gt; T(y, \\theta(s))) \\] 5.2.2 Test quantities The definition of a posterior p-value does not specify a particular test-statistic, \\(T\\), to use. The best advice is that \\(T\\) depends on the application. Gelman et al. (2013, 146) Speed of light example uses the 90% interval (61st and 6th order statistics). Gelman et al. (2013, 147) binomial trial example uses the number of swicthes (0 to 1, or 1 to 0) in order to test independence. Gelman et al. (2013, 148) hierarchical model for adolesce smoking uses percent of adolescents in the sample who never smoked percentage in the sample who smoked in all waves precentage of “incident smoker”: adolescents who began the study and non-smokers and ended as smokers. 5.2.3 p-values vs. u-values A posterior predictive p-value is different than a classical p-value. Posterior predictive p-value distributed uniform if the model is true Classical p-value distributed uniform if the null hypothesis (\\(H_0\\)) is true A u-value is any function of the data that has a \\(U(0, 1)\\) sampling distribution (Gelman et al. 2013, 151) a u-value can be averaged over \\(\\theta\\), but it is not Bayesian, and is not a probability distribution posterior p-value: probability statement, conditional on model and data, about future observations 5.2.4 Marginal predictive checks Compare statistics for each observation. Conditional Predictive Ordinate (CPO): The CPO (Gelfand 1996) is the leave-on-out cross-validation predictive density: \\[ p(y_i | y_{-i}) = \\int p(y_i | \\theta) p(\\theta | y_{-i}) d\\,\\theta \\] The pointwise predicted LOO probabilities can be calculated using PSIS-LOO or WAIC in the loo package. Predictive Concordance and Predictive QuantilesP Gelfand (1996) classifies any \\(y_i\\) that is outside the central 95% predictive posterior of \\(y^{rep}_i\\) is an outlier. Let the predictive quantile (\\(PQ_i\\)) be \\[ PQ_i = p(y_i^{(rep)} &gt; y_i) . \\] Then the predictive concordance be the proportion of \\(y_i\\) that are not outliers. Gelfand (1996) argues that the predictive concordance should match 95% - in other words that the posterior predictive distribution should have the correct coverage. (Laplace Demon p. 20) 5.2.5 Outliers Can be identified by the inverse-CPO. larger than 40 are possible outliers, and those higher than 70 are extreme values (Ntzoufras 2009, p. 376). Congdon (2005) scales CPO by dividing each by its individual max and considers observations with scaled CPO under 0.01 as outliers. 5.2.6 Grapical Posterior Predictive Checks Visualization can surprise you, but it doesn’t scale well. Modeling scales well, but it can’t surprise you. – paraphrase of Hadley Hickham Instead of calculating posterior probabilities, plot simulated data and observed data and visually compare them. See Gelman et al. (2013, 154). plot simulated data and real data (Gelman et al. 2013, 154). This is similar to ideas in Wickham et al. (2010). plot summary statistics or inferences residual plots Bayesian residuals have a distribution \\(r_i^{(s)} = y_i - \\E(y_i | \\theta^{s})\\) Bayesian resdiual graph plots single realization of the residuals, or a summary of their posterior distributions binned plots are best for discrete data (Gelman et al. 2013, 157) 5.3 Sources See Gelman and Shalizi (2012b), Gelman and Shalizi (2012a), Kruschke (2013) "],
["introduction-to-stan-and-linear-regression.html", "6 Introduction to Stan and Linear Regression 6.1 Prerequites 6.2 The Statistical Model 6.3 Maximum A Posteriori estimation", " 6 Introduction to Stan and Linear Regression This chapter is an introduction to writing and running a Stan model in R. Also see the rstan vignette for similar content. 6.1 Prerequites For this section we will use the duncan dataset included in the car package. Duncan’s occupational prestige data is an example dataset used throughout the popular Fox regression text, Applied Regression Analysis and Generalized Linear Models (Fox 2016). It is originally from Duncan (1961) consists of survey data on the prestige of occupations in the US in 1950, and several predictors: type of occupation, income, and education of that data(&quot;Duncan&quot;, package = &quot;car&quot;) 6.2 The Statistical Model The first step in running a Stan model is defining the Bayesian statistical model that will be used for inference. Let’s run the regression of occupational prestige on the type of occupation, income, and education: \\[ \\begin{multline} y_i = \\beta_0 + \\beta_1 I(\\mathtt{type} = \\mathtt{&quot;prof&quot;}) + \\beta_2 I(\\mathtt{type} = \\mathtt{&quot;wc&quot;}) \\\\ + \\beta_3 \\mathtt{income} + \\beta_4 \\mathtt{education} + \\epsilon_i \\end{multline} \\] duncan_lm &lt;- lm(prestige ~ type + income + education, data = Duncan) duncan_lm #&gt; #&gt; Call: #&gt; lm(formula = prestige ~ type + income + education, data = Duncan) #&gt; #&gt; Coefficients: #&gt; (Intercept) typeprof typewc income education #&gt; -0.185 16.658 -14.661 0.598 0.345 There are \\(n = 45\\) observations in the dataset. Let \\(y\\) be a \\(n \\times 1\\) vector of the values of prestige. Let \\(X\\) be the \\(n \\times k\\) design matrix of the regression. In this case, \\(k = 5\\), \\[ X = \\begin{bmatrix} 1 &amp; \\mathtt{typeprof} &amp; \\mathtt{typewc} &amp; \\mathtt{income} &amp; \\mathtt{education} \\end{bmatrix} \\] In OLS, we get the frequentist estimates of \\(\\hat{\\beta}\\) by minimizing the squared errors, \\[ \\hat{\\beta}_{OLS} = \\argmin_{\\beta} \\sum_{i = 1}^n (y_i - \\beta&#39; x_i)^2 = \\argmin \\sum_{i = 1}^n \\hat{\\epsilon}_i \\] For valid inference we need to make assumptions about \\(\\epsilon_i\\), namely that they are uncorrelated with \\(X\\), \\(\\Cov(\\epsilon, X) = 0\\), and that they are i.i.d, \\(\\Cov(\\epsilon_i, \\epsilon_j) = 0\\), \\(\\Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). However, no specific distributional form is or needs to be assumed for \\(\\epsilon\\) since CLT results show that, asymptotically, the sampling distribution of \\(\\beta\\) is distributed normal. Additionally, although \\(\\hat\\sigma^2 = \\sum_{i = 1}^n \\epsilon_i / (n - k - 1)\\) is a estimator of \\(\\sigma^2\\), standard errors of the standard error of the regression are not directly provided. In Bayesian inference, our target is the posterior distribution of the parameters, \\(\\beta\\) and \\(\\sigma\\): \\(p(\\beta, \\sigma^2 | y, X)\\). Since all uncertainty in Bayesian inference is provided via probability, we will need to explicitly provide parametric distributions for the likelihood and parameters. \\[ p(\\beta, \\sigma | y, X) \\propto p(y | \\beta, \\sigma) p(\\beta, \\sigma) \\] For a Bayesian linear regression model, we’ll need to specify distributions for \\(p(y | \\beta, \\sigma)\\) and \\(p(\\beta, \\sigma)\\). Likelihood: \\(p(y_i | x_i, \\beta, \\sigma)\\) suppose that the observations are distributed independent normal: \\[ y_i \\sim N(\\beta&#39;x_i, \\sigma^2) \\] Priors: The model needs to specify a prior distribution for the parameters \\((\\beta, \\sigma)\\). Rather than specify a single distribution for \\(\\beta\\) and \\(\\sigma\\), it will be easier to specify independent (separate) distributions for \\(\\beta\\) and \\(\\sigma\\). The Stan manual and … provide For the normal distribution, assume i.i.d. normal distributions for each element of \\(\\beta\\): \\[ \\beta_k \\sim N(b, s) \\] For the scale parameter of the normal distribution, \\(\\sigma\\), we will use a half-Cauchy. The Cauchy distribution is a special case of the Student t distribution when the degrees of freedom is 1. In Bayesian stats, it has the property that it concentrates probability mass around its median (zero), but has very wide tails, so if the prior distribution guess is wrong, the parameter can still adapt to data. A half-Cauchy distribution is a Cauchy distribution but with support of \\((0, \\infty)\\) instead of the entire real line. \\[ \\sigma \\sim C^{+}(0, w) \\] Combining all the previous equations, our statistical model for linear regression is, \\[ \\begin{aligned}[t] y &amp;\\sim N(\\mu, \\sigma) \\\\ \\mu &amp;= X \\beta \\\\ \\beta &amp;\\sim N(b, s) \\\\ \\sigma &amp;\\sim C^{+}(0, w) \\end{aligned} \\] This defines a Bayesian model gives us \\[ p(\\beta, \\sigma | y, X, b, s, w) \\propto p(y | X, \\beta) p(\\beta | b, s) p(\\sigma | w) \\] The targets of inference in this model are the two parameters: \\(\\beta\\) (regression coefficients), and \\(\\sigma\\) (standard deviation of the regression). This is conditional on the observed or assumed quantities, which including both the data \\(y\\) (response) and \\(X\\) (predictors), as well the values defining the prior distributions: \\(b\\), \\(s\\), and \\(w\\). Now that we’ve defined a statistical model, we can write it as a Stan model. Stan models are written in its own domain-specific language that focuses on declaring the statistical model (parameters, variables, distributions) while leaving the details of the sampling algorithm to Stan. A Stan model consists of blocks which contain declarations of variables and/or statements. Each block has a specific purpose in the model. functions { // OPTIONAL: user-defined functions } data { // read in data ... } transformed data { // Create new variables/auxiliary variables from the data } parameters { // Declare parameters that will be estimated } transformed parameters { // Create new variables/auxiliary variables from the parameters } model { // Declare your probability model: priors, hyperpriors &amp; likelihood } generated quantities { // Declare any quantities other than simulated parameters to be generated } The file lm.stan is a Stan model for the linear regression model previously defined. data { // number of observations int n; // response vector vector[n] y; // number of columns in the design matrix X int k; // design matrix X matrix [n, k] X; // beta prior real b_loc; real&lt;lower = 0.0&gt; b_scale; // sigma prior real sigma_scale; } parameters { // regression coefficient vector vector[k] b; // scale of the regression errors real&lt;lower = 0.0&gt; sigma; } transformed parameters { // mu is the observation fitted/predicted value // also called yhat vector[n] mu; mu = X * b; } model { // priors b ~ normal(b_loc, b_scale); sigma ~ cauchy(0, sigma_scale); // likelihood y ~ normal(mu, sigma); } generated quantities { // simulate data from the posterior vector[n] y_rep; for (i in 1:n) { y_rep[i] = normal_rng(mu[i], sigma); } } mod1 &lt;- stan_model(&quot;stan/lm.stan&quot;) See the Stan Modeling Language User’s Guide and Reference Manual for details of the Stan Language. NoteSince a Stan model compiles to C++ code, you may receive some warning messages such as /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] static void set_zero_all_adjoints() { ^ In file included from file1d4a4d50faa.cpp:8: In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/src/stan/model/model_header.hpp:4: As long as your model compiles, you can ignore these compiler warnings (On the other hard, warnings that occur during sampling should not be ignored). If the Stan model does not give you a syntax error when parsing the model, it should compile to valid C++.[^bugs][^c-warnings] See [bugs]: In the rare case that the Stan parser transpiles the Stan model to C++ but cannot compile the C++ code, it is a bug in Stan. Follow the instructions on how to inform the Stan developers about bugs. [c-warnings]: The extended installation instructions for MacOS/Linux and Windows have instructions for adding compiler options to the R Makevars file. 6.2.1 Sampling In order to sample from the model, we need to at least give it the values for the data to use: n, k, y, X, and the data associated with the priors. mod1_data &lt;- list( y = Duncan$prestige, n = nrow(Duncan) ) The data types in Stan are all numeric (either integers or reals), but they include matrices and vectors. However, there is nothing like a data frame in Stan. Whereas in the R function lm we can provide a formula and a data set for where to look for objects, and the function will create the appropriate \\(X\\) matrix for the regression, we will need to create that matrix ourselves—expanding categorical variables to indicator variables, and expanding interactions and other functions of the predictors. However, we need to do that all manually. The function stats is the workhorse function used in lm and many other R functions to convert a formula into the matrix used in estimation. X &lt;- model.matrix(prestige ~ type + income + education, data = Duncan) mod1_data$X &lt;- X mod1_data$k &lt;- ncol(X) We still need to provide the values for the prior distributions. For specific values of the prior distributions, assume uninformative priors for beta by setting the mean to zero and the variances to large numbers. \\[ \\beta_k \\sim N(0, 1000) \\] mod1_data$b_loc &lt;- 0 mod1_data$b_scale &lt;- 1000 For prior of the regression scale parameter \\(\\sigma\\), use a half-Cauchy distribution with a large scale parameter, which is a good choice for the priors of scale parameters. \\[ \\sigma \\sim C^{+}(0, 50) \\] mod1_data$sigma_scale &lt;- 50 Now, sample from the posterior, using the function sampling: mod1_fit &lt;- sampling(mod1, data = mod1_data) 6.2.2 Convergence Diagnostics and Model Fit Convergence Diagnostics: Is this the posterior distribution that you were looking for? These don’t directly say anything about how “good” the model is in terms representing the data, they are only evaluating how well the sampler is doing at sampling the posterior distribution of the given model. If there are problems with these, then the sample results do not represent the posterior distribution, and your inferences will be biased. mcse: n_eff: Rhat divergences Model fit: Is this statistical model appropriate for the data? Or better than other models? Posterior predictive checks Information criteria: WAIC Leave-one-out Cross-Validation 6.3 Maximum A Posteriori estimation The Statistical Rethinking text focuses on maximum a posteriori (MAP) estimation. In addition to sampling from the posterior distribution using HMC, the same Stan model can be used to estimate the MAP estimate of the parameters. Use the optimizing function to find the MAP estimates of the model: mod1_fit_opt &lt;- optimizing(mod1, data = mod1_data) #&gt; Initial log joint probability = -562878 #&gt; Error evaluating model log probability: Non-finite gradient. #&gt; Error evaluating model log probability: Non-finite function evaluation. #&gt; #&gt; Error evaluating model log probability: Non-finite gradient. #&gt; #&gt; Optimization terminated normally: #&gt; Convergence detected: relative gradient magnitude is below tolerance mod1_fit_opt #&gt; $par #&gt; b[1] b[2] b[3] b[4] b[5] sigma mu[1] #&gt; -0.198 16.644 -14.679 0.598 0.346 9.180 83.221 #&gt; mu[2] mu[3] mu[4] mu[5] mu[6] mu[7] mu[8] #&gt; 85.742 93.064 80.419 84.416 58.025 86.835 98.817 #&gt; mu[9] mu[10] mu[11] mu[12] mu[13] mu[14] mu[15] #&gt; 55.232 89.198 67.121 95.735 95.390 69.979 76.581 #&gt; mu[16] mu[17] mu[18] mu[19] mu[20] mu[21] mu[22] #&gt; 42.297 63.674 71.659 56.754 91.402 27.337 32.818 #&gt; mu[23] mu[24] mu[25] mu[26] mu[27] mu[28] mu[29] #&gt; 42.531 19.734 20.301 41.370 57.890 32.377 20.553 #&gt; mu[30] mu[31] mu[32] mu[33] mu[34] mu[35] mu[36] #&gt; 34.739 18.789 6.405 33.889 11.747 17.537 19.265 #&gt; mu[37] mu[38] mu[39] mu[40] mu[41] mu[42] mu[43] #&gt; 18.350 19.041 11.056 15.772 17.341 18.602 10.897 #&gt; mu[44] mu[45] y_rep[1] y_rep[2] y_rep[3] y_rep[4] y_rep[5] #&gt; 36.365 15.642 83.699 86.726 94.091 85.121 76.229 #&gt; y_rep[6] y_rep[7] y_rep[8] y_rep[9] y_rep[10] y_rep[11] y_rep[12] #&gt; 67.198 94.040 107.471 55.463 87.214 81.968 100.999 #&gt; y_rep[13] y_rep[14] y_rep[15] y_rep[16] y_rep[17] y_rep[18] y_rep[19] #&gt; 94.443 61.613 73.923 35.785 72.805 69.508 51.738 #&gt; y_rep[20] y_rep[21] y_rep[22] y_rep[23] y_rep[24] y_rep[25] y_rep[26] #&gt; 86.781 18.800 37.047 49.372 22.102 22.935 46.992 #&gt; y_rep[27] y_rep[28] y_rep[29] y_rep[30] y_rep[31] y_rep[32] y_rep[33] #&gt; 54.856 48.034 14.845 51.714 23.681 -1.549 18.705 #&gt; y_rep[34] y_rep[35] y_rep[36] y_rep[37] y_rep[38] y_rep[39] y_rep[40] #&gt; 14.650 27.235 7.695 26.999 6.659 11.935 -2.765 #&gt; y_rep[41] y_rep[42] y_rep[43] y_rep[44] y_rep[45] #&gt; 16.126 6.159 23.402 38.479 9.517 #&gt; #&gt; $value #&gt; [1] -122 #&gt; #&gt; $return_code #&gt; [1] 0 It can also return samples from the multivariate normal (Laplace) approximation to the posterior distribution. Adding the option hessian = TRUE returns the hessian, which is defined on the unconstrained parameter space (all parameters are defined over \\((-\\infty, \\infty)\\)). To get a sample of values from that multivariate normal distribution set draws = TRUE. These draws will be from the unconstrained parameter space, unless constrained = TRUE, in which case they will be on the scales of the original parameters. mod1_fit_opt &lt;- optimizing(mod1, data = mod1_data, hessian = TRUE, constrained = TRUE) #&gt; Initial log joint probability = -40591.1 #&gt; Optimization terminated normally: #&gt; Convergence detected: relative gradient magnitude is below tolerance "],
["heteroskedasticity-and-robust-regression.html", "7 Heteroskedasticity and Robust Regression 7.1 Prerequisites 7.2 Linear Regression with Student t distributed errors 7.3 Heteroskedasticity 7.4 References", " 7 Heteroskedasticity and Robust Regression 7.1 Prerequisites VGAM is needed for the Laplace distribution. library(&quot;VGAM&quot;) 7.2 Linear Regression with Student t distributed errors Like OLS, Bayesian linear regression with normally distributed errors is sensitive to outliers. The normal distribution has narrow tail probabilities. This plots the normal, Double Exponential (Laplace), and Student-t (df = 4) distributions all with mean 0 and scale 1, and the surprise (\\(- log(p)\\)) at each point. Higher surprise is a lower log-likelihood. Both the Student-t and Double Exponential distributions have surprise values well below the normal in the ranges (-6, 6).2 This means that outliers impose less of a penalty on the log-posterio models using these distributions, and the regression line would need to move less to incorporate those observations since the error distribution will not consider them as unusual. z &lt;- seq(-6, 6, length.out = 100) bind_rows( tibble(z = z, p = dnorm(z, 0, 1), distr = &quot;Normal&quot;), tibble(z = z, p = dt(z, 4), distr = &quot;Student-t (df = 4)&quot;), tibble(z = z, p = VGAM::dlaplace(z, 0, 1), distr = &quot;Double Exponential&quot;)) %&gt;% mutate(`-log(p)` = -log(p)) %&gt;% ggplot(aes(x = z, y = `-log(p)`, colour = distr)) + geom_line() unionization &lt;- read_tsv(&quot;data/western1995/unionization.tsv&quot;, col_types = cols( country = col_character(), union_density = col_double(), left_government = col_double(), labor_force_size = col_number(), econ_conc = col_double() )) mod_data &lt;- preprocess_lm(union_density ~ left_government + log(labor_force_size) + econ_conc, data = unionization) mod_data &lt;- within(mod_data, { b_loc &lt;- 0 b_scale &lt;- 100 sigma_scale &lt;- sd(y) }) The max_treedepth parameter needed to be increased because in some runs it was hitting the maximum treedepth. This is likely due to the wide tails of the Student t distribution. mod_t_fit &lt;- sampling(mod_t, data = mod_data, control = list(max_treedepth = 11)) #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.846309 seconds (Warm-up) #&gt; 0.720782 seconds (Sampling) #&gt; 1.56709 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 1 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.798868 seconds (Warm-up) #&gt; 0.802543 seconds (Sampling) #&gt; 1.60141 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 2 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.753821 seconds (Warm-up) #&gt; 0.679116 seconds (Sampling) #&gt; 1.43294 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 3 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is 0, but must be &gt; 0! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.897422 seconds (Warm-up) #&gt; 0.785585 seconds (Sampling) #&gt; 1.68301 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 4 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. summary(mod_t_fit, pars = c(&quot;nu&quot;, &quot;sigma&quot;, &quot;b&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; nu 21.699 0.24416 14.3626 3.599 11.279 18.431 28.918 57.369 3460 #&gt; sigma 10.441 0.04339 2.0681 7.059 9.015 10.166 11.610 15.230 2272 #&gt; b[1] 66.279 1.47339 53.0047 -43.640 32.578 67.960 102.590 166.056 1294 #&gt; b[2] 0.274 0.00149 0.0806 0.114 0.223 0.275 0.326 0.432 2914 #&gt; b[3] -4.494 0.09324 3.4316 -11.013 -6.858 -4.598 -2.252 2.676 1354 #&gt; b[4] 10.789 0.50310 18.3043 -23.366 -2.307 10.319 22.500 48.544 1324 #&gt; Rhat #&gt; nu 1.000 #&gt; sigma 1.001 #&gt; b[1] 1.002 #&gt; b[2] 0.999 #&gt; b[3] 1.002 #&gt; b[4] 1.002 Compare those results when using a model with summary(mod_normal_fit, pars = c(&quot;b&quot;, &quot;sigma&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; b[1] 72.930 1.5774 52.9423 -32.488 38.804 72.844 108.590 175.722 1127 #&gt; b[2] 0.269 0.0019 0.0813 0.105 0.217 0.268 0.321 0.431 1835 #&gt; b[3] -4.859 0.1048 3.5445 -11.707 -7.235 -4.898 -2.621 2.184 1143 #&gt; b[4] 8.368 0.5112 17.6960 -25.508 -3.307 8.001 20.290 43.378 1198 #&gt; sigma 11.070 0.0621 2.1395 7.879 9.582 10.739 12.195 16.245 1188 #&gt; Rhat #&gt; b[1] 1 #&gt; b[2] 1 #&gt; b[3] 1 #&gt; b[4] 1 #&gt; sigma 1 Alternatively, the Double Exponential (Laplace) distribution can be used for the errors. This is the equivalent to least quantile regression, where the regression line is the median (50% quantile) mod_dbl_exp &lt;- stan_model(&quot;stan/lms.stan&quot;) #&gt; In file included from filebc3d4e835a7a.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/core.hpp:12: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: #&gt; In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/math/tools/config.hpp:13: #&gt; In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/config.hpp:39: #&gt; /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] #&gt; # define BOOST_NO_CXX11_RVALUE_REFERENCES #&gt; ^ #&gt; &lt;command line&gt;:6:9: note: previous definition is here #&gt; #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 #&gt; ^ #&gt; In file included from filebc3d4e835a7a.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/core.hpp:42: #&gt; /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] #&gt; static void set_zero_all_adjoints() { #&gt; ^ #&gt; In file included from filebc3d4e835a7a.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/core.hpp:43: #&gt; /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] #&gt; static void set_zero_all_adjoints_nested() { #&gt; ^ #&gt; In file included from filebc3d4e835a7a.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/prim/mat.hpp:59: #&gt; /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] #&gt; size_t fft_next_good_size(size_t N) { #&gt; ^ #&gt; In file included from filebc3d4e835a7a.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/prim/mat.hpp:298: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/prim/arr.hpp:39: #&gt; In file included from /Users/jrnold/Library/R/3.3/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: #&gt; In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/numeric/odeint.hpp:61: #&gt; In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: #&gt; In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array.hpp:21: #&gt; In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/base.hpp:28: #&gt; /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; 8 warnings generated. mod_dbl_exp_fit &lt;- sampling(mod_dbl_exp, data = mod_data) #&gt; #&gt; SAMPLING FOR MODEL &#39;lms&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.11096 seconds (Warm-up) #&gt; 0.875358 seconds (Sampling) #&gt; 1.98632 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 1 #&gt; count #&gt; Exception thrown at line 33: double_exponential_lpdf: Scale parameter is inf, but must be finite! 5 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;lms&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.01465 seconds (Warm-up) #&gt; 1.02955 seconds (Sampling) #&gt; 2.0442 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 2 #&gt; count #&gt; Exception thrown at line 33: double_exponential_lpdf: Scale parameter is inf, but must be finite! 3 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;lms&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.2061 seconds (Warm-up) #&gt; 1.24753 seconds (Sampling) #&gt; 2.45363 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 3 #&gt; count #&gt; Exception thrown at line 33: double_exponential_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;lms&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.14636 seconds (Warm-up) #&gt; 0.981335 seconds (Sampling) #&gt; 2.12769 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 4 #&gt; count #&gt; Exception thrown at line 33: double_exponential_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. summary(mod_dbl_exp_fit, par = c(&quot;b&quot;, &quot;sigma&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; b[1] 38.693 1.90247 51.2313 -60.417 5.004 37.97 71.661 140.290 725 #&gt; b[2] 0.298 0.00225 0.0837 0.131 0.245 0.30 0.352 0.458 1387 #&gt; b[3] -2.971 0.11773 3.2419 -9.408 -5.073 -3.01 -0.842 3.242 758 #&gt; b[4] 20.981 0.67250 18.2690 -15.589 9.157 21.53 33.057 56.774 738 #&gt; sigma 9.050 0.06676 2.2260 5.585 7.533 8.75 10.209 14.423 1112 #&gt; Rhat #&gt; b[1] 1 #&gt; b[2] 1 #&gt; b[3] 1 #&gt; b[4] 1 #&gt; sigma 1 7.3 Heteroskedasticity In applied regression, heteroskedasticity consistent (HC) or robust standard errors are often used. However, there is straighforwardly direct translation of HC standard error to regression model this in a Bayesian setting. The sandwich method of estimating HC errors uses the same point estimates for the regression coefficients as OLS, but estimates the standard errors of those coefficients in a second stage from the OLS residuals. Disregarding differences in frequentist vs. Bayesian inference, it is clear that a direct translation of that method could not be fully Bayesian since the coefficients and errors are not estimated jointly. In a linear normal regression model with heteroskedasticity, each observation has its own scale parameter, \\(\\sigma_i\\), \\[ \\begin{aligned}[t] y_i &amp;\\sim \\dnorm(X \\beta, \\sigma_i) . \\end{aligned} \\] It should be clear that without proper priors this model is not identified, meaning that the posterior distribution is improper. To estimate this model we have to apply some model to the scale terms, \\(\\sigma_i\\). In fact, you can think of homoskedasticity as the simplest such model; assuming that all \\(\\sigma_i = \\sigma\\). A more general model of \\(\\sigma_i\\) should encode any information the analyst has about the scale terms. This can be a distribution or functions of covariates for how we think observations may have different values. 7.3.1 Covariates A simple model of heteroskedasticity is if the observations can be split into groups. Suppose the observations are partitioned into \\(k = 1, \\dots, K\\) groups, and \\(k[i]\\) is the group of observation \\(i\\), \\[ \\sigma_i = \\sigma_{k[i]} \\] Another choice would be to model the scale term with a regression model, for example, \\[ \\log(\\sigma_i) \\sim \\dnorm(X \\gamma, \\tau) \\] 7.3.2 Student-t It turns out that the Student-t distribution of error terms from the Robust Regression chapter can also be derived as a model of heteroskedasticity. A reparameterization that will be used quite often is to rewrite a normal distributions with unequal scale parameters as a continous mixture of a common global scale parameter (\\(\\sigma\\)), and observation specific local scale parameters, \\(\\lambda_i\\),[^globalmixture] \\[ y_i \\sim \\dnorm(X\\beta, \\lambda_i \\sigma) . \\] If the local scale paramters are distributed as, \\[ \\lamba^2 \\sim \\dinvgamma(\\nu / 2, \\nu / 2) \\] then the above is equivalent to a regression with errors distributed Student-t errors with \\(\\nu\\) degrees of freedom, \\[ y_i \\sim \\dt{\\nu}(X \\beta, \\sigma) . \\] [^globalmixture] See this for a visualization of a Student-t distribution a mixture of Normal distributions, and this for a derivation of the Student t distribution as a mixture of normals. This scale mixture of normals representation will also be used with shrinkage priors on the regression coefficients. Example: Simulate Student-t distribution with \\(\\nu\\) degrees of freedom as a scale mixture of normals. For *s in 1:S$, Simulate \\(z_s \\sim \\dgamma(\\nu / 2, \\nu / 2)\\) \\(x_s = 1 / \\sqrt{z_s}2\\) is draw from \\(\\dt{\\nu}(0, 1)\\). When using R, ensure that you are using the correct parameterization of the gamma distribution. Left to reader 7.4 References 7.4.1 Robust regression See Gelman and Hill (2007 sec 6.6), Gelman et al. (2013 ch 17) Stan Development Team (2016 Sec 8.4) for the Stan example using a Student-t distribution 7.4.2 Heteroskedasticity Gelman et al. (2013 Sec. 14.7) for models with unequal variances and correlations. Stan Development Team (2016) reparameterizes the Student t distribution as a mixture of gamma distributions in Stan. The Double Exponential distribution still has a thinner tail than the Student-t at higher values.↩ "],
["references-3.html", "References", " References "]
]

[
["index.html", "Updating: A Set of Bayesian Notes Preface", " Updating: A Set of Bayesian Notes Jeffrey B. Arnold Preface Notes on Bayesian methods - written to supplement CS&amp;SS/STAT 564: Bayesian Statistics for the Social Sciences. These notes largely focus on the application and theory necessary for quantitative social scientists to successfully apply Bayesian statistical methods. I also don’t hesitate to link to those who have already explained things well, and focus my efforts on places where I haven’t found good explanations (or explanations I understand), or places where I need to write notes to deepen my own understanding. All these chapters will use the rstan package library(&quot;rstan&quot;) "],
["bayesian-inference.html", "1 Bayesian Inference", " 1 Bayesian Inference "],
["markov-chain-monte-carlo.html", "2 Markov Chain Monte Carlo 2.1 Monte Carlo Sampling 2.2 Markov Chain Monte Carlo Sampling 2.3 References", " 2 Markov Chain Monte Carlo 2.1 Monte Carlo Sampling Monte Carlo methods are used to numerically approximate integrals, when the integral function is not tractable but the function being integrated is. In Bayesian stats, the mean of a probability density \\(p(\\theta)\\) is \\[ \\mu = \\int_{\\Theta} \\theta p(\\theta) \\, d \\theta . \\] Except for cases in which the distribution \\(p(\\theta)\\) has a known form (not the case for most applied models) for functional form of the integral isn’t known, but \\(p(\\theta)\\) is The Monte Carlo estimate of \\(\\mu\\) is. Draw \\(N\\) independent samples, \\(\\theta^{(1)}, \\dots, \\theta^{(N)}\\), from \\(p(\\theta)\\) Estimate \\(\\hat{\\mu}\\) with, \\[ \\hat{\\mu} = \\frac{1}{N} \\sum_{n = 1}^N \\theta^{(N)} . \\] If \\(p(\\theta)\\) has finite mean and variance, the law of large numbers ensures that the Monte Carlo estimate converges to the true value \\[ \\lim_{N \\to \\infty} \\hat\\mu \\to \\mu \\] and the estimation error is governed by the CLT, \\[ | \\mu - \\hat{\\mu} | \\propto \\frac{1}{\\sqrt{N}} \\] Example The mean of \\(Y = X^2\\) where \\(X \\sim \\dnorm(0, 1)\\). Draw a sample from \\(Y\\), x &lt;- rnorm(1024, 0, 1) ^ 2 The Monte Carlo estimates of the mean is mean(x) #&gt; [1] 0.977 with standard error, sd(x) / sqrt(length(x)) #&gt; [1] 0.042 2.2 Markov Chain Monte Carlo Sampling Problem: Monte Carlo sampling requires the samples to be independent. But what if you cannot draw independent samples? Solution: Markov Chain Monte Carlo are a class of algorithms to sample from a distribution when independent samples cannot be drawn. However, the samples in MCMC will be dependent. 2.3 References Team (2016 Ch. 28) "],
["mcmc-diagnostics.html", "3 MCMC Diagnostics 3.1 Reparameterize Models 3.2 Convergence Diagnostics 3.3 Autocorrelation, Effective Sample Size, and MCSE 3.4 Thinning 3.5 HMC-NUT Specific Diagnostics", " 3 MCMC Diagnostics There are two parts of checking a Bayesian model: diagnostics: Is the sampler working? Is it adequately approximating the specified posterior distribution: \\(p(\\theta | D)\\). model fit: Does the model adequately represent the data? This chapter covers the former. Also see the bayesplot vignette Visual MCMC diagnostics using the bayesplot package, which though specific to the provides, provides a good overview of these diagnostics. 3.1 Reparameterize Models Reduce correlation between parameters (e.g. see mcmc_pairs) Put parameters on the same scale. The samplers work best when all parameters are roughly on the same scale, e.g. \\(\\approx 1\\). Try to avoid situations where parameters are orders of magnitude different, e.g. 1e-5 and 1e+10. Increase the informativeness of priors. If parameters are too uninformative, the posterior distribution may have wide tails that hamper sampling. One way of thinking about it is that the model is only “weakly identified” and requires either more data or more informative priors to estimate. 3.2 Convergence Diagnostics Under certain conditions, MCMC algorithms will draw a sample from the target posterior distribution after it has converged to equilbrium. However, since in practice, any sample is finite, there is no guarantee about whether its converged, or is close enough to the posterior distribution. In general there is no way to prove that the sampler has converged.1 However, there are several statistics that indicate that a sampler has not converged. 3.2.1 Potential Scale Reduction (\\(\\hat{R}\\)) In equilibrium, the distribution of samples from chains should be the same regardless of the initial starting values of the chains (Team 2016, Sec 28.2). One way to check this is to compare the distributions of multiple chains—in equilibrium they should all have the same mean. Additionally, the split \\(\\hat{R}\\) tests for convergence by splitting the chain in half, and testing the hypothesis that the means are the same in each half. This tests for non-stationarity within a chain. See Team (2016 Sec 28.2) for the equations to calculate these. TODO: Examples of passing and non-passing Rhat chains using fake data generated from known functions with a given autocorrelation. Rule of Thumb: The rule of thumb is that R-hat values for all less than 1.1 source. Note that all parameters must show convergence. This is a necessary but not sufficient condition for convergence. 3.2.2 References Gelman et al. (2013, 267) Team (2016 Ch 28.) for how Stan calculates Rhat, autocorrelations, and ESS. Gelman and Rubin (1992) introduce the R-hat statistic 3.3 Autocorrelation, Effective Sample Size, and MCSE MCMC samples are dependent. This does not effect the validity of inference on the posterior if the samplers has time to explore the posterior distribution, but it does affect the efficiency of the sampler. In other words, highly correlated MCMC samplers requires more samples to produce the same level of Monte Carlo error for an estimate. 3.3.1 Effective Sample Size The effective sample size (ESS) measures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample. Suppose that the \\(\\rho^2_t\\) is the ACF function of a sample of size \\(N\\), the effective sample size, \\(N_eff\\), is \\[ N_{eff} = \\frac{N}{\\sum_{t = -\\infty}^\\infty \\rho_t} = \\frac{N}{1 + 2 \\sum_{t = -\\infty}^\\infty \\rho_t}. \\] TODO show that if \\(\\rho_t = 1\\) for all \\(t\\) then \\(N_eff = 1\\), and if \\(\\rho_t = 0\\) for all \\(t\\) then \\(N_eff = N\\) Computing the effective sample size requires calculating an autocorrelation. A multichain estimate of the autocorrelation is found by computing the variogram with the correlations for all lags, \\[ V_t = \\frac{1}{m(n - t)} \\sum_{j = 1}^m \\sum_{i = t + 1}^n (\\psi_{i,j} - \\psi_{i - t,j})^2 \\] The estimate of the autocorrelations \\(\\hat{\\rho}_t\\) is \\[ \\hat{\\rho}_t = 1 - \\frac{V_t}{2 \\widehat{\\mathrm{var}}^+} \\] The estimates of the autocorrelations can be noisy, so \\(\\hat{\\rho}_t\\) are summed from 0, to the last \\(t\\) such that \\(\\rho\\) is positive (\\(T\\)), \\[ \\hat{n}_{eff} = \\frac{mn}{1 + 2 \\sum_{t = 1}^T \\hat{\\rho}_t} \\] See also Team (2016 Sec 28.4), Geyer (2011), and Gelman et al. (2013 Sec 11.5). This isn’t the only way to calculate the effective sample size. The coda package function coda uses a different method. The differences are due to how the autocorrelation is calculated. Example: Comparison of the effective sample sizes for data generated with various levels of autocorrelation. The package rstan does not directly expose the function it uses to calculate ESS, so this ess function does so (for a single chain). ess &lt;- function(x) { N &lt;- length(x) V &lt;- map_dbl(seq_len(N - 1), function(t) { mean(diff(x, lag = t) ^ 2, na.rm = TRUE) }) rho &lt;- head_while(1 - V / var(x), `&gt;`, y = 0) N / (1 + sum(rho)) } n &lt;- 1024 sims &lt;- map_df(c(0, 0.5, 0.75, 0.99), function(ar) { tibble(ar = ar, y = if (ar == 0) rnorm(n) else arima.sim(list(ar = ar), n), x = seq_along(y), n_eff = ess(y), label = sprintf(&quot;AR = %.2f (n_eff = %.0f)&quot;, ar, n_eff)) } ) ggplot(sims, aes(x = x, y = y)) + geom_line() + facet_wrap(~ label, scales = &quot;free_y&quot;) + labs(x = &quot;&quot;, y = &quot;&quot;) 3.4 Thinning Since the autocorrelation tends to decrease as the lag increases, thinning samples will reduce the final autocorrelation in the sample while also reducing the total number of samples saved. Due to the autocorrelation, the reduction in the number of effective samples will often be less than number of samples removed in thinning. Both of these will produce 1,000 samples from the poserior, but effective sample size of \\(B\\) will be greater than the effective sample size of \\(A\\), since after thinnin g the autocorrelation in \\(B\\) will be lower. A Generating 1,000 samples after convergence and save all of them B Generating 10,000 samples after convergence and save every 10th sample In this case, A produces 10,000 samples, and B produces 1,000. The effective sample size of A will be higher than B. However, due to autocorrelation, the proportional reduction in the effective sample size in B will be less than the thinning: \\(N_{eff}(A) / N_{eff}(B) &lt; 10\\). A Generating 10,000 samples after convergence and save all of them B Generating 10,000 samples after convergence and save every 10th sample Thinning trades off sample size for memory, and due to autocorrelation in samples, loss in effective sample size is less than the loss in sample size. Thinning has become less of an issue as memory has become less of a computational constraint, and samplers have become more efficient. The following example simulates random values from an autocorrelated series, and applies different levels of thinning. Thinning is always decreasing the effective sample size. However, the number of effective samples per sample (n_eff / n) increases until the thinning is large enough that the thinned samples are uncorrelated. thin_ess &lt;- function(thin, x) { if (thin &gt; 1) { # keep only thinned rows x_thinned &lt;- x[(seq_len(length(x)) %% thin) == 1] } else { x_thinned &lt;- x } tibble(thin = thin, n = length(x_thinned), n_eff = ess(x_thinned), `n_eff / n` = n_eff / n) } map_df(c(1, 2, 4, 8, 16, 32), thin_ess, x = arima.sim(list(ar = .9), 4096)) #&gt; # A tibble: 6 × 4 #&gt; thin n n_eff `n_eff / n` #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 4096 1266 0.309 #&gt; 2 2 2048 1087 0.531 #&gt; 3 4 1024 791 0.773 #&gt; 4 8 512 512 1.000 #&gt; 5 16 256 256 1.000 #&gt; 6 32 128 128 1.000 Gelman et al. (2013, 282–83) Team (2016, 354–55) 3.4.1 Traceplots Trace plots are a timeseries of sampler iterations, e.g. as produced by bayesplot. These can, but should not, be used to assess convergence, such visual inspection is ‘notoriously unreliable’ (Gelman et al. 2013, 285) it cannot scale to many parameters Traceplots may be useful for diagnosing convergence problems after \\(\\hat{R}\\) or or \\(n_eff\\) indicates problems. Some possible issues to check in these plots are multimodality (the traceplot jumps between different distributions) wide posterior tails (the traceplot shows regions where the sampler will reach and have difficulty returning to the main distribution) 3.4.2 Monte Carlo Standard Error (MCSE) The Monte Carlo standard error is the uncertainty about a statistic in the sample due to sampling error. With a independent sample of size \\(N\\), the MCSE for the sample mean is \\[ MCSE(\\bar{\\theta}) = \\frac{s}{\\sqrt{N}} \\] where \\(s\\) is the sample standard deviation. However, MCMC are generally not independent, and the MCSE will be higher than that of an independent sample. One way to calculate the MCSE with autocorrelated samples is to use the effective sample size instead of the sample size, \\[ MCSE(\\bar{\\theta}) = \\frac{s}{\\sqrt{N_{eff}}} \\] An MCSE estimator for the mean is \\[ \\mathrm{MCSE}(\\hat{\\theta}) = \\frac{\\sd(\\theta)}{\\sqrt{n_{neff}}} \\] An MCSE estimator for any posterior probability, where \\(\\hat{p} = \\Pr(f(\\theta))\\), follows from the standard error of a proportion, but using the effective sample size, \\[ MCSE(\\hat{p}) = \\sqrt{\\hat{p} (1 - \\hat{p}) / n_{eff}} \\] See Flegal, Haran, and Jones (2008) and the mcmcse for methods to calculate MCMC standard errors for means and quantiles using subsampling methods. Flegal, Haran, and Jones (2008) argues for using ESS as a stopping rule and convergence diagnostic for Bayesian inference. The estimation of standard errors for quantiles, as would be used in is more complicated. See the package mcmcse for Monte Carlo standard errors of quantiles (though calculated in a different method than rstan). Gelman et al. (2013 Sec. 10.5) Flegal, Haran, and Jones (2008) Talk by Geyer on MCSE 3.5 HMC-NUT Specific Diagnostics Hamiltonian Monte Carlo (HMC), and the No-U-Turn Sampler (HMC-NUTS) in particular, produce several diagnostics that indicate that the sampler is breaking and, thus, not sampling from the posterior distribution. This is unusual, as most Bayesian sampling methods do not give indication of whether they are working well, and all that can be checked are the properties of the samples themselves with methods such \\(\\hat{R}\\). Three specific HMC-NUTS diagnostics are divergent transitions maximum treedepth Bayesian fraction of missing information The general way to fix these issues is the manually adjust the HMC-NUTS sampler parameters.n Stepsize: Length of the steps to take Treedepth: Number of steps to take During the warmup perion, Stan tunes these values, however these auto-tuned parameters may not always be optimal. The other alternative is to reparameterize the models. 3.5.1 Divergent transitions The problem: The details of the HMC are technical and can be found TODO. The gist of the problem is that Stan is using a discrete approximation of a continuous function when integrating. If the step sizes are too large, the discrete approximation does not work. Helpfully, when the approximation is poor it does not fail without any indication but will produce “divergent transitions”. If there are too many divergent transitions, then the sampler is not drawing samples from the entire posterior and inferences will be biased The solution: Reduce the step size. This can be done by increasing the the adapt_delta parameter. This is the target average proposal acceptance probability in the adaptation, which is used to determine the step size during warmup. A higher desired acceptance probability (closer to 1) reduces the the step size. A smaller step size means that it will require more steps to explore the posterior distribution. See Team (2016, 380) 3.5.2 Maximum Treedepth The problem: NUTS is an intelligent method to select the number of steps to take in each iteration. However, there is still a maximum number of steps that NUTS will try. If the sampler is often hitting the maximum number of steps, it means that the optimal number of steps to take in each iteration is higher than the maximum. While divergent transitions bias inference, a too-small maximum treedepth only affects efficiency. The sampler is still exploring the posterior distribution, but the exploration will be slower and the autocorrelation higher (effective sample size lower) than if the maximum treedepth were set higher. The solution: Increase the maximum treedepth. 3.5.3 Bayesian Fraction of Missing Information This is rather technical. See Betancourt (2016). This is also the case in optimization with non-convex objective functions.↩ "],
["posterior-inference.html", "4 Posterior Inference 4.1 Prerequisites 4.2 Introduction 4.3 Functions of the Posterior Distribution 4.4 Marginal Effects", " 4 Posterior Inference 4.1 Prerequisites The haven package is used to read Stata .dta files. library(&quot;rubbish&quot;) library(&quot;haven&quot;) 4.2 Introduction The posterior distribution is the probability distribution \\(\\Pr(\\theta | y)\\). One we have the posterior distribution, or more often a sample from the posterior distribution, it is relatively easy to perform inference on any function of the posterior. Common statistics used to summarize the posterior distribution: mean: \\(\\E(p(\\theta | y)) \\approx \\frac{1}{S} \\sum_{i = 1}^S \\theta^{(s)}\\) median: \\(\\median(p(\\theta | y)) \\approx \\median \\theta^{(s)}\\) quantiles: 2.5%, 5%, 25%, 50%, 75%, 95%, 97.5% credible interval: central credible interval: the interval between the p/2% and 1 - p/2% quantiles highest posterior density interval: the narrowest interval containing p% of distribution marginal densities 4.3 Functions of the Posterior Distribution It is also easy to conduct inference on functions of the posterior distribution. Suppose \\(\\theta^{(1)}, \\dots, \\theta^{(S)}\\) are a sample from \\(p(\\theta | y)\\), the \\(f(\\theta^{(1)}), \\dots, f(\\theta^{(S)})\\) are a sample from \\(p(f(\\theta) | y)\\). This is not easy for methods like MLE that produce point estimates: Even in OLS, non-linear functions coefficients generally require either the Delta method or bootstrapping to calculate confidence intervals. Berry, Golder, and Milton (2012), Golder (2017),Brambor, Clark, and Golder (2006) discuss calculating confidence intervals See Rainey (2016b) on “transformation induced bias” See Carpenter (2016) on how reparameterization affects point estimates; this is a Stan Case study with working code 4.4 Marginal Effects 4.4.1 Example: Marginal Effect Plot for X Berry, Golder, and Milton (2012) replicates Alexseev (2006) as an example of a model with an interaction between \\(X\\) and \\(Z\\). \\[ Y = \\beta_0 + \\beta_x X + \\beta_z Z + \\beta_{xz} X Z + \\epsilon \\] In this case, the hypothesis of interest involves the marginal effect of \\(X\\) on \\(Y\\), \\[ \\frac{\\partial \\E(Y|.)}{\\partial X} = \\beta_z + \\beta_{xz} Z \\] Since there is an interaction, the marginal effect of \\(X\\) is not simply the coefficient \\(\\beta_z\\), but is a function of another predictor, \\(Z\\). Point estimates of the marginal effects with interactions are relatively easy to construct, but confidence intervals for the MLE estimates quickly involve multiple terms and either the Delta method approximation or bootstrapping to calculate. We will consider this problem from a Bayesian estimation perspective, and calculate point estimates (posterior mean) and credible intervals of the marginal effects. The particular example is Alexseev (2006), which analyzes how changes in the ethnic composition of Russian regions affected the vote share of the extreme Russian nationalist Zhirinovsky Bloc in 2003 Russian State Duma elections.[^alexseev1] alexseev &lt;- read_dta(&quot;data/alexseev.dta&quot;) [alexseev1]: Some of the replication code and material can be found on Matt Golder’s website. One claim of Alexseev (2006) was that support for anti-immigrant parties depends on the percentage of the population of the dominant ethnic group (Slavic) and the change in the percentage the non-dominant share. To test that hypothesis, Alexseev (2006) estimates the following model, \\[ \\begin{multline} \\mathtt{xenovote}_i = \\beta_0 + \\beta_s \\mathtt{slavicshare}_i + \\beta_{n} \\mathtt{changenonslav} + \\\\ \\beta_{sn} (\\mathtt{slavicshare}_i \\times \\mathtt{changenonslav}_i) + \\gamma z_{i} + \\epsilon_{i}, \\end{multline} \\] where \\(z_i\\) is a vector of control variables. xenovote: Xenophobic voting. Share of vote for the Zhironovsky Bloc. slavicshare: Slavic Share. Proportion Slavic in the district. changenonslav: \\(\\Delta\\) non-Slavic Share Change in the proprotion of non-Slavic groups in the region. The model was estimated by OLS,2 mod_f &lt;- (xenovote ~ slavicshare * changenonslav + inc9903 + eduhi02 + unemp02 + apt9200 + vsall03 + brdcont) lm(mod_f, data = alexseev) #&gt; #&gt; Call: #&gt; lm(formula = mod_f, data = alexseev) #&gt; #&gt; Coefficients: #&gt; (Intercept) slavicshare #&gt; 8.942878 0.031486 #&gt; changenonslav inc9903 #&gt; -0.851108 0.000234 #&gt; eduhi02 unemp02 #&gt; -0.039512 1.432013 #&gt; apt9200 vsall03 #&gt; 0.030125 0.661163 #&gt; brdcont slavicshare:changenonslav #&gt; 2.103688 0.008226 Use the lm_preprocess function in the rubbish package to turn the model formula into a list with relevant data. mod_data &lt;- lm_preprocess(mod_f, data = alexseev)[c(&quot;X&quot;, &quot;y&quot;)] mod_data &lt;- within(mod_data, { n &lt;- nrow(X) k &lt;- ncol(X) M &lt;- 100 changenonslav &lt;- seq(min(X[ , &quot;changenonslav&quot;]), max(X[ , &quot;changenonslav&quot;]), length.out = M) idx_b_slavicshare &lt;- which(colnames(X) == &quot;slavicshare&quot;) idx_b_slavicshare_changenonslav &lt;- which(colnames(X) == &quot;slavicshare:changenonslav&quot;) b_loc &lt;- 0 # data appropriate prior b_scale &lt;- max(apply(X, 2, sd)) * 3 sigma_scale &lt;- sd(y) }) mod data { // number of observations int n; // response vector vector[n] y; // number of columns in the design matrix X int k; // design matrix X matrix [n, k] X; // marfx // indexes of main and interaction coef int idx_b_slavicshare; int idx_b_slavicshare_changenonslav; int M; vector[M] changenonslav; // beta prior real b_loc; real b_scale; // sigma prior real sigma_scale; } parameters { // regression coefficient vector vector[k] b; // scale of the regression errors real sigma; } transformed parameters { // mu is the observation fitted/predicted value // also called yhat vector[n] mu; mu = X * b; } model { // priors b ~ normal(b_loc, b_scale); sigma ~ cauchy(0, sigma_scale); // likelihood y ~ normal(mu, sigma); } generated quantities { # hardcoded marginal effectx vector[M] dydx; dydx = b[idx_b_slavicshare] + b[idx_b_slavicshare_changenonslav] * changenonslav; } The function rstan extracts parameters from the rstan object as a list with an element for each parameter. dydx_all &lt;- rstan::extract(mod_fit, pars = &quot;dydx&quot;)$dydx To make it easier to use with ggplot, convert it to a data frame with columns .id (number), changeonslav (original value of changeonslav passed to the sampler), and value (the value of the marginal effect). dydx_all &lt;- dydx_all %&gt;% as.tibble() %&gt;% mutate(.iter = row_number()) %&gt;% gather(.id, value, -.iter) %&gt;% # merge with original values of changenonslav left_join(tibble(.id = paste0(&quot;V&quot;, seq_along(mod_data$changenonslav)), changenonslav = mod_data$changenonslav), by = &quot;.id&quot;) Since values for a marginal effect line is generated for each iteration, the posterior distribution of \\(\\partial E(\\mathtt{xenovote}|.) / \\partial slavicshare\\) can be plotted as lines. Since the number of lines would be too many to display effectively, plot 256 of them: dydx_all %&gt;% filter(.iter %in% sample(unique(.iter), 2 ^ 8)) %&gt;% ggplot(aes(x = changenonslav, y = value, group = .iter)) + geom_line(alpha = 0.3) + ylab(&quot;Marginal effect of slavic share&quot;) + xlab(paste(expression(Delta, &quot;non-Slavic Share&quot;))) Alternatively, we can summarize the posterior distribution of the marginal effects with a line (posterior mean) and credible interval regions (50%, 90%): dydx_summary &lt;- dydx_all %&gt;% group_by(changenonslav) %&gt;% summarise(mean = mean(value), q5 = quantile(value, 0.05), q25 = quantile(value, 0.25), q75 = quantile(value, 0.75), q95 = quantile(value, 0.95)) ggplot() + modelr::geom_ref_line(h = 0) + geom_ribbon(data = dydx_summary, mapping = aes(x = changenonslav, ymin = q5, ymax = q95), alpha = 0.2) + geom_ribbon(data = dydx_summary, mapping = aes(x = changenonslav, ymin = q25, ymax = q75), alpha = 0.2) + geom_line(data = dydx_summary, mapping = aes(x = changenonslav, y = mean), colour = &quot;blue&quot;) + geom_rug(data = alexseev, mapping = aes(x = changenonslav), sides = &quot;b&quot;) + ylab(&quot;Marginal effect of slavic share&quot;) + xlab(expression(paste(Delta, &quot;non-Slavic Share&quot;))) The plot above also includes a rug with the observed values of changenonslav in the sample. Q: For each value of changenonslav, what is the probability that the marginal effect of slavicshare is greater than 0? Q: Reestimate the model, but calculate the marginal effect of slavicshare for all observed values of changenonslav in the sample. For each observation, calculate the probability that the marginal effect is greater than 0. What proporation of observations is the probability that the marginal effect is greater than zero. Q: Suppose you want to calculate the expected probability that the marginal effect of slavicshare is greater than zero in the sample. Let \\(\\theta^{S}_i\\) be the parameter for the marginal effect of slavicshare on the xenovote. Consider these two calculations: \\[ \\frac{1}{N} \\sum_{i = 1}^n \\left( \\frac{1}{S} \\sum_{s = 1}^S I(\\theta^{(s)}_i &gt; 0) \\right) \\] and \\[ \\frac{1}{S} \\sum_{s = 1}^S \\left( \\frac{1}{N} \\sum_{i = 1}^N I(\\theta^{(s)}_i &gt; 0) \\right) . \\] Are they the same? What are their substantive interpretations? Q: Construct the same plot but for Figure 5(b) in Berry, Golder, and Milton (2012), which displays the marginaleffects of \\(\\Delta\\) non-Slavic on Xenophobic voting. The original model used cluster robust standard errors, which will be ignored for now.↩ "],
["model-checking.html", "5 Model Checking 5.1 Why check models? 5.2 Posterior Predictive Checks 5.3 Sources", " 5 Model Checking 5.1 Why check models? In theory—Bayesian model should include all relevant substantive knowledge and subsume all possible theories. In practice—It won’t. Need to check how the model fits data. The question is not whether a model is “true”; it isn’t (Box 1976). But is it good enough for the purposes of the analysis. See Gelman, Meng, and Stern (1996), Gelman (2007), Gelman (2009), Gelman et al. (2013 Ch. 6), Gelman and Shalizi (2012b), Kruschke (2013), Gelman and Shalizi (2012a), Gelman (2014) for more discussion of the motivation and use of posterior predictive checks. 5.2 Posterior Predictive Checks One way to evaluate the fit of a model is posterior predictive checks Fit the model to the data to get the posterior distribution of the parameters: \\(p(\\theta | D)\\) Simulate data from the fitted model: \\(p(\\tilde{D} | \\theta, D)\\) Compare the simulated data (or a statistic thereof) to the observed data and a statistic thereof. The comparison between data simulated from the model can be formal or visual. Within a Stan function, this is done in the generated quantities block using a _rng distribution functions: generated quantities { vector[n] yrep; for (i in 1:n) { yrep[i] ~ } } The package bayesplot includes multiple functions for posterior predictive checks; see the help for PPC-overview for a summary of these functions. 5.2.1 Bayesian p-values A posterior predictive p-value is a the tail posterior probability for a statistic generated from the model compared to the statistic observed in the data. Let \\(y = (y_1, \\dots, y_n)\\) be the observed data. Suppose the model has been fit and there is a set of simulation \\(\\theta^(s)\\), \\(s = 1, \\dots, n_sims\\). In replicated dataset, \\(y^{rep(s)\\), has been generated from the predictive distribution of the data, \\(p(y^{(rep)} | \\theta = \\theta^{(s)}\\). Then the ensemble of simulated datasets, \\((y^{rep(s)}, \\dots, y^{rep(nsims)})\\), is a sample from the posterior predictive distribution, \\(p(y^{(rep)} | y)\\) The model can be tested by means of discrepancy statistics, which are some function of the data and parameters, \\(T(y, \\theta)\\). If \\(\\theta\\) was known, then compare discrepancy by \\(T(y^{(rep)}, \\theta)\\). The statistical significance is \\(p = \\Pr(T(y^{(rep)}, \\theta) &gt; T(y, \\theta) | y, \\theta)\\). If \\(\\theta\\) is unknown, then average over the posterior distribution of \\(\\theta\\), \\[ \\begin{aligned}[t] p &amp;= \\Pr(T(y^{(rep)}, \\theta) &gt; T(y, \\theta) | y) \\\\ &amp;= \\int Pr(T(y^{(rep)}, \\theta) &gt; T(y, \\theta) | y, \\theta) p(\\theta | y) d\\,\\theta , \\end{aligned} \\] which is easily estimated from the MCMC samples as, \\[ p = \\frac{1}{n_{sims}}\\sum_{s = 1}^{n_{sims}} 1( T(y^{rep(s)}, \\theta(s)) &gt; T(y, \\theta(s))) \\] 5.2.2 Test quantities The definition of a posterior p-value does not specify a particular test-statistic, \\(T\\), to use. The best advice is that \\(T\\) depends on the application. Gelman et al. (2013, 146) Speed of light example uses the 90% interval (61st and 6th order statistics). Gelman et al. (2013, 147) binomial trial example uses the number of swicthes (0 to 1, or 1 to 0) in order to test independence. Gelman et al. (2013, 148) hierarchical model for adolesce smoking uses percent of adolescents in the sample who never smoked percentage in the sample who smoked in all waves precentage of “incident smoker”: adolescents who began the study and non-smokers and ended as smokers. 5.2.3 p-values vs. u-values A posterior predictive p-value is different than a classical p-value. Posterior predictive p-value distributed uniform if the model is true Classical p-value distributed uniform if the null hypothesis (\\(H_0\\)) is true A u-value is any function of the data that has a \\(U(0, 1)\\) sampling distribution (Gelman et al. 2013, 151) a u-value can be averaged over \\(\\theta\\), but it is not Bayesian, and is not a probability distribution posterior p-value: probability statement, conditional on model and data, about future observations 5.2.4 Marginal predictive checks Compare statistics for each observation. Conditional Predictive Ordinate (CPO): The CPO (Gelfand 1996) is the leave-on-out cross-validation predictive density: \\[ p(y_i | y_{-i}) = \\int p(y_i | \\theta) p(\\theta | y_{-i}) d\\,\\theta \\] The pointwise predicted LOO probabilities can be calculated using PSIS-LOO or WAIC in the loo package. Predictive Concordance and Predictive QuantilesP Gelfand (1996) classifies any \\(y_i\\) that is outside the central 95% predictive posterior of \\(y^{rep}_i\\) is an outlier. Let the predictive quantile (\\(PQ_i\\)) be \\[ PQ_i = p(y_i^{(rep)} &gt; y_i) . \\] Then the predictive concordance be the proportion of \\(y_i\\) that are not outliers. Gelfand (1996) argues that the predictive concordance should match 95% - in other words that the posterior predictive distribution should have the correct coverage. (Laplace Demon p. 20) 5.2.5 Outliers Can be identified by the inverse-CPO. larger than 40 are possible outliers, and those higher than 70 are extreme values (Ntzoufras 2009, p. 376). Congdon (2005) scales CPO by dividing each by its individual max and considers observations with scaled CPO under 0.01 as outliers. 5.2.6 Grapical Posterior Predictive Checks Visualization can surprise you, but it doesn’t scale well. Modeling scales well, but it can’t surprise you. – paraphrase of Hadley Hickham Instead of calculating posterior probabilities, plot simulated data and observed data and visually compare them. See Gelman et al. (2013, 154). plot simulated data and real data (Gelman et al. 2013, 154). This is similar to ideas in Wickham et al. (2010). plot summary statistics or inferences residual plots Bayesian residuals have a distribution \\(r_i^{(s)} = y_i - \\E(y_i | \\theta^{s})\\) Bayesian resdiual graph plots single realization of the residuals, or a summary of their posterior distributions binned plots are best for discrete data (Gelman et al. 2013, 157) 5.3 Sources See Gelman and Shalizi (2012b), Gelman and Shalizi (2012a), Kruschke (2013) "],
["introduction-to-stan-and-linear-regression.html", "6 Introduction to Stan and Linear Regression 6.1 Prerequites 6.2 The Statistical Model", " 6 Introduction to Stan and Linear Regression This chapter is an introduction to writing and running a Stan model in R. Also see the rstan vignette for similar content. 6.1 Prerequites For this section we will use the duncan dataset included in the car package. Duncan’s occupational prestige data is an example dataset used throughout the popular Fox regression text, Applied Regression Analysis and Generalized Linear Models (Fox 2016). It is originally from Duncan (1961) consists of survey data on the prestige of occupations in the US in 1950, and several predictors: type of occupation, income, and education of that data(&quot;Duncan&quot;, package = &quot;car&quot;) 6.2 The Statistical Model The first step in running a Stan model is defining the Bayesian statistical model that will be used for inference. Let’s run the regression of occupational prestige on the type of occupation, income, and education: \\[ \\begin{multline} y_i = \\beta_0 + \\beta_1 I(\\mathtt{type} = \\mathtt{&quot;prof&quot;}) + \\beta_2 I(\\mathtt{type} = \\mathtt{&quot;wc&quot;}) \\\\ + \\beta_3 \\mathtt{income} + \\beta_4 \\mathtt{education} + \\epsilon_i \\end{multline} \\] duncan_lm &lt;- lm(prestige ~ type + income + education, data = Duncan) duncan_lm #&gt; #&gt; Call: #&gt; lm(formula = prestige ~ type + income + education, data = Duncan) #&gt; #&gt; Coefficients: #&gt; (Intercept) typeprof typewc income education #&gt; -0.185 16.658 -14.661 0.598 0.345 There are \\(n = 45\\) observations in the dataset. Let \\(y\\) be a \\(n \\times 1\\) vector of the values of prestige. Let \\(X\\) be the \\(n \\times k\\) design matrix of the regression. In this case, \\(k = 5\\), \\[ X = \\begin{bmatrix} 1 &amp; \\mathtt{typeprof} &amp; \\mathtt{typewc} &amp; \\mathtt{income} &amp; \\mathtt{education} \\end{bmatrix} \\] In OLS, we get the frequentist estimates of \\(\\hat{\\beta}\\) by minimizing the squared errors, \\[ \\hat{\\beta}_{OLS} = \\argmin_{\\beta} \\sum_{i = 1}^n (y_i - \\beta&#39; x_i)^2 = \\argmin \\sum_{i = 1}^n \\hat{\\epsilon}_i \\] For valid inference we need to make assumptions about \\(\\epsilon_i\\), namely that they are uncorrelated with \\(X\\), \\(\\Cov(\\epsilon, X) = 0\\), and that they are i.i.d, \\(\\Cov(\\epsilon_i, \\epsilon_j) = 0\\), \\(\\Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). However, no specific distributional form is or needs to be assumed for \\(\\epsilon\\) since CLT results show that, asymptotically, the sampling distribution of \\(\\beta\\) is distributed normal. Additionally, although \\(\\hat\\sigma^2 = \\sum_{i = 1}^n \\epsilon_i / (n - k - 1)\\) is a estimator of \\(\\sigma^2\\), standard errors of the standard error of the regression are not directly provided. In Bayesian inference, our target is the posterior distribution of the parameters, \\(\\beta\\) and \\(\\sigma\\): \\(p(\\beta, \\sigma^2 | y, X)\\). Since all uncertainty in Bayesian inference is provided via probability, we will need to explicitly provide parametric distributions for the likelihood and parameters. \\[ p(\\beta, \\sigma | y, X) \\propto p(y | \\beta, \\sigma) p(\\beta, \\sigma) \\] For a Bayesian linear regression model, we’ll need to specify distributions for \\(p(y | \\beta, \\sigma)\\) and \\(p(\\beta, \\sigma)\\). Likelihood: \\(p(y_i | x_i, \\beta, \\sigma)\\) suppose that the observations are distributed independent normal: \\[ y_i \\sim N(\\beta&#39;x_i, \\sigma^2) \\] Priors: The model needs to specify a prior distribution for the parameters \\((\\beta, \\sigma)\\). Rather than specify a single distribution for \\(\\beta\\) and \\(\\sigma\\), it will be easier to specify independent (separate) distributions for \\(\\beta\\) and \\(\\sigma\\). The Stan manual and … provide For the normal distribution, assume i.i.d. normal distributions for each element of \\(\\beta\\): \\[ \\beta_k \\sim N(b, s) \\] For the scale parameter of the normal distribution, \\(\\sigma\\), we will use a half-Cauchy. The Cauchy distribution is a special case of the Student t distribution when the degrees of freedom is 1. In Bayesian stats, it has the property that it concentrates probability mass around its median (zero), but has very wide tails, so if the prior distribution guess is wrong, the parameter can still adapt to data. A half-Cauchy distribution is a Cauchy distribution but with support of \\((0, \\infty)\\) instead of the entire real line. \\[ \\sigma \\sim C^{+}(0, w) \\] Combining all the previous equations, our statistical model for linear regression is, \\[ \\begin{aligned}[t] y &amp;\\sim N(\\mu, \\sigma) \\\\ \\mu &amp;= X \\beta \\\\ \\beta &amp;\\sim N(b, s) \\\\ \\sigma &amp;\\sim C^{+}(0, w) \\end{aligned} \\] This defines a Bayesian model gives us \\[ p(\\beta, \\sigma | y, X, b, s, w) \\propto p(y | X, \\beta) p(\\beta | b, s) p(\\sigma | w) \\] The targets of inference in this model are the two parameters: \\(\\beta\\) (regression coefficients), and \\(\\sigma\\) (standard deviation of the regression). This is conditional on the observed or assumed quantities, which including both the data \\(y\\) (response) and \\(X\\) (predictors), as well the values defining the prior distributions: \\(b\\), \\(s\\), and \\(w\\). Now that we’ve defined a statistical model, we can write it as a Stan model. Stan models are written in its own domain-specific language that focuses on declaring the statistical model (parameters, variables, distributions) while leaving the details of the sampling algorithm to Stan. A Stan model consists of blocks which contain declarations of variables and/or statements. Each block has a specific purpose in the model. functions { // OPTIONAL: user-defined functions } data { // read in data ... } transformed data { // Create new variables/auxiliary variables from the data } parameters { // Declare parameters that will be estimated } transformed parameters { // Create new variables/auxiliary variables from the parameters } model { // Declare your probability model: priors, hyperpriors &amp; likelihood } generated quantities { // Declare any quantities other than simulated parameters to be generated } The file lm.stan is a Stan model for the linear regression model previously defined. data { // number of observations int n; // response vector vector[n] y; // number of columns in the design matrix X int k; // design matrix X matrix [n, k] X; // beta prior real b_loc; real&lt;lower = 0.0&gt; b_scale; // sigma prior real sigma_scale; } parameters { // regression coefficient vector vector[k] b; // scale of the regression errors real&lt;lower = 0.0&gt; sigma; } transformed parameters { // mu is the observation fitted/predicted value // also called yhat vector[n] mu; mu = a + X * b; } model { // priors b ~ normal(b_loc, b_scale); sigma ~ cauchy(0, sigma_scale); // likelihood y ~ normal(mu, sigma); } generated quantities { // simulate data from the posterior vector[n] y_rep; for (i in 1:n) { y_rep[i] = normal_rng(mu[i], sigma); } } mod1 &lt;- stan_model(&quot;stan/lm.stan&quot;) mod1 data { // number of observations int n; // response vector vector[n] y; // number of columns in the design matrix X int k; // design matrix X matrix [n, k] X; // beta prior real b_loc; real b_scale; // sigma prior real sigma_scale; } parameters { // regression coefficient vector vector[k] b; // scale of the regression errors real sigma; } transformed parameters { // mu is the observation fitted/predicted value // also called yhat vector[n] mu; mu = X * b; } model { // priors b ~ normal(b_loc, b_scale); sigma ~ cauchy(0, sigma_scale); // likelihood y ~ normal(mu, sigma); } generated quantities { // simulate data from the posterior vector[n] y_rep; for (i in 1:n) { y_rep[i] = normal_rng(mu[i], sigma); } } See the Stan Modeling Language User’s Guide and Reference Manual for details of the Stan Language. NoteSince a Stan model compiles to C++ code, you may receive some warning messages such as /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] static void set_zero_all_adjoints() { ^ In file included from file1d4a4d50faa.cpp:8: In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/src/stan/model/model_header.hpp:4: As long as your model compiles, you can ignore these compiler warnings (On the other hard, warnings that occur during sampling should not be ignored). If the Stan model does not give you a syntax error when parsing the model, it should compile to valid C++.[^bugs][^c-warnings] See [bugs]: In the rare case that the Stan parser transpiles the Stan model to C++ but cannot compile the C++ code, it is a bug in Stan. Follow the instructions on how to inform the Stan developers about bugs. [c-warnings]: The extended installation instructions for MacOS/Linux and Windows have instructions for adding compiler options to the R Makevars file. 6.2.1 Sampling In order to sample from the model, we need to at least give it the values for the data to use: n, k, y, X, and the data associated with the priors. mod1_data &lt;- list( y = Duncan$prestige, n = nrow(Duncan) ) The data types in Stan are all numeric (either integers or reals), but they include matrices and vectors. However, there is nothing like a data frame in Stan. Whereas in the R function lm we can provide a formula and a data set for where to look for objects, and the function will create the appropriate \\(X\\) matrix for the regression, we will need to create that matrix ourselves—expanding categorical variables to indicator variables, and expanding interactions and other functions of the predictors. However, we need to do that all manually. The function stats is the workhorse function used in lm and many other R functions to convert a formula into the matrix used in estimation. X &lt;- model.matrix(prestige ~ type + income + education, data = Duncan) mod1_data$X &lt;- X mod1_data$k &lt;- ncol(X) We still need to provide the values for the prior distributions. For specific values of the prior distributions, assume uninformative priors for beta by setting the mean to zero and the variances to large numbers. \\[ \\beta_k \\sim N(0, 1000) \\] mod1_data$b_loc &lt;- 0 mod1_data$b_scale &lt;- 1000 For prior of the regression scale parameter \\(\\sigma\\), use a half-Cauchy distribution with a large scale parameter, which is a good choice for the priors of scale parameters. \\[ \\sigma \\sim C^{+}(0, 50) \\] mod1_data$sigma_scale &lt;- 50 Now, sample from the posterior, using the function sampling: mod1_fit &lt;- sampling(mod1, data = mod1_data) 6.2.2 Convergence Diagnostics and Model Fit Convergence Diagnostics: Is this the posterior distribution that you were looking for? These don’t directly say anything about how “good” the model is in terms representing the data, they are only evaluating how well the sampler is doing at sampling the posterior distribution of the given model. If there are problems with these, then the sample results do not represent the posterior distribution, and your inferences will be biased. mcse: n_eff: Rhat divergences Model fit: Is this statistical model appropriate for the data? Or better than other models? Posterior predictive checks Information criteria: WAIC Leave-one-out Cross-Validation "],
["heteroskedasticity-and-robust-regression.html", "7 Heteroskedasticity and Robust Regression 7.1 Prerequisites 7.2 Linear Regression with Student t distributed errors 7.3 Heteroskedasticity 7.4 References", " 7 Heteroskedasticity and Robust Regression 7.1 Prerequisites VGAM is needed for the Laplace distribution. library(&quot;VGAM&quot;) 7.2 Linear Regression with Student t distributed errors Like OLS, Bayesian linear regression with normally distributed errors is sensitive to outliers. The normal distribution has narrow tail probabilities. This plots the normal, Double Exponential (Laplace), and Student-t (df = 4) distributions all with mean 0 and scale 1, and the surprise (\\(- log(p)\\)) at each point. Higher surprise is a lower log-likelihood. Both the Student-t and Double Exponential distributions have surprise values well below the normal in the ranges (-6, 6).3 This means that outliers impose less of a penalty on the log-posterior models using these distributions, and the regression line would need to move less to incorporate those observations since the error distribution will not consider them as unusual. z &lt;- seq(-6, 6, length.out = 100) bind_rows( tibble(z = z, p = dnorm(z, 0, 1), distr = &quot;Normal&quot;), tibble(z = z, p = dt(z, 4), distr = &quot;Student-t (df = 4)&quot;), tibble(z = z, p = VGAM::dlaplace(z, 0, 1), distr = &quot;Double Exponential&quot;)) %&gt;% mutate(`-log(p)` = -log(p)) %&gt;% ggplot(aes(x = z, y = `-log(p)`, colour = distr)) + geom_line() z &lt;- seq(-6, 6, length.out = 100) bind_rows( tibble(z = z, p = dnorm(z, 0, 1), distr = &quot;Normal&quot;), tibble(z = z, p = dt(z, 4), distr = &quot;Student-t (df = 4)&quot;), tibble(z = z, p = VGAM::dlaplace(z, 0, 1), distr = &quot;Double Exponential&quot;)) %&gt;% mutate(`-log(p)` = -log(p)) %&gt;% ggplot(aes(x = z, y = p, colour = distr)) + geom_line() mod_t data { // number of observations int n; // response vector vector[n] y; // number of columns in the design matrix X int k; // design matrix X matrix [n, k] X; // beta prior real b_loc; real b_scale; // sigma prior real sigma_scale; } parameters { // regression coefficient vector vector[k] b; // scale of the regression errors real sigma; real nu; } transformed parameters { // mu is the observation fitted/predicted value // also called yhat vector[n] mu; mu = X * b; } model { // priors b ~ normal(b_loc, b_scale); sigma ~ cauchy(0, sigma_scale); nu ~ gamma(2, 0.1); // likelihood y ~ student_t(nu, mu, sigma); } generated quantities { // simulate data from the posterior vector[n] y_rep; // log-likelihood values vector[n] log_lik; for (i in 1:n) { y_rep[i] = student_t_rng(nu, mu[i], sigma); log_lik[i] = student_t_lpdf(y[i] | nu, mu[i], sigma); } } unionization &lt;- read_tsv(&quot;data/western1995/unionization.tsv&quot;, col_types = cols( country = col_character(), union_density = col_double(), left_government = col_double(), labor_force_size = col_number(), econ_conc = col_double() )) mod_data &lt;- lm_preprocess(union_density ~ left_government + log(labor_force_size) + econ_conc, data = unionization) mod_data &lt;- within(mod_data, { b_loc &lt;- 0 b_scale &lt;- 1000 sigma_scale &lt;- sd(y) }) The max_treedepth parameter needed to be increased because in some runs it was hitting the maximum tree depth. This is likely due to the wide tails of the Student t distribution. mod_t_fit &lt;- sampling(mod_t, data = mod_data, control = list(max_treedepth = 11)) #&gt; #&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.924295 seconds (Warm-up) #&gt; 0.797333 seconds (Sampling) #&gt; 1.72163 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 1 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.912943 seconds (Warm-up) #&gt; 0.842836 seconds (Sampling) #&gt; 1.75578 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 2 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.978823 seconds (Warm-up) #&gt; 0.815824 seconds (Sampling) #&gt; 1.79465 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 3 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is 0, but must be &gt; 0! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;lm_student_t&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.903471 seconds (Warm-up) #&gt; 0.748265 seconds (Sampling) #&gt; 1.65174 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 4 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. summary(mod_t_fit, pars = c(&quot;b&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; b[1] 90.924 2.19841 66.781 -44.196 47.81 91.762 133.164 223.22 923 1 #&gt; b[2] 0.273 0.00162 0.083 0.103 0.22 0.275 0.328 0.43 2626 1 #&gt; b[3] -6.082 0.13953 4.322 -14.791 -8.92 -6.101 -3.263 2.57 959 1 #&gt; b[4] 2.763 0.74224 22.668 -43.434 -11.60 2.445 17.292 48.50 933 1 Compare those results when using a model with mod_normal data { // number of observations int n; // response vector vector[n] y; // number of columns in the design matrix X int k; // design matrix X matrix [n, k] X; // beta prior real b_loc; real b_scale; // sigma prior real sigma_scale; } parameters { // regression coefficient vector vector[k] b; // scale of the regression errors real sigma; } transformed parameters { // mu is the observation fitted/predicted value // also called yhat vector[n] mu; mu = X * b; } model { // priors b ~ normal(b_loc, b_scale); sigma ~ cauchy(0, sigma_scale); // likelihood y ~ normal(mu, sigma); } generated quantities { // simulate data from the posterior vector[n] y_rep; // log-likelihood posterior vector[n] log_lik; for (i in 1:n) { y_rep[i] = normal_rng(mu[i], sigma); log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); } } mod_normal_fit &lt;- sampling(mod_normal, data = mod_data) #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.551241 seconds (Warm-up) #&gt; 0.482792 seconds (Sampling) #&gt; 1.03403 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 9e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.507048 seconds (Warm-up) #&gt; 0.455422 seconds (Sampling) #&gt; 0.96247 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.493311 seconds (Warm-up) #&gt; 0.568471 seconds (Sampling) #&gt; 1.06178 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 9e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.511245 seconds (Warm-up) #&gt; 0.405449 seconds (Sampling) #&gt; 0.916694 seconds (Total) summary(mod_normal_fit, pars = c(&quot;b&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; b[1] 95.987 2.41105 63.9174 -27.7095 53.147 95.592 137.966 223.008 703 #&gt; b[2] 0.270 0.00194 0.0842 0.0979 0.217 0.273 0.326 0.434 1891 #&gt; b[3] -6.356 0.15521 4.2031 -14.8646 -9.116 -6.339 -3.532 1.768 733 #&gt; b[4] 0.858 0.80219 21.5103 -41.1619 -13.309 0.972 15.252 43.658 719 #&gt; Rhat #&gt; b[1] 1.01 #&gt; b[2] 1.00 #&gt; b[3] 1.01 #&gt; b[4] 1.01 7.2.1 Double Exponential (Laplace) Errors An alternative form of “robust” regression is to use the Double Exponential (Laplace) distributions for the errors. This is the equivalent to least median regression, where the regression line is the median (50% quantile) mod_dbl_exp data { // number of observations int n; // response vector vector[n] y; // number of columns in the design matrix X int k; // design matrix X matrix [n, k] X; // beta prior real b_loc; real b_scale; // sigma prior real sigma_scale; } parameters { // regression coefficient vector vector[k] b; // scale of the regression errors real sigma; } transformed parameters { // mu is the observation fitted/predicted value vector[n] mu; // tau are obs-level scale params mu = X * b; } model { // priors b ~ normal(b_loc, b_scale); sigma ~ cauchy(0, sigma_scale); // likelihood y ~ double_exponential(mu, sigma); } generated quantities { // simulate data from the posterior vector[n] y_rep; // log-likelihood values vector[n] log_lik; // use a single loop since both y_rep and log_lik are elementwise for (i in 1:n) { y_rep[i] = double_exponential_rng(mu[i], sigma); log_lik[i] = double_exponential_lpdf(y[i] | mu[i], sigma); } } summary(mod_dbl_exp_fit, par = c(&quot;b&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; b[1] 60.671 2.58294 72.5324 -84.418 15.896 56.321 105.938 207.022 789 #&gt; b[2] 0.298 0.00217 0.0815 0.126 0.248 0.304 0.354 0.442 1415 #&gt; b[3] -4.303 0.15685 4.4911 -13.482 -7.160 -4.100 -1.555 4.641 820 #&gt; b[4] 13.381 0.91373 25.6984 -38.570 -2.215 14.588 29.270 64.348 791 #&gt; Rhat #&gt; b[1] 1 #&gt; b[2] 1 #&gt; b[3] 1 #&gt; b[4] 1 Model comparison loo_t &lt;- loo(extract_log_lik(mod_normal_fit, &quot;log_lik&quot;)) #&gt; Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k- #&gt; diagnostic&#39;) for details. loo_normal &lt;- loo(extract_log_lik(mod_t_fit, &quot;log_lik&quot;)) #&gt; Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k- #&gt; diagnostic&#39;) for details. loo_dbl_exp &lt;- loo(extract_log_lik(mod_dbl_exp_fit, &quot;log_lik&quot;)) 7.3 Heteroskedasticity In applied regression, heteroskedasticity consistent (HC) or robust standard errors are often used. However, there is straightforwardly direct translation of HC standard error to regression model this in a Bayesian setting. The sandwich method of estimating HC errors uses the same point estimates for the regression coefficients as OLS, but estimates the standard errors of those coefficients in a second stage from the OLS residuals. Disregarding differences in frequentist vs. Bayesian inference, it is clear that a direct translation of that method could not be fully Bayesian since the coefficients and errors are not estimated jointly. In a linear normal regression model with heteroskedasticity, each observation has its own scale parameter, \\(\\sigma_i\\), \\[ \\begin{aligned}[t] y_i &amp;\\sim \\dnorm(X \\beta, \\sigma_i) . \\end{aligned} \\] It should be clear that without proper priors this model is not identified, meaning that the posterior distribution is improper. To estimate this model we have to apply some model to the scale terms, \\(\\sigma_i\\). In fact, you can think of homoskedasticity as the simplest such model; assuming that all \\(\\sigma_i = \\sigma\\). A more general model of \\(\\sigma_i\\) should encode any information the analyst has about the scale terms. This can be a distribution or functions of covariates for how we think observations may have different values. 7.3.1 Covariates A simple model of heteroskedasticity is if the observations can be split into groups. Suppose the observations are partitioned into \\(k = 1, \\dots, K\\) groups, and \\(k[i]\\) is the group of observation \\(i\\), \\[ \\sigma_i = \\sigma_{k[i]} \\] Another choice would be to model the scale term with a regression model, for example, \\[ \\log(\\sigma_i) \\sim \\dnorm(X \\gamma, \\tau) \\] 7.3.2 Student-t Error The Student-t distribution of error terms from the Robust Regression chapter is also model of heteroskedasticity. A reparameterization that will be used quite often is to rewrite a normal distributions with unequal scale parameters as the product of a common global scale parameter (\\(\\sigma\\)), and observation specific local scale parameters, \\(\\lambda_i\\),[^globalmixture] \\[ y_i \\sim \\dnorm(X\\beta, \\lambda_i \\sigma) . \\] If the local variance parameters are distributed inverse-gamma, \\[ \\lambda^2 \\sim \\dinvgamma(\\nu / 2, \\nu / 2) \\] then the above is equivalent to a regression with errors distributed Student-t errors with \\(\\nu\\) degrees of freedom, \\[ y_i \\sim \\dt{\\nu}(X \\beta, \\sigma) . \\] [^globalmixture] See this for a visualization of a Student-t distribution a mixture of Normal distributions, and this for a derivation of the Student t distribution as a mixture of normal distributions. This scale mixture of normal representation will also be used with shrinkage priors on the regression coefficients. Example: Simulate Student-t distribution with \\(\\nu\\) degrees of freedom as a scale mixture of normal. For *s in 1:S$, Simulate \\(z_s \\sim \\dgamma(\\nu / 2, \\nu / 2)\\) \\(x_s = 1 / \\sqrt{z_s}2\\) is draw from \\(\\dt{\\nu}(0, 1)\\). When using R, ensure that you are using the correct parameterization of the gamma distribution. Left to reader 7.4 References 7.4.1 Robust regression See Gelman and Hill (2007 sec 6.6), Gelman et al. (2013 ch 17) Team (2016 Sec 8.4) for the Stan example using a Student-t distribution 7.4.2 Heteroskedasticity Gelman et al. (2013 Sec. 14.7) for models with unequal variances and correlations. Team (2016) reparameterizes the Student t distribution as a mixture of gamma distributions in Stan. 7.4.3 Qunatile regression Benoit and Poel (2017) Yu and Zhang (2005) for the three-parameter asymmetric Laplace distribution The Double Exponential distribution still has a thinner tail than the Student-t at higher values.↩ "],
["generalized-linear-models.html", "8 Generalized Linear Models 8.1 Generalized Linear Models 8.2 Count Models 8.3 Example 8.4 Negative Binomial 8.5 Multinomial / Categorical Models 8.6 Gamma Regression 8.7 Beta Regression 8.8 Ordered Logistic 8.9 References", " 8 Generalized Linear Models 8.1 Generalized Linear Models Generalized linear models (GLMs) are a class of commonly used models.[^glm-r] In GLMs, the mean is specified as a function of a linear model of predictors, \\[ E(Y) = \\mu = g^{-1}(\\mat{X} \\vec{\\beta}) . \\] GLMs are a generalization of linear regression from an unbounded continuous outcome variable to other types of data: binary, count, categorical, bounded continuous. A GLM consists of three components: A probability distribution (family) specifying the conditional distribution of the response variable. In GLMs, the distribution is in the exponential family: Normal, Binomial, Poisson, Categorical, Multinomial, Poisson, Beta. A linear predictor, which is a linear function of the predictors, \\[ \\eta = \\mat{X} \\vec{\\beta} . \\] A link function (\\(g(.)\\)) which maps the expected value to the the linear predictor, \\[ g(\\mu) = \\eta . \\] The link function is smooth and invertible, and the inverse link function or mean function maps the linear predictor to the mean, \\[ \\mu = g^{-1}(\\eta) . \\] The link function (\\(g\\)) and its inverse ($g^{-1}) translate \\(\\eta\\) from \\((\\-infty, +\\infty)\\) to the proper range for the probability distribution and back again. These models are often estimated with MLE, as with the function stats. However, these are also easily estimated in a Bayesian setting. See the help for stats for common probability distributions, stats for common links, and the Wikipedia page for a table of common GLMs. See the function VGAM for even more examples of link functions and probability distributions. Common Link Functions and their inverses. Table derived from Fox (2016, 419). Link Range of \\(\\mu_i\\) \\(\\eta_i = g(\\mu_i)\\) \\(\\mu_i = g^{-1}(\\eta)_i\\) Identity \\((-\\infty, \\infty)\\) \\(\\mu_i\\) \\(\\eta_i\\) Inverse \\((-\\infty, \\infty) \\setminus \\{0\\}\\) \\(\\mu_i^{-1}\\) \\(\\eta_i^{-1}\\) Log \\((0, \\infty)\\) \\(\\log(\\mu_i)\\) \\(\\exp(\\eta_i)\\) Inverse-square \\((0, \\infty)\\) \\(\\mu_i^{-2}\\) \\(\\eta_i^{-1/2}\\) Square-root \\((0, \\infty)\\) \\(\\sqrt{\\mu_i}\\) \\(\\eta_{i}^2\\) Logit \\((0, 1)\\) \\(\\log(\\mu / (1 - \\mu_i)\\) \\(1 / (1 + \\exp(-\\eta_i))\\) Probit \\((0, 1)\\) \\(\\Phi^{-1}(\\mu_i)\\) \\(\\Phi(\\eta_i)\\) Cauchit \\((0, 1)\\) \\(\\tan(\\pi (\\mu_i - 1 / 2))\\) \\(\\frac{1}{\\pi} \\arctan(\\eta_i) + \\frac{1}{2}\\) Log-log \\((0, 1)\\) \\(-\\log(-log(\\mu_i))\\) \\(\\exp(-\\exp(-\\eta_i))\\) Complementary Log-log \\((0, 1)\\) \\(\\log(-log(1 - \\mu_i))\\) \\(1 - \\exp(-\\exp(\\eta_i))\\) Common distributions and link functions. Table derived from Fox (2016, 421), Wikipedia, and stats. Distribution Canonical Link Range of \\(Y_i\\) Other link functions Normal Identity real: \\((-\\infty, +\\infty)\\) log, inverse Exponential Inverse real: \\((0, +\\infty)\\) identity, log Gamma Inverse real: \\((0, +\\infty)\\) identity, log Inverse-Gaussian Inverse-squared real: \\((0, +\\infty)\\) inverse, identity, log Bernoulli Logit integer: \\(\\{0, 1\\}\\) probit, cauchit, log, cloglog Binomial Logit integer: \\(0, 1, \\dots, n_i\\) probit, cauchit, log, cloglog Poisson Log integer: \\(0, 1, 2, \\dots\\) identity, sqrt Categorical Logit \\(0, 1, \\dots, K\\) Multinomial Logit K-vector of integers, \\(\\{x_1, \\dots, x_K\\}\\) s.t. \\(\\sum_k x_k = N\\). 8.2 Count Models 8.2.1 Poisson The Poisson model is used for unbounded count data, \\[ Y = 0, 1, \\dots, \\infty \\] The outcome is modeled as a Poisson distribution \\[ y_i \\sim \\dpois(\\lambda_i) \\] with positive mean parameter \\(\\lambda_i \\in (0, \\infty)\\). Since \\(\\lambda_i\\) has to be positive, the most common link function is the log, \\[ \\log(\\lambda_i) = \\exp(\\vec{x}_i&#39; \\vec{\\beta}) \\] which has the inverse, \\[ \\lambda_i = \\log(\\vec{x}_i \\vec{\\beta}) \\] In Stan, the Poisson distribution has two implementations: poisson_lpdf poisson_log_lpdf: Poisson with a log link. This is for numeric stability. Also, rstanarm supports the Poisson. 8.3 Example A regression model of bilateral sanctions for the period 1939 to 1983. The outcome variable is the number of countries imposing sanctions. data(&quot;sanction&quot;, package = &quot;Zelig&quot;) library(&quot;rstan&quot;) library(&quot;tidyverse&quot;) library(&quot;magrittr&quot;) #&gt; #&gt; Attaching package: &#39;magrittr&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; set_names #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; extract #&gt; The following object is masked from &#39;package:rstan&#39;: #&gt; #&gt; extract URL &lt;- &quot;https://raw.githubusercontent.com/carlislerainey/priors-for-separation/master/br-replication/data/need.csv&quot; autoscale &lt;- function(x, center = TRUE, scale = TRUE) { nvals &lt;- length(unique(x)) if (nvals &lt;= 1) { out &lt;- x } else if (nvals == 2) { out &lt;- if (scale) { (x - min(x, na.rm = TRUE)) / diff(range(x, finite = TRUE)) } else x if (center) { out &lt;- x - mean(x) } } else { out &lt;- if (center) { x - mean(x, na.rm = TRUE) } else x out &lt;- if (scale) out / sd(out, na.rm = TRUE) } out } f &lt;- (oppose_expansion ~ dem_governor + obama_win + gop_leg + percent_uninsured + income + percent_nonwhite + percent_metro) br &lt;- read_csv(URL) %&gt;% mutate(oppose_expansion = 1 - support_expansion, dem_governor = -1 * gop_governor, obama_win = as.integer(obama_share &gt;= 0.5), percent_nonwhite = percent_black + percent_hispanic) %&gt;% rename(gop_leg = legGOP) %&gt;% # keep only variables in the formula model.frame(f, data = .) %&gt;% # drop missing values (if any?) drop_na() #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_integer(), #&gt; state = col_character(), #&gt; state_abbr = col_character(), #&gt; house12 = col_double(), #&gt; sen12 = col_double(), #&gt; support_expansion_new = col_character(), #&gt; percent_uninsured = col_double(), #&gt; ideology = col_double(), #&gt; income = col_double(), #&gt; percent_black = col_double(), #&gt; percent_hispanic = col_double(), #&gt; percent_metro = col_double(), #&gt; dsh = col_double(), #&gt; obama_share = col_double() #&gt; ) #&gt; See spec(...) for full column specifications. br_scaled &lt;- br %&gt;% # Autoscale all vars but response mutate_at(vars(-oppose_expansion), autoscale) glm(f, data = br, family = &quot;binomial&quot;) %&gt;% summary() #&gt; #&gt; Call: #&gt; glm(formula = f, family = &quot;binomial&quot;, data = br) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.374 -0.461 -0.131 0.630 2.207 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 4.5103 4.5986 0.98 0.327 #&gt; dem_governor -4.1556 1.4794 -2.81 0.005 ** #&gt; obama_win -2.1470 1.3429 -1.60 0.110 #&gt; gop_leg -0.1865 1.2974 -0.14 0.886 #&gt; percent_uninsured -0.3072 0.1651 -1.86 0.063 . #&gt; income -0.0421 0.0776 -0.54 0.587 #&gt; percent_nonwhite 17.8505 48.3030 0.37 0.712 #&gt; percent_metro -12.4390 32.4446 -0.38 0.701 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 68.593 on 49 degrees of freedom #&gt; Residual deviance: 37.948 on 42 degrees of freedom #&gt; AIC: 53.95 #&gt; #&gt; Number of Fisher Scoring iterations: 5 library(&quot;rstanarm&quot;) fit1 &lt;- stan_glm(f, data = br, family = &quot;binomial&quot;) #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 7.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.76 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.227826 seconds (Warm-up) #&gt; 0.216907 seconds (Sampling) #&gt; 0.444733 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.2258 seconds (Warm-up) #&gt; 0.234074 seconds (Sampling) #&gt; 0.459874 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.232678 seconds (Warm-up) #&gt; 0.224351 seconds (Sampling) #&gt; 0.457029 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.221657 seconds (Warm-up) #&gt; 0.224032 seconds (Sampling) #&gt; 0.445689 seconds (Total) fit2 &lt;- stan_glm(f, data = br, prior = NULL, family = &quot;binomial&quot;) #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.56167 seconds (Warm-up) #&gt; 0.243438 seconds (Sampling) #&gt; 1.80511 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.37833 seconds (Warm-up) #&gt; 0.208435 seconds (Sampling) #&gt; 1.58676 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.06808 seconds (Warm-up) #&gt; 0.245541 seconds (Sampling) #&gt; 1.31363 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.21401 seconds (Warm-up) #&gt; 0.217275 seconds (Sampling) #&gt; 1.43128 seconds (Total) 8.4 Negative Binomial The Negative Binomial model is also used for unbounded count data, \\[ Y = 0, 1, \\dots, \\infty \\] The Poisson distribution has the restriction that the mean is equal to the variance, \\(\\E(X) = \\Var(X) = \\lambda\\). The Negative Binomial distribution has an additional parameter that allows the variance to vary (though it is always larger than the mean). The outcome is modeled as a negative binomial distribution, \\[ y_i \\sim \\dbinom(\\alpha_i, \\beta) \\] with shape \\(\\alpha \\in \\R^{+}\\) and inverse scale \\(\\beta \\in \\R^{+}\\), and \\(\\E(y) = \\alpha_i / \\beta\\) and \\(\\Var(Y) = \\frac{\\alpha_i}{\\beta^2}(\\beta + 1)\\). Then the mean can be modeled and transformed to the \\[ \\begin{aligned}[t] \\mu_i &amp;= \\log( \\vec{x}_i \\vec{\\gamma} ) \\\\ \\alpha_i &amp;= \\mu_i / \\beta \\end{aligned} \\] Important The negative binomial distribution has many different parameterizations. An alternative parameterization of the negative binomial uses the mean and a over-dispersion parameter. \\[ y_i \\sim \\dnbinomalt(\\mu_i, \\phi) \\] with location parameter \\(\\mu \\in \\R^{+}\\) and over-dispersion parameter \\(\\phi \\in \\R^{+}\\), and \\(\\E(y) = \\mu_i\\) and \\(\\Var(Y) = \\mu_i + \\frac{\\mu_i^2}{\\phi}\\). Then the mean can be modeled and transformed to the \\[ \\begin{aligned}[t] \\mu_i &amp;= \\log( \\vec{x}_i \\vec{\\gamma} ) \\\\ \\end{aligned} \\] In Stan, there are multiple parameterizations of the neg_binomial_lpdf(y | alpha, beta)with shape parameter alpha and inverse scale parameter beta. neg_binomial_2_lpdf(y | mu, phi) with mean mu and over-dispersion parameter phi. neg_binomial_2_log_lpdf(y | eta, phi) with log-mean eta and over-dispersion parameter phi Also, rstanarm supports Poisson and negative binomial models. Gelman et al. (2013 Ch 16) 8.4.1 References For general references on count models see Gelman and Hill (2007, 109–16) McElreath (2016 Ch 10) Fox (2016 Ch. 14) Gelman et al. (2013 Ch. 16) 8.5 Multinomial / Categorical Models 8.6 Gamma Regression The response variable is continuous and positive. In gamma regression, the coefficient of variation is constant rather than the variance. \\[ y_i \\sim \\dgamma(\\alpha_i, \\beta) \\] and \\[ \\begin{aligned}[t] \\alpha_i &amp;= \\mu_i / \\beta \\\\ \\mu_i &amp;= \\vec{x}_i \\vec{\\gamma} \\end{aligned} \\] In Stan, gamma(y | alpha, beta) with shape parameter \\(\\alpha &gt; 0\\) and inverse scale parameter \\(\\beta &gt; 0\\). Then \\(\\E(Y) = \\alpha / \\beta\\) and \\(\\Var(Y) = \\alpha / \\beta^2\\). 8.7 Beta Regression This is for a response variable that is a proportion, \\(y_i \\in (0, 1)\\), \\[ y_i \\sim \\dbeta(\\alpha_i, \\beta_i) \\] and \\[ \\begin{aligned}[t] \\mu_i &amp;= g^{-1}(\\vec{x}_i&#39; \\vec{\\gamma}) \\\\ \\alpha_i &amp;= \\mu_i \\phi \\\\ \\beta_i &amp;= (1 - \\mu_i) \\phi \\end{aligned} \\] Additionally, the \\(\\phi\\) parameter could also be modeled. In Stan: beta(y | alpha, beta) with positive prior successes plus one, \\(\\alpha &gt; 0\\), and negative prior failures plus one, \\(\\beta &gt; 0\\). Then \\(\\E(Y) = \\alpha / (\\alpha + \\beta)\\) and \\(\\Var(Y) = \\alpha\\beta / ((\\alpha + \\beta)^2 (\\alpha + \\beta + 1))\\). rstanarm function rstasnarm See: Ferrari and Cribari-Neto (2004), Cribari-Neto and Zeileis (2010), and Grün, Kosmidis, and Zeileis (2012) on beta regression. rstanarm documentation Modeling Rates/Proportions using Beta Regression with rstanarm 8.8 Ordered Logistic See rstanarm function rstasnarm Gelman and Hill (2007 Ch 6.5) *rstanarm** vignette Estimating Ordinal Regression Models with rstanarm 8.9 References Texts: Gelman et al. (2013 Ch 16) Gelman and Hill (2007 Ch. 5-6) McElreath (2016 Ch. 9) King (1998) discusses MLE estimation of many common GLM models Many econometrics/statistics textbooks, e.g. Fox (2016), discuss GLMs. Though they are not derived from a Bayesian context, they can easily transferred. "],
["binomial-models.html", "9 Binomial Models 9.1 Rare Events Logit 9.2 Case Control", " 9 Binomial Models Binomial models are used to an outcome that is a bounded integer, \\[ y_i \\in 0, 1, 2, \\dots, n . \\] The outcome is distributed Binomial, \\[ \\begin{aligned}[t] y_i \\sim \\dbin \\left(n_i, \\pi \\right) \\end{aligned} \\] A binary outcome is a common special case, \\[ y_i \\in \\{0, 1\\}, \\] and \\[ \\begin{aligned}[t] y_i &amp;\\sim \\dbin \\left(1, \\pi \\right) &amp; \\text{for all $i$} \\\\ \\end{aligned} \\] Depending on the link function, these are logit and probit models that appear in the literature. 9.0.1 Link Functions {link-function} The parameter \\(\\pi \\in (0, 1)\\) is often modeled with a link function is and a linear predictor. \\[ \\pi_i = g^{-1}(\\vec{x}_i \\vec{\\beta}}) \\] There are several common link functions, but they all have to map \\(R \\to (0, 1)\\).4 Logit: The logistic function, \\[ \\pi_i = \\logistic(x_i\\T \\beta) = \\frac{1}{1 + \\exp(- x_i\\T\\beta)} . \\] Stan function softmax. Probit: The CDF of the normal distribution. \\[ \\pi_i = \\Phi(x_i\\T \\beta) \\] Stan function normal_cdf. cauchit: The CDF of the Cauchy distribution. Stan function cauchy_cdf. cloglog: The inverse of the conditional log-log function (cloglog) is \\[ \\pi_i = 1 - \\exp(-\\exp(x_i\\T \\beta)) . \\] Stan function inv_cloglog. Of these link functions, the probit has the narrowest tails (sensitivity to outliers), followed by the logit, and cauchit. The cloglog function is different in that it is asymmetric.5 At zero its value is above 0.5, whereas the cauchit, logit, and probit links all equal 0.5 at 0, make.link(&quot;cloglog&quot;)$linkinv(0) #&gt; [1] 0.632 map(c(&quot;logit&quot;, &quot;probit&quot;, &quot;cauchit&quot;, &quot;cloglog&quot;), make.link) %&gt;% map_df( function(link) { tibble(x = seq(-4, 4, length.out = 101), y = link$linkinv(x), link_name = link$name) } ) %&gt;% ggplot(aes(x = x, y = y, colour = link_name)) + geom_line() 9.0.2 Stan In Stan, the Binomial distribution has two implementations: binomial_lpdf binomial_logit_lpdf. The later implementation is for numeric stability. Taking an exponential of a value can be numerically unstable, and binomial_logit_lpdf input is on the logit scale: Whereas, \\[ y_i \\sim \\mathsf{binomial}(1 / (1 + \\exp(x_i \\beta))) \\] the following is true, \\[ y_i \\sim \\mathsf{binomial\\_logit}(x_i \\beta) \\] 9.0.3 Example: Vote Turnout A general Stan model for estimating logit models is: mod1 // Logit Model // // y ~ Bernoulli(p) // p = a + X B // b0 \\sim cauchy(0, 10) // b \\sim cauchy(0, 2.5) data { // number of observations int N; // response // vectors are only real numbers // need to use an array int y[N]; // number of columns in the design matrix X int K; // design matrix X // should not include an intercept matrix [N, K] X; } transformed data { # default scales same as rstanarm # assume data is centered and scaled real a_scale; vector[K] b_scale; a_scale = 10.0; b_scale = rep_vector(2.5, K); } parameters { // regression coefficient vector real a; vector[K] b; } transformed parameters { vector[N] p; p = inv_logit(a + X * b); } model { // priors a ~ normal(0.0, a_scale); b ~ normal(0.0, b_scale); // likelihood y ~ binomial(1, p); } generated quantities { // simulate data from the posterior vector[N] y_rep; // log-likelihood posterior vector[N] log_lik; for (i in 1:N) { y_rep[i] = binomial_rng(1, p[i]); log_lik[i] = binomial_lpmf(y[i] | 1, p[i]); } } Estimate a model of vote turnout in the 1992 from the American National Election Survey (ANES). The data is from Zelig.6 data(&quot;turnout&quot;, package = &quot;Zelig&quot;) Vote choice (vote) is modeled as a function of age, income, and race. mod_formula &lt;- vote ~ poly(age, 2) + income + educate + race - 1 mod1_data &lt;- lm_preprocess(mod_formula, data = turnout) 9.0.4 Separation *Separation(https://en.wikipedia.org/wiki/Separation_(statistics)* is when a predictor perfectly predicts a binary response variable (Rainey 2016a, Zorn (2005)) complete separation: the predictor perfectly predicts both 0’s and 1’s. quasi-complete separation: the predictor perfectly predicts either 0’s or 1’s. This is related and similar to identification in MLE and multicollinearity in OLS. The general solution is to penalize the likelihood, which in a Bayesian context is equivalent to placing a proper prior on the coefficient of the separating variable. Using a weakly informatin prior such as those suggested by is sufficient to solve separation, \\[ \\beta_k \\sim N(0, 2.5) \\] where all the columns of \\(\\code{x}\\) are assumed to mean zero, unit variance (or otherwise standardized). The half-Cauchy prior, \\(C^{+}(0, 2.5)\\), suggested in Gelman et al. (2008) is insufficiently informative to to deal with separation (Ghosh, Li, and Mitra 2015), but finite-variance weakly informative Student-t or Normal distributions will work. These are the priors suggested by Stan and used by default in rstanarm rstanarm. Rainey (2016a) provides a mixed MLE/Bayesian simulation based approach to apply a prior to the variable with separation, while keeping the other coefficients at their MLE values. Since the results are highly sensitive to the prior, multiple priors should be tried (informative, skeptical, and enthusiastic). Firth (1993) suggests the Jeffreys invariant prior, \\[ p(\\beta_k) \\propto |I(\\beta)|^{\\frac{1}{2}} \\] where \\(|I(\\beta)|\\) is the information matrix, \\[ \\begin{aligned}[t] I(\\beta) &amp;= \\mat{X}\\T \\mat{W} \\mat{X} \\\\ \\mat{W} &amp;= \\diag(\\pi_i (1 - \\pi_i)) \\end{aligned} \\] This is the Jeffreys invariant prior. This was also recommended Zorn (2005). Greenland and Mansournia (2015) suggest a log-F prior distribution which has an intuitive interpretation related to the number of observations. 9.0.4.1 Example: Support of ACA Medicaid Expansion This example is from Rainey (2016a) from the original paper Barrilleaux and Rainey (2014) with replication code here. library(&quot;rstan&quot;) library(&quot;tidyverse&quot;) library(&quot;magrittr&quot;) #&gt; #&gt; Attaching package: &#39;magrittr&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; set_names #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; extract #&gt; The following object is masked from &#39;package:rstan&#39;: #&gt; #&gt; extract URL &lt;- &quot;https://raw.githubusercontent.com/carlislerainey/priors-for-separation/master/br-replication/data/need.csv&quot; f &lt;- (oppose_expansion ~ dem_governor + obama_win + gop_leg + percent_uninsured + income + percent_nonwhite + percent_metro) br &lt;- read_csv(URL) %&gt;% mutate(oppose_expansion = 1 - support_expansion, dem_governor = -1 * gop_governor, obama_win = as.integer(obama_share &gt;= 0.5), percent_nonwhite = percent_black + percent_hispanic) %&gt;% rename(gop_leg = legGOP) %&gt;% # keep only variables in the formula model.frame(f, data = .) %&gt;% # drop missing values (if any?) drop_na() #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_integer(), #&gt; state = col_character(), #&gt; state_abbr = col_character(), #&gt; house12 = col_double(), #&gt; sen12 = col_double(), #&gt; support_expansion_new = col_character(), #&gt; percent_uninsured = col_double(), #&gt; ideology = col_double(), #&gt; income = col_double(), #&gt; percent_black = col_double(), #&gt; percent_hispanic = col_double(), #&gt; percent_metro = col_double(), #&gt; dsh = col_double(), #&gt; obama_share = col_double() #&gt; ) #&gt; See spec(...) for full column specifications. br_scaled &lt;- br %&gt;% # Autoscale all vars but response mutate_at(vars(-oppose_expansion), autoscale) glm(f, data = br, family = &quot;binomial&quot;) %&gt;% summary() #&gt; #&gt; Call: #&gt; glm(formula = f, family = &quot;binomial&quot;, data = br) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.374 -0.461 -0.131 0.630 2.207 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 4.5103 4.5986 0.98 0.327 #&gt; dem_governor -4.1556 1.4794 -2.81 0.005 ** #&gt; obama_win -2.1470 1.3429 -1.60 0.110 #&gt; gop_leg -0.1865 1.2974 -0.14 0.886 #&gt; percent_uninsured -0.3072 0.1651 -1.86 0.063 . #&gt; income -0.0421 0.0776 -0.54 0.587 #&gt; percent_nonwhite 17.8505 48.3030 0.37 0.712 #&gt; percent_metro -12.4390 32.4446 -0.38 0.701 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 68.593 on 49 degrees of freedom #&gt; Residual deviance: 37.948 on 42 degrees of freedom #&gt; AIC: 53.95 #&gt; #&gt; Number of Fisher Scoring iterations: 5 fit1 &lt;- stan_glm(f, data = br, family = &quot;binomial&quot;) #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 8.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.85 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.224432 seconds (Warm-up) #&gt; 0.210568 seconds (Sampling) #&gt; 0.435 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.222537 seconds (Warm-up) #&gt; 0.231917 seconds (Sampling) #&gt; 0.454454 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.229284 seconds (Warm-up) #&gt; 0.221269 seconds (Sampling) #&gt; 0.450553 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.217782 seconds (Warm-up) #&gt; 0.220388 seconds (Sampling) #&gt; 0.43817 seconds (Total) fit2 &lt;- stan_glm(f, data = br, prior = NULL, family = &quot;binomial&quot;) #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.55551 seconds (Warm-up) #&gt; 0.243953 seconds (Sampling) #&gt; 1.79946 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.39207 seconds (Warm-up) #&gt; 0.209127 seconds (Sampling) #&gt; 1.6012 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.06268 seconds (Warm-up) #&gt; 0.242076 seconds (Sampling) #&gt; 1.30476 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;bernoulli&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.21102 seconds (Warm-up) #&gt; 0.217264 seconds (Sampling) #&gt; 1.42828 seconds (Total) 9.1 Rare Events Logit 9.2 Case Control In binary outcome variables, sometimes it is useful to sample on the dependent variable. For example, King and Zeng (2001b) and King and Zeng (2001a) discuss applications with respect to conflicts in international relations. For most country-pairs, for most years, there is no conflict. If some data are costly to gather, it may be cost efficient to get data for conflicts and then randomly select a smaller number of non-conflicts on which to gather data. The sample will no longer be representative, but the estimates can be corrected. The reason this works well, is that if there are very few 1’s, additional 0’s have little influence on the estimation (King and Zeng (2001b)). This should hold more generally will unbalanaced classes; in some sense, the amount of effective observations is not much more than the number in the lowest category. King and Zeng (2001b) propose two corrections: Correcting the intercept (prior correctio) Weighting observations The prior correction ntoes that \\[ \\pi_i = \\frac{1}{1 + \\exp(-X \\beta)} \\] The unbalanced sample only affects the intercept. If \\(\\hat\\beta_0\\) is the intercept from the MLE, the case-control corrected intercept \\(\\tilde{\\beta}\\) is, \\[ \\tilde\\beta_0^* = \\hat\\beta_0 - \\ln \\left(\\frac{1 - \\tau}{\\tau} \\frac{\\bar{y}}{1 - \\bar{y}} \\right) \\] In an MLE setting, this can be applied after estimation, but used in any predicted values. In a Bayesian setting, this correct should be applied within the model by adding the offset to the estimation. In a Stan model, this could be implemented by directly incrementing these values data { int N; int y[N]; real tau; } transformed data { real offset; real y_mean; y_mean = mean(y); offset = log((1 - tau) / tau * (y_mean) / (1 - y_mean)); } parameters { real alpha0; } transformed parameters { real alpha; alpha &lt;- alpha0 - offset; } If there was uncertainty about \\(\\tau\\), then \\(\\tau\\) could be modeled as a parameter. It may also be okay to only correct the intercept in a generated quantities block? (not sure). An alternative approach is to use a weighted likelihood: ones are weighted by \\(\\tau / \\bar{y}\\) zeros are weighted by \\((1 - \\tau) / \\bar{1 - \\bar{y}}\\) The log likelihood would then be \\[ \\ln L_w(\\beta | y) = w_1 \\sum_{Y_i = 1} \\ln (\\pi_i) + w_0 \\sum_{Y_i = 0} \\ln (1 - \\pi_i) \\] In Stan, this can be implemented by directly weighting the log-posterior contributions of each observation. For example, something like this, if (y[i]) { target += w * binomial_lpdf(1, pi[i]) } else { target += (1 - w) * binomial_lpdf(1, pi[i]) } See the example for Zelig-relogit 9.2.0.1 References Firth (1993) proposes a penalized likelihood approach using the Jeffreys invariant prior King and Zeng (2001a) and King and Zeng (2001b) apply an approach similar to the penalized likelihood approach for the similar problem of rare events Zorn (2005) also suggests using the Firth logistic regression to avoid perfect separation Rainey (2016a) shows that Cauchy(0, 2.5) priors can be used Greenland and Mansournia (2015) provide another default prior to for binomial models: log F(1,1) and log F(2, 2) priors. These have the nice property that they are interpretable as additional observations. 9.2.1 References For general references on binomial models see Team (2016 Sec. 8.5) McElreath (2016 Ch 10) Gelman and Hill (2007) [Ch. 5; Sec 6.4-6.5] Fox (2016 Ch. 14) Gelman et al. (2013 Ch. 16) Since the cumulative distribution function of a distribution maps reals to \\((0, 1)\\), any CDF can be used as a link function.↩ Beck, Katz, and Tucker (1998) show that the cloglog link function can be derived from a grouped duration model with binary response variables.↩ Example from Zelig-logit.↩ "],
["unbounded-count-models.html", "10 Unbounded Count Models 10.1 Poisson 10.2 Negative Binomial 10.3 Example: Bilateral Sanctions 10.4 Negative Binomial", " 10 Unbounded Count Models Unbounded count models are models in which the response is a natural number with no upper bound, \\[ y_i = 0, 1, \\dots, \\infty . \\] The two distributions most commonly used to model this are Poisson Negative Binomial 10.1 Poisson The Poisson distribution: \\[ y_i \\sim \\dpois(\\lambda_i) , \\] 10.2 Negative Binomial The Negative Binomial model is also used for unbounded count data, \\[ Y = 0, 1, \\dots, \\infty \\] The Poisson distribution has the restriction that the mean is equal to the variance, \\(\\E(X) = \\Var(X) = \\lambda\\). The Negative Binomial distribution has an additional parameter that allows the variance to vary (though it is always larger than the mean). The outcome is modeled as a negative binomial distribution, \\[ y_i \\sim \\dbin(\\alpha_i, \\beta) \\] with shape \\(\\alpha \\in \\R^{+}\\) and inverse scale \\(\\beta \\in \\R^{+}\\), and \\(\\E(y) = \\alpha_i / \\beta\\) and \\(\\Var(Y) = \\frac{\\alpha_i}{\\beta^2}(\\beta + 1)\\). Then the mean can be modeled and transformed to the \\[ \\begin{aligned}[t] \\mu_i &amp;= \\log( \\vec{x}_i \\vec{\\gamma} ) \\\\ \\alpha_i &amp;= \\mu_i / \\beta \\end{aligned} \\] Important The negative binomial distribution has many different parameterizations. An alternative parameterization of the negative binomial uses the mean and a over-dispersion parameter. \\[ y_i \\sim \\dnbinalt(\\mu_i, \\phi) \\] with location parameter \\(\\mu \\in \\R^{+}\\) and over-dispersion parameter \\(\\phi \\in \\R^{+}\\), and \\(\\E(y) = \\mu_i\\) and \\(\\Var(Y) = \\mu_i + \\frac{\\mu_i^2}{\\phi}\\). Then the mean can be modeled and transformed to the \\[ \\begin{aligned}[t] \\mu_i &amp;= \\log( \\vec{x}_i \\vec{\\gamma} ) \\\\ \\end{aligned} \\] 10.2.1 Stan In Stan, there are multiple parameterizations of the neg_binomial_lpdf(y | alpha, beta)with shape parameter alpha and inverse scale parameter beta. neg_binomial_2_lpdf(y | mu, phi) with mean mu and over-dispersion parameter phi. neg_binomial_2_log_lpdf(y | eta, phi) with log-mean eta and over-dispersion parameter phi Also, rstanarm supports Poisson and negative binomial models. 10.2.2 Example: Number of Number o 10.2.3 References For general references on count models see Gelman et al. (2013 Ch 16) Gelman and Hill (2007, 109–16) McElreath (2016 Ch 10) Fox (2016 Ch. 14) Gelman et al. (2013 Ch. 16) where \\(y_i \\in 0, 1, 2, \\dots\\). The parameter \\(\\lambda_i \\in (0, \\infty)\\) is both the mean and variance of the distribution. 10.2.4 Link functions In many applications, \\(\\lambda\\), is modeled as some function of covariates or other parameters. Since \\(\\lambda_i\\) must be positive, the most common link function is the log, \\[ \\log(\\lambda_i) = \\exp(\\vec{x}_i&#39; \\vec{\\beta}) \\] which has the inverse, \\[ \\lambda_i = \\log(\\vec{x}_i \\vec{\\beta}) \\] 10.2.5 Stan In Stan, the Poisson distribution has two implementations: poisson_lpdf poisson_log_lpdf The Poisson with a log link. This is for numeric stability. In rstanarm use rstanarm for a Poisson GLM. rstanarm vignette on counte models. 10.3 Example: Bilateral Sanctions A regression model of bilateral sanctions for the period 1939 to 1983 (Martin 1992). The outcome variable is the number of countries imposing sanctions. mod_poisson1 &lt;- stan_model(&quot;stan/poisson1.stan&quot;) data(&quot;sanction&quot;, package = &quot;Zelig&quot;) f &lt;- num ~ coop + target -1L reg_model_data &lt;- lm_preprocess(f, data = sanction) sanction_data &lt;- list(X = autoscale(reg_model_data$X), y = reg_model_data$y) sanction_data$N &lt;- nrow(sanction_data$X) sanction_data$K &lt;- ncol(sanction_data$X) fit_sanction_pois &lt;- sampling(mod_poisson1, data = sanction_data) #&gt; #&gt; SAMPLING FOR MODEL &#39;poisson1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 4.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.09828 seconds (Warm-up) #&gt; 0.095359 seconds (Sampling) #&gt; 0.193639 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;poisson1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.101347 seconds (Warm-up) #&gt; 0.103472 seconds (Sampling) #&gt; 0.204819 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;poisson1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.101324 seconds (Warm-up) #&gt; 0.107138 seconds (Sampling) #&gt; 0.208462 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;poisson1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.10045 seconds (Warm-up) #&gt; 0.093848 seconds (Sampling) #&gt; 0.194298 seconds (Total) 10.4 Negative Binomial The Negative Binomial model is also used for unbounded count data, \\[ Y = 0, 1, \\dots, \\infty \\] The Poisson distribution has the restriction that the mean is equal to the variance, \\(\\E(X) = \\Var(X) = \\lambda\\). The Negative Binomial distribution has an additional parameter that allows the variance to vary (though it is always larger than the mean). The outcome is modeled as a negative binomial distribution, \\[ y_i \\sim \\dnegbinom(\\alpha_i, \\beta) \\] with shape \\(\\alpha \\in \\R^{+}\\) and inverse scale \\(\\beta \\in \\R^{+}\\), and \\(\\E(y) = \\alpha_i / \\beta\\) and \\(\\Var(Y) = \\frac{\\alpha_i}{\\beta^2}(\\beta + 1)\\). Then the mean can be modeled and transformed to the \\[ \\begin{aligned}[t] \\mu_i &amp;= \\log( \\vec{x}_i \\vec{\\gamma} ) \\\\ \\alpha_i &amp;= \\mu_i / \\beta \\end{aligned} \\] Important The negative binomial distribution has many different parameterizations. An alternative parameterization of the negative binomial uses the mean and a over-dispersion parameter. \\[ y_i \\sim \\dnbinomalt(\\mu_i, \\phi) \\] with location parameter \\(\\mu \\in \\R^{+}\\) and over-dispersion parameter \\(\\phi \\in \\R^{+}\\), and \\(\\E(y) = \\mu_i\\) and \\(\\Var(Y) = \\mu_i + \\frac{\\mu_i^2}{\\phi}\\). Then the mean can be modeled and transformed to the \\[ \\begin{aligned}[t] \\mu_i &amp;= \\log( \\vec{x}_i \\vec{\\gamma} ) \\\\ \\end{aligned} \\] In Stan, there are multiple parameterizations of the neg_binomial_lpdf(y | alpha, beta)with shape parameter alpha and inverse scale parameter beta. neg_binomial_2_lpdf(y | mu, phi) with mean mu and over-dispersion parameter phi. neg_binomial_2_log_lpdf(y | eta, phi) with log-mean eta and over-dispersion parameter phi Also, rstanarm supports Poisson and negative binomial models. 10.4.1 Example: Economic Sanctions II {ex-econ-sanctions-2} Continuing the economic sanctions example of Martin (1992). mod_negbin1 &lt;- stan_model(&quot;stan/negbin1.stan&quot;) fit_sanction_nb &lt;- sampling(mod_negbin1, data = sanction_data, control = list(adapt_delta = 0.95)) #&gt; #&gt; SAMPLING FOR MODEL &#39;negbin1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 7.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.76 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; [1] &quot;Error in sampler$call_sampler(args_list[[i]]) : &quot; #&gt; [2] &quot; Exception thrown at line 48: neg_binomial_2_rng: Random number that came from gamma distribution is 4.11693e+10, but must be less than 1.07374e+09&quot; #&gt; error occurred during calling the sampler; sampling not done summary(fit_sanction_nb, par = c(&quot;a&quot;, &quot;b&quot;, &quot;phi&quot;))$summary #&gt; Stan model &#39;negbin1&#39; does not contain samples. #&gt; NULL We can compare the loo_sanction_pois &lt;- loo(extract_log_lik(fit_sanction_pois, &quot;log_lik&quot;)) #&gt; Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k- #&gt; diagnostic&#39;) for details. # loo_sanction_nb &lt;- loo(extract_log_lik(fit_sanction_nb, &quot;log_lik&quot;)) # loo::compare(loo_sanction_pois, loo_sanction_nb) 10.4.2 References For general references on count models see Gelman and Hill (2007, 109–16) McElreath (2016 Ch 10) Fox (2016 Ch. 14) Gelman et al. (2013 Ch. 16) "],
["categorical-variables.html", "11 Categorical Variables 11.1 Example: Mexico Vote Choice", " 11 Categorical Variables 11.1 Example: Mexico Vote Choice This example from Zelig uses a multinomial model data(&quot;mexico&quot;, package = &quot;Zelig&quot;) "],
["ordered-categorical-outcomes.html", "12 Ordered Categorical Outcomes", " 12 Ordered Categorical Outcomes See rstanarm function rstasnarm Gelman and Hill (2007 Ch 6.5) *rstanarm** vignette Estimating Ordinal Regression Models with rstanarm "],
["shrinkage-regularization.html", "13 Shrinkage and Regularization 13.1 Normal Linear Regression Model 13.2 Penalized Regression 13.3 Bayesian Shrinkage Priors 13.4 Differences between Bayesian Shrinkage and Penalized Likelihood 13.5 Hierarchical Shrinkage Priors 13.6 Example 13.7 Shrinkage Parameters 13.8 Choice of Hyperparameter on \\(\\tau\\) 13.9 R Implementations 13.10 Bayesian Model Averaging 13.11 Slab and Spike Priors 13.12 Technical Notes 13.13 Multiple Comparisons and Thresholding rules 13.14 Examples of Applications of Sensitivity Analysis", " 13 Shrinkage and Regularization Shrinkage estimation deliberately introduces biases into the model to improve overall performance, often at the cost of individual estimates (Efron and Hastie 2016, 91). This is opposed to MLE, which produces unbiased estimates (asymptotically, given certain regularity conditions). Likewise, the Bayesian estimates with non- or weakly-informative priors will produce estimates similar to the MLE. With shrinkage, the priors are used to produce estimates different than the MLE case. Regularization describes any method that reduces variability in high dimensional estimation or prediction problems (Efron and Hastie 2016). 13.1 Normal Linear Regression Model Consider the single output linear Gaussian regression model with several input variables, given by \\[ \\begin{aligned}[t] y_i \\sim N(\\vec{x}_i&#39; \\vec{\\beta}, \\sigma^2) \\end{aligned} \\] where \\(\\vec{x}\\) is a \\(k\\)-vector of predictors, and \\(\\vec{\\beta}\\) are the coefficients. What priors do we put on \\(\\beta\\)? Improproper priors: \\(\\beta_k \\propto 1\\) This produces the equivalent of MLE estimates. Non-informative priors: These are priors which have such wide variance that they have little influence on the posterior, e.g. \\(\\beta_k \\sim N(0, 1e6)\\). The primary reason for these (as opposed to simply using an improper prior) is that some MCMC methods, e.g. Gibbs sampling as used in JAGS or BUGS, require proper prior distributions for all parameters. Shrinkage priors have a couple characteristics they push \\(\\beta_k \\to 0\\) while in the other cases, the scale of the prior on \\(\\beta\\) is fixed, in shrinkage priors there is often a hyperprior on it. E.g. \\(\\beta_k \\sim N(0, \\tau)\\), where \\(\\tau\\) is also a parameter to be estimated. 13.2 Penalized Regression Penalized regressions are regressions of the form: \\[ \\hat{\\beta}_{penalized} = \\argmin_{\\beta} \\sum_{i = 1}^n (\\vec{x}_i\\T \\vec{\\beta} - y_i)^2 + f(\\beta) \\] where \\(f\\) is some sort of penalty function on \\(\\beta\\) that penalizes larger (in magnitude) values of \\(\\beta\\). Two common forms Ridge: uses an \\(\\ell_2\\) penalty: \\(\\vec{beta}^2\\) Lasso: uses an \\(\\ell_1\\) penalty: \\(|\\vec{\\beta}|\\) 13.2.1 Ridge Regression Ridge regression uses the following penalty (Hoerl and Kennard 1970): \\[ \\hat{\\beta}_{\\text{ridge}} = \\argmin_{\\beta} \\sum_{i = 1}^n (\\vec{x}_i\\T \\vec{\\beta} - y_i)^2 + \\lambda \\sum_{k} \\beta_k^2 \\] This penalty produces smaller in magnitude coefficients, \\(|\\hat{\\beta}_{ridge}| &lt; |\\hat{\\beta}_{OLS}|\\). However, this “bias” in the coefficients can be offset by a lower variance, better MSE, and better out-of-sample performance than the OLS estimates. The point estimate for ridge regression coefficients is: \\[ \\hat{\\vec{\\beta}}_{\\text{ridge}} = {(\\mat{X}\\T \\mat{X} + \\lambda \\mat{I}_p)}^{-1} \\mat{X}\\T \\vec{y} \\] The variance-covariance matrix of the point estimate is, \\[ \\mathrm{df}(\\lambda) = \\tr(\\mat{X}(\\mat{X}\\T \\mat{X} + \\lambda \\mat{I}_p)^{-1} \\mat{X}\\T) = \\sum_{j = 1}^p \\frac{d_j^2}{d_j^2 + \\lambda} \\] where \\(d_j\\) are the singular values of \\(X\\) Some implications: \\(\\hat{\\vec{\\beta}}\\) exists even if \\(\\hat{\\vec{\\beta}}_{\\text{OLS}}\\) (\\((\\mat{X}\\T\\mat{X})^{-1}\\)), i.e. cases of \\(n &gt; p\\) and collinearity, does not exist. If \\(\\mat{X}\\) is orthogonal (mean 0, unit variance, zero correlation), \\(\\mat{X}\\T \\mat{X} = n \\mat{I}_p\\) then \\[ \\hat{\\vec{\\beta}}_{\\text{ridge}} = \\frac{n}{n + \\lambda} \\hat{\\vec{\\beta}}_{\\text{ols}} \\] meaning \\[ |\\hat{\\vec{\\beta}}_{\\text{ols}}| &gt; |\\hat{\\vec{\\beta}}_{\\text{ridge}}| \\geq 0 \\] Ridge does not produce sparse estimates, since \\((n / (n + \\lambda)) \\vec{\\vec{\\beta}}_{ols} = 0\\) iff \\(\\vec{\\vec{\\beta}}_{ols} = 0\\) \\(\\lambda = 0\\), then there is no shrinkage \\(\\lambda \\to \\infty\\), then there is complete shrinkage and all coefficients are tend to 0. 13.2.2 Lasso The Lasso or LASSO (least absolute shrinkage and selection operator) replaces squared the penalty on \\(\\beta\\) with an absolute value penalty (Tibshirani 1996): \\[ \\hat{\\beta}_{\\text{lasso}} = \\argmin_{\\beta} \\frac{1}{2 \\sigma} \\sum_{i = 1}^n (\\vec{x}_i\\T \\vec{\\beta} - y_i)^2 + \\lambda \\sum_{k} |\\beta_k| \\] The absolute value penalty will put some \\(\\hat{\\beta}_k = 0\\), producing a “sparse” solution. Properties: Unlike ridge regression, it sets some coefficients to exactly 0 If variables are perfectly correlated, there is no unique solution (unlike the ridge regression) Used as the best convex approximation of the “best subset selection” regression problem, which finds the number of nonzero entries in a vector. 13.3 Bayesian Shrinkage Priors \\[ \\log p(\\theta|y, x) \\propto \\frac{1}{2 \\sigma} \\sum_{i = 1}^n (\\vec{x}_i\\T \\vec{\\beta} - y_i)^2 + \\lambda \\sum_{k} \\beta_k^2 \\] In the first case, the log density of a normal distribution is, \\[ \\log p(y | \\mu, x) \\propto \\frac{1}{2 \\sigma} (x - \\mu)^2 \\] The first regression term is the produce of normal distributions (sum of their log probabilities), \\[ y_i \\sim N(\\vec{x}_i\\T \\vec{\\beta}, \\sigma) \\] The second term, \\(\\lambda \\sum_{k} \\beta_k^2\\) is also the sum of the log of densities of i.i.d. normal densities, with mean 0, and scale \\(\\tau = 1 / 2 \\lambda\\), \\[ \\beta_k \\sim N(0, \\tau^2) \\] The only difference in the LASSO is the penalty term, which uses an absolute value penalty for \\(\\beta_k\\). That term corresponds to a sum of log densities of i.i.d. double exponential (Laplace) distributions. The double exponential distribution density is similar to a normal distribution, \\[ \\log p(y | \\mu, \\sigma) \\propto - \\frac{|y - \\mu|}{\\sigma} \\] So the LASSO penalty is equivalent to the log density of a double exponential distribution with location \\(0\\), and scale \\(1 / \\lambda\\). \\[ \\beta_k \\sim \\dlaplace(0, \\tau) \\] 13.4 Differences between Bayesian Shrinkage and Penalized Likelihood There are several differences between Bayesian approaches to shrinkage and penalized ML approaches. The point estimates: ML: mode Bayesian: posterior mean (or median) In Lasso ML: the mode produces exact zeros and sparsity Bayesian: posterior mean is not sparse (zero) Choosing the shrinkage penalty: ML: cross-validation Bayesian: a prior is placed on the shrinkage penalty, and it is estimated as part of the posterior. 13.5 Hierarchical Shrinkage Priors \\[ \\begin{aligned} \\beta_k &amp;\\sim \\dnorm(0, \\lambda_i^2 \\tau^2) \\\\ \\lambda_i &amp;\\sim \\dt{\\nu}^{+}(0, 1) \\end{aligned} \\] If \\(\\nu = 1\\), then this is the Horseshoe prior (Carvalho, Polson, and Scott 2010, Carvalho, Polson, and Scott (2009), Pas, Kleijn, and Vaart (2014), Datta and Ghosh (2013), Polson and Scott (2011), Piironen and Vehtari (2016)) Hierarchical Shrinkage Plus (HS-\\(t_{\\nu}\\)+) \\[ \\begin{aligned} \\beta_k &amp;\\sim \\dnorm(0, \\lambda_i^2 \\eta_i^2 \\tau^2) \\\\ \\lambda_i &amp;\\sim \\dt{\\nu}^{+}(0, 1) \\\\ \\eta_i &amp;\\sim \\dt{\\nu}^{+}(0, 1) \\end{aligned} \\] This induces even more shrinkage towards zero than the If \\(\\nu = 1\\), then this is the Horseshoe+ prior as introduced by Bhadra et al. (2015). In linear regression \\[ \\begin{aligned}[t] p(\\beta | \\Lambda, \\tau, \\sigma^2, D) &amp;= N(\\beta, \\bar{\\beta}, \\Sigma) \\\\ \\bar{\\beta} &amp;= \\tau^2 \\Lambda (\\tau^2 \\Lambda + \\sigma^2 (X&#39;X)^{-1})^{-1} \\hat{\\beta} \\\\ \\Sigma &amp;= (\\tau^{-2} \\Lambda^{-1} + \\frac{1}{\\sigma^2} X&#39;X)^{-1} \\end{aligned} \\] where \\(\\Lambda = \\diag(\\lambda_1^2, \\dots, \\lambda_D^2)\\), and \\(\\hat{\\beta}\\) is the MLE estimate, \\((X&#39;X)^{-1} X&#39; y\\). If predictors are uncorrelated with mean zero and unit variance, then \\[ X&#39;X \\approx n I \\] and \\[ \\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j \\] where \\[ \\kappa_j = \\frac{1}{1 + n \\sigma^{-2} \\tau^2 \\lambda_j^2} \\] where \\(\\kappa_j\\) is the shrinkage factor for the coefficient \\(\\beta_j\\), which is how much it is shrunk towards zero from the MLE. \\(\\kappa_j = 1\\) is complete shrinkage, and \\(\\kappa_j = 0\\) is no shrinkage. So \\(\\bar{\\beta} \\to 0\\) as \\(\\tau \\to 0\\) and \\(\\bar{\\beta} \\to \\hat{\\beta}\\) as \\(\\tau \\to \\infty\\). Using a plug-in estimate of \\(\\tau\\) using cross-validation or the maximum marginal likelihood. The danger is that \\(\\hat{\\tau} = 0\\) if it is very sparse. van de Pas et al (2014) show that the optimal value (up to a log factor) in terms of MSE and posterior contraction rates compared to the true \\(\\beta^*\\) is \\[ \\tau^* = \\frac{p^*}{n} \\] where \\(p^*\\) is the number of non-zero coefficients in the true coefficient vector \\(\\beta^*\\). The effective number of nonzero coefficients is, \\[ m_{\\mathrm{eff}} = \\sum_{j = 1}^D (1 - \\kappa_j) \\] Some other notes To calculate the distribution of \\(\\kappa_j\\) given a distribution of \\(\\lambda\\). Note that \\[ \\kappa_j(\\lambda_j) = \\frac{1}{1 + n \\sigma^{-2} \\tau^2 \\lambda_j^2} \\] is monotonically decreasing in \\(\\lambda_j\\). It is also invertible, \\[ \\lambda_j(\\kappa_j) = \\sqrt{\\frac{1}{(1 + n \\sigma^{-2} \\tau^2) \\kappa_j}} \\] The derivative of this with respect to \\(\\kappa_j\\) is \\[ \\frac{\\partial \\lambda_j(\\kappa_j)}{\\partial \\kappa_j} = - \\sqrt{\\frac{1}{(1 + n \\sigma^{-2} \\tau^2)}} \\kappa_j^{-\\frac{3}{2}} \\] The distribution of \\(\\kappa\\), given the distribution \\(f_\\lambda\\) for lambda is, \\[ \\begin{aligned}[t] f_\\kappa(\\kappa_j) &amp;= f_\\lambda(\\lambda_j(\\kappa_j)) \\left| \\frac{\\partial \\lambda_j(\\kappa_j)}{\\partial \\kappa_j} \\right| \\\\ &amp;= f_\\lambda\\left(\\sqrt{\\frac{1}{(1 + n \\sigma^{-2} \\tau^2) \\kappa_j}}\\right) \\left| (1 + n \\sigma^{-2} \\tau^2)^{-\\frac{1}{2}} \\kappa_j^{-\\frac{3}{2}} \\right| \\\\ \\end{aligned} \\] Suppose that the distribution is given for precision, \\(\\lambda_j^{-2}\\). Then the inverse is, \\[ \\lambda_j^{-2}(\\kappa_j) = (1 + n \\sigma^{-2} \\tau^2) \\kappa_j \\] with derivative, \\[ \\frac{\\partial \\lambda_j^{-2}(\\kappa_j)}{\\partial \\kappa_j} = (1 + n \\sigma^{-2} \\tau^2) \\] Thus, \\[ \\begin{aligned}[t] f_\\kappa(\\kappa_j) &amp;= f_{\\lambda^{-2}}(\\lambda_j^{-2}(\\kappa_j)) \\left| \\frac{\\partial \\lambda_j^{-2}(\\kappa_j)}{\\partial \\kappa_j} \\right| \\\\ &amp;= f_{\\lambda^{-2}}\\left((1 + n \\sigma^{-2} \\tau^2) \\kappa_j \\right) \\left| (1 + n \\sigma^{-2} \\tau^2) \\right| \\\\ \\end{aligned} \\] Suppose that the distribution is given for variance \\(\\lambda_j^2\\). Then the inverse is, \\[ \\lambda_j^2(\\kappa_j) = \\frac{1}{(1 + n \\sigma^{-2} \\tau^2) \\kappa_j} \\] with derivative, \\[ \\frac{\\partial \\lambda_j^2(\\kappa_j)}{\\partial \\kappa_j} = -(1 + n \\sigma^{-2} \\tau^2)^{-1} \\kappa_j^{-2} \\] Thus, \\[ \\begin{aligned}[t] f_\\kappa(\\kappa_j) &amp;= f_{\\lambda^2}(\\lambda_j^2(\\kappa_j)) \\left| \\frac{\\partial \\lambda_j^2(\\kappa_j)}{\\partial \\kappa_j} \\right| \\\\ &amp;= f_{\\lambda^2}\\left(\\frac{1}{(1 + n \\sigma^{-2} \\tau^2) \\kappa_j}\\right) \\left| (1 + n \\sigma^{-2} \\tau^2)^{-1} \\kappa_j^{-2} \\right| \\\\ \\end{aligned} \\] I may also be useful to consider the distribution of \\(\\kappa\\) given the distribution of \\(\\tau\\). Note that \\[ \\kappa_j(\\tau) = \\frac{1}{1 + n \\sigma^{-2} \\tau^2 \\lambda_j^2} \\] is monotonically decreasing in \\(\\tau\\). It is also invertible, \\[ \\tau(\\kappa_j) = \\sqrt{\\frac{1}{(1 + n \\sigma^{-2} \\lambda_j^2) \\kappa_j}} \\] The derivative of this with respect to \\(\\kappa_j\\) is \\[ \\frac{\\partial \\tau(\\kappa_j)}{\\partial \\kappa_j} = - {(1 + n \\sigma^{-2} \\lambda_j^2)}^{-\\frac{1}{2}} \\kappa_j^{-\\frac{3}{2}} \\] The distribution of \\(\\kappa\\), given the distribution \\(f_\\lambda\\) for lambda is, \\[ \\begin{aligned}[t] f_\\kappa(\\kappa_j) &amp;= f_\\tau(\\tau(\\kappa_j)) \\left| \\frac{\\partial \\tau(\\kappa_j)}{\\partial \\kappa_j} \\right| \\\\ &amp;= f_\\tau\\left(\\frac{1}{(1 + n \\sigma^{-2} \\lambda_j^2) \\kappa_j} \\right) \\left| {(1 + n \\sigma^{-2} \\lambda_j^2)}^{-\\frac{1}{2}} \\kappa_j^{-\\frac{3}{2}} \\right| \\\\ \\end{aligned} \\] Allan Riddell. Epistemology of the corral: regression and variable selection with Stan and the Horseshoe prior March 10, 2014. 13.6 Example See the documentation. library(&quot;rstan&quot;) library(&quot;loo&quot;) library(&quot;glmnet&quot;) library(&quot;tidyverse&quot;) library(&quot;forcats&quot;) library(&quot;rubbish&quot;) URL &lt;- &quot;https://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data&quot; col_types &lt;- cols( X1 = col_integer(), lcavol = col_double(), lweight = col_double(), age = col_integer(), lbph = col_double(), svi = col_integer(), lcp = col_double(), gleason = col_integer(), pgg45 = col_integer(), lpsa = col_double(), train = col_logical() ) prostate &lt;- read_tsv(URL, col_types = col_types, skip = 1, col_names = names(col_types$cols)) Recall the prostate data example: we are interested in the level of prostate-specific antigen (PSA), elevated in men who have prostate cancer. The data prostate has data on on the level of prostate-specific antigen (PSA), which is elevated in men with prostate cancer, for 97 men with prostate cancer, and clinical predictors. f &lt;- lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45 - 1L prostate_data &lt;- lm_preprocess(f, data = prostate)[c(&quot;y&quot;, &quot;X&quot;)] %&gt;% within({ X &lt;- scale(X) K &lt;- ncol(X) N &lt;- nrow(X) }) run_with_tau &lt;- function(tau, mod, data, ...) { cat(&quot;Tau = &quot;, tau) data$tau &lt;- tau fit &lt;- sampling(mod, data = data, refresh = -1, verbose = FALSE, ...) out &lt;- list() out$summary &lt;- summary(fit, par = &quot;b&quot;)$summary %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;) ## calculate posterior modes out$summary$mode &lt;- apply(rstan::extract(fit, &quot;b&quot;)[[1]], 2, LaplacesDemon::Mode) out$summary$tau &lt;- tau out$loo &lt;- loo(extract_log_lik(fit)) out$lppd &lt;- mean(extract_log_lik(fit)) out$tau &lt;- tau out } mod_lm_coef_normal_1 &lt;- stan_model(&quot;stan/lm-coef-normal-1.stan&quot;) mod_lm_coef_normal_1 data { // number of observations int N; // response vector vector[N] y; // number of columns in the design matrix X int K; // design matrix X matrix [N, K] X; // real tau; } transformed data { real y_sd; real a_pr_scale; real sigma_pr_scale; real tau_pr_scale; y_sd = sd(y); sigma_pr_scale = y_sd * 5.; a_pr_scale = 10.; } parameters { // regression coefficient vector real a; vector[K] b; // scale of the regression errors real sigma; } transformed parameters { // mu is the observation fitted/predicted value // also called yhat vector[N] mu; mu = X * b; } model { // priors a ~ normal(0., a_pr_scale); b ~ normal(0., tau); sigma ~ cauchy(0., sigma_pr_scale); // likelihood y ~ normal(mu, sigma); } generated quantities { // simulate data from the posterior vector[N] y_rep; // log-likelihood posterior vector[N] log_lik; // mean log likelihood for (n in 1:N) { y_rep[n] = normal_rng(mu[n], sigma); log_lik[n] = normal_lpdf(y[n] | mu[n], sigma); } } tau_values &lt;- 2 ^ seq(2, -5, by = -.5) coefpath_normal &lt;- map(tau_values, run_with_tau, mod = mod_lm_coef_normal_1, data = prostate_data) #&gt; Tau = 4 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 5.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.56 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.17799 seconds (Warm-up) #&gt; 0.126119 seconds (Sampling) #&gt; 0.304109 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.153112 seconds (Warm-up) #&gt; 0.128688 seconds (Sampling) #&gt; 0.2818 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.172674 seconds (Warm-up) #&gt; 0.138926 seconds (Sampling) #&gt; 0.3116 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.168585 seconds (Warm-up) #&gt; 0.13534 seconds (Sampling) #&gt; 0.303925 seconds (Total) #&gt; #&gt; Tau = 2.83 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.168349 seconds (Warm-up) #&gt; 0.137366 seconds (Sampling) #&gt; 0.305715 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.166031 seconds (Warm-up) #&gt; 0.139874 seconds (Sampling) #&gt; 0.305905 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.162088 seconds (Warm-up) #&gt; 0.127644 seconds (Sampling) #&gt; 0.289732 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.177064 seconds (Warm-up) #&gt; 0.131347 seconds (Sampling) #&gt; 0.308411 seconds (Total) #&gt; Warning: Some Pareto k diagnostic values are slightly high. See #&gt; help(&#39;pareto-k-diagnostic&#39;) for details. #&gt; Tau = 2 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.157295 seconds (Warm-up) #&gt; 0.12598 seconds (Sampling) #&gt; 0.283275 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.165807 seconds (Warm-up) #&gt; 0.128247 seconds (Sampling) #&gt; 0.294054 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.17434 seconds (Warm-up) #&gt; 0.137339 seconds (Sampling) #&gt; 0.311679 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.155348 seconds (Warm-up) #&gt; 0.12996 seconds (Sampling) #&gt; 0.285308 seconds (Total) #&gt; #&gt; Tau = 1.41 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.151035 seconds (Warm-up) #&gt; 0.122661 seconds (Sampling) #&gt; 0.273696 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.168477 seconds (Warm-up) #&gt; 0.138765 seconds (Sampling) #&gt; 0.307242 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.166347 seconds (Warm-up) #&gt; 0.119069 seconds (Sampling) #&gt; 0.285416 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.165977 seconds (Warm-up) #&gt; 0.13783 seconds (Sampling) #&gt; 0.303807 seconds (Total) #&gt; #&gt; Tau = 1 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.155363 seconds (Warm-up) #&gt; 0.135929 seconds (Sampling) #&gt; 0.291292 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.156522 seconds (Warm-up) #&gt; 0.12641 seconds (Sampling) #&gt; 0.282932 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.152098 seconds (Warm-up) #&gt; 0.117134 seconds (Sampling) #&gt; 0.269232 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.169239 seconds (Warm-up) #&gt; 0.126917 seconds (Sampling) #&gt; 0.296156 seconds (Total) #&gt; #&gt; Tau = 0.707 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.144612 seconds (Warm-up) #&gt; 0.125574 seconds (Sampling) #&gt; 0.270186 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.141649 seconds (Warm-up) #&gt; 0.119192 seconds (Sampling) #&gt; 0.260841 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.152489 seconds (Warm-up) #&gt; 0.131134 seconds (Sampling) #&gt; 0.283623 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.161852 seconds (Warm-up) #&gt; 0.13558 seconds (Sampling) #&gt; 0.297432 seconds (Total) #&gt; #&gt; Tau = 0.5 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.132697 seconds (Warm-up) #&gt; 0.101804 seconds (Sampling) #&gt; 0.234501 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.141479 seconds (Warm-up) #&gt; 0.124046 seconds (Sampling) #&gt; 0.265525 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.145724 seconds (Warm-up) #&gt; 0.107125 seconds (Sampling) #&gt; 0.252849 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.138335 seconds (Warm-up) #&gt; 0.0871 seconds (Sampling) #&gt; 0.225435 seconds (Total) #&gt; #&gt; Tau = 0.354 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.137622 seconds (Warm-up) #&gt; 0.085612 seconds (Sampling) #&gt; 0.223234 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.132972 seconds (Warm-up) #&gt; 0.106316 seconds (Sampling) #&gt; 0.239288 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.124177 seconds (Warm-up) #&gt; 0.087221 seconds (Sampling) #&gt; 0.211398 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.141065 seconds (Warm-up) #&gt; 0.090947 seconds (Sampling) #&gt; 0.232012 seconds (Total) #&gt; #&gt; Tau = 0.25 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.123187 seconds (Warm-up) #&gt; 0.085269 seconds (Sampling) #&gt; 0.208456 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.116354 seconds (Warm-up) #&gt; 0.084139 seconds (Sampling) #&gt; 0.200493 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.123148 seconds (Warm-up) #&gt; 0.08502 seconds (Sampling) #&gt; 0.208168 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.129021 seconds (Warm-up) #&gt; 0.087296 seconds (Sampling) #&gt; 0.216317 seconds (Total) #&gt; #&gt; Tau = 0.177 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.146875 seconds (Warm-up) #&gt; 0.084014 seconds (Sampling) #&gt; 0.230889 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.114281 seconds (Warm-up) #&gt; 0.082986 seconds (Sampling) #&gt; 0.197267 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.124388 seconds (Warm-up) #&gt; 0.085893 seconds (Sampling) #&gt; 0.210281 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.127223 seconds (Warm-up) #&gt; 0.081284 seconds (Sampling) #&gt; 0.208507 seconds (Total) #&gt; #&gt; Tau = 0.125 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.130665 seconds (Warm-up) #&gt; 0.082034 seconds (Sampling) #&gt; 0.212699 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.122393 seconds (Warm-up) #&gt; 0.083755 seconds (Sampling) #&gt; 0.206148 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.117707 seconds (Warm-up) #&gt; 0.08409 seconds (Sampling) #&gt; 0.201797 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.11971 seconds (Warm-up) #&gt; 0.081726 seconds (Sampling) #&gt; 0.201436 seconds (Total) #&gt; #&gt; Tau = 0.0884 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.130868 seconds (Warm-up) #&gt; 0.083027 seconds (Sampling) #&gt; 0.213895 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.121938 seconds (Warm-up) #&gt; 0.082334 seconds (Sampling) #&gt; 0.204272 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.134972 seconds (Warm-up) #&gt; 0.085134 seconds (Sampling) #&gt; 0.220106 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.12292 seconds (Warm-up) #&gt; 0.079876 seconds (Sampling) #&gt; 0.202796 seconds (Total) #&gt; #&gt; Tau = 0.0625 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.141243 seconds (Warm-up) #&gt; 0.080644 seconds (Sampling) #&gt; 0.221887 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.147777 seconds (Warm-up) #&gt; 0.080076 seconds (Sampling) #&gt; 0.227853 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.14366 seconds (Warm-up) #&gt; 0.079912 seconds (Sampling) #&gt; 0.223572 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.12876 seconds (Warm-up) #&gt; 0.079623 seconds (Sampling) #&gt; 0.208383 seconds (Total) #&gt; #&gt; Tau = 0.0442 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.135554 seconds (Warm-up) #&gt; 0.080896 seconds (Sampling) #&gt; 0.21645 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.138415 seconds (Warm-up) #&gt; 0.082123 seconds (Sampling) #&gt; 0.220538 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.143854 seconds (Warm-up) #&gt; 0.079031 seconds (Sampling) #&gt; 0.222885 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.140652 seconds (Warm-up) #&gt; 0.222103 seconds (Sampling) #&gt; 0.362755 seconds (Total) #&gt; #&gt; Tau = 0.0312 #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.152801 seconds (Warm-up) #&gt; 0.082293 seconds (Sampling) #&gt; 0.235094 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.187366 seconds (Warm-up) #&gt; 0.082302 seconds (Sampling) #&gt; 0.269668 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.191033 seconds (Warm-up) #&gt; 0.085223 seconds (Sampling) #&gt; 0.276256 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-normal-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 3.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.188981 seconds (Warm-up) #&gt; 0.081884 seconds (Sampling) #&gt; 0.270865 seconds (Total) plot_coefpaths &lt;- function(coefpaths, stat = &quot;mean&quot;) { ggplot(map_df(coefpaths, &quot;summary&quot;), aes_string(x = &quot;log2(tau)&quot;, y = stat, colour = &quot;fct_reorder2(parameter, tau, mean)&quot;, fill = &quot;parameter&quot;)) + modelr::geom_ref_line(h = 0) + geom_line() + labs(colour = &quot;Parameter&quot;) } plot_coefpaths(coefpath_normal) plot_coefpath_loo &lt;- function(x) { map_df(x, function(x) { tibble(tau = x$tau, elpd = x$loo$elpd_loo, lppd = x$lppd, p = x$loo$p_loo) }) %&gt;% gather(parameter, value, -tau) %&gt;% ggplot(aes(x = tau, y = value)) + geom_point() + geom_line() + facet_wrap(~ parameter, scale = &quot;free_y&quot;, ncol = 1) } plot_coefpath_loo(coefpath_normal) Which is the “best” \\(tau\\)? get_best_tau &lt;- function(coefpath) { map_df(coefpath, function(x) { tibble(tau = x$tau, elpd = x$loo$elpd_loo, p = x$loo$p_loo) }) %&gt;% filter(elpd == max(elpd)) } get_best_tau(coefpath_normal) #&gt; # A tibble: 1 × 3 #&gt; tau elpd p #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.177 -234 2.3 The mean estimate of \\(\\tau\\) is higher than the best estimate, and there is some uncertainty over it. mod_lm_coef_normal_2 &lt;- stan_model(&quot;stan/lm-coef-normal-2.stan&quot;) #&gt; In file included from file278426df96ec.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:12: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/math/tools/config.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/config.hpp:39: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] #&gt; # define BOOST_NO_CXX11_RVALUE_REFERENCES #&gt; ^ #&gt; &lt;command line&gt;:6:9: note: previous definition is here #&gt; #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 #&gt; ^ #&gt; In file included from file278426df96ec.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:42: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] #&gt; static void set_zero_all_adjoints() { #&gt; ^ #&gt; In file included from file278426df96ec.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:43: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] #&gt; static void set_zero_all_adjoints_nested() { #&gt; ^ #&gt; In file included from file278426df96ec.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:59: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] #&gt; size_t fft_next_good_size(size_t N) { #&gt; ^ #&gt; In file included from file278426df96ec.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:298: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr.hpp:39: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint.hpp:61: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array.hpp:21: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/base.hpp:28: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; 8 warnings generated. fit_normal &lt;- sampling(mod_lm_coef_normal_2, data = prostate_data, refresh = -1, control = list(adapt_delta = 0.99)) #&gt; #&gt; Gradient evaluation took 3.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.377087 seconds (Warm-up) #&gt; 0.244246 seconds (Sampling) #&gt; 0.621333 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.395013 seconds (Warm-up) #&gt; 0.219145 seconds (Sampling) #&gt; 0.614158 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.290592 seconds (Warm-up) #&gt; 0.233561 seconds (Sampling) #&gt; 0.524153 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.411155 seconds (Warm-up) #&gt; 0.273809 seconds (Sampling) #&gt; 0.684964 seconds (Total) summary(fit_normal, &quot;tau&quot;)$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; tau 0.265 0.00519 0.139 0.0542 0.167 0.245 0.339 0.603 718 1 loo(extract_log_lik(fit_normal)) #&gt; Computed from 4000 by 97 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -234.6 3.0 #&gt; p_loo 3.8 0.4 #&gt; looic 469.2 6.0 #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5) #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. mcmc_dens(as.array(fit_normal), &quot;tau&quot;) mcmc_dens(as.array(fit_normal), regex_pars = &quot;^b&quot;) 13.6.1 Double Exponential (Laplace) Prior A second prior to consider for \\(\\vec\\beta\\) is the Double Exponential. mod_lasso_1 &lt;- stan_model(&quot;stan/lm-coef-lasso-1.stan&quot;) #&gt; In file included from file278426ec8e21.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:12: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/math/tools/config.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/config.hpp:39: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] #&gt; # define BOOST_NO_CXX11_RVALUE_REFERENCES #&gt; ^ #&gt; &lt;command line&gt;:6:9: note: previous definition is here #&gt; #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 #&gt; ^ #&gt; In file included from file278426ec8e21.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:42: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] #&gt; static void set_zero_all_adjoints() { #&gt; ^ #&gt; In file included from file278426ec8e21.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:43: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] #&gt; static void set_zero_all_adjoints_nested() { #&gt; ^ #&gt; In file included from file278426ec8e21.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:59: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] #&gt; size_t fft_next_good_size(size_t N) { #&gt; ^ #&gt; In file included from file278426ec8e21.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:298: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr.hpp:39: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint.hpp:61: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array.hpp:21: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/base.hpp:28: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; 8 warnings generated. coefpath_lasso &lt;- map(tau_values, run_with_tau, mod = mod_lasso_1, data = prostate_data) #&gt; Tau = 4 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.169471 seconds (Warm-up) #&gt; 0.136615 seconds (Sampling) #&gt; 0.306086 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.160739 seconds (Warm-up) #&gt; 0.132459 seconds (Sampling) #&gt; 0.293198 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.163054 seconds (Warm-up) #&gt; 0.119725 seconds (Sampling) #&gt; 0.282779 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 3.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.156085 seconds (Warm-up) #&gt; 0.13623 seconds (Sampling) #&gt; 0.292315 seconds (Total) #&gt; #&gt; Tau = 2.83 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.16491 seconds (Warm-up) #&gt; 0.131669 seconds (Sampling) #&gt; 0.296579 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.15949 seconds (Warm-up) #&gt; 0.122812 seconds (Sampling) #&gt; 0.282302 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 3.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.158244 seconds (Warm-up) #&gt; 0.128231 seconds (Sampling) #&gt; 0.286475 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.144683 seconds (Warm-up) #&gt; 0.127824 seconds (Sampling) #&gt; 0.272507 seconds (Total) #&gt; #&gt; Tau = 2 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.154439 seconds (Warm-up) #&gt; 0.123893 seconds (Sampling) #&gt; 0.278332 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.151097 seconds (Warm-up) #&gt; 0.132078 seconds (Sampling) #&gt; 0.283175 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.165071 seconds (Warm-up) #&gt; 0.128108 seconds (Sampling) #&gt; 0.293179 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.165049 seconds (Warm-up) #&gt; 0.11433 seconds (Sampling) #&gt; 0.279379 seconds (Total) #&gt; #&gt; Tau = 1.41 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.151507 seconds (Warm-up) #&gt; 0.128273 seconds (Sampling) #&gt; 0.27978 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.153933 seconds (Warm-up) #&gt; 0.114526 seconds (Sampling) #&gt; 0.268459 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.148666 seconds (Warm-up) #&gt; 0.13089 seconds (Sampling) #&gt; 0.279556 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.157489 seconds (Warm-up) #&gt; 0.133079 seconds (Sampling) #&gt; 0.290568 seconds (Total) #&gt; #&gt; Tau = 1 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.157807 seconds (Warm-up) #&gt; 0.119827 seconds (Sampling) #&gt; 0.277634 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.152256 seconds (Warm-up) #&gt; 0.129752 seconds (Sampling) #&gt; 0.282008 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.152488 seconds (Warm-up) #&gt; 0.115369 seconds (Sampling) #&gt; 0.267857 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.138709 seconds (Warm-up) #&gt; 0.119415 seconds (Sampling) #&gt; 0.258124 seconds (Total) #&gt; #&gt; Tau = 0.707 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.148239 seconds (Warm-up) #&gt; 0.130681 seconds (Sampling) #&gt; 0.27892 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.159341 seconds (Warm-up) #&gt; 0.13126 seconds (Sampling) #&gt; 0.290601 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.150785 seconds (Warm-up) #&gt; 0.132919 seconds (Sampling) #&gt; 0.283704 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.140309 seconds (Warm-up) #&gt; 0.1175 seconds (Sampling) #&gt; 0.257809 seconds (Total) #&gt; #&gt; Tau = 0.5 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.168461 seconds (Warm-up) #&gt; 0.137516 seconds (Sampling) #&gt; 0.305977 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.178259 seconds (Warm-up) #&gt; 0.13673 seconds (Sampling) #&gt; 0.314989 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.163207 seconds (Warm-up) #&gt; 0.12666 seconds (Sampling) #&gt; 0.289867 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.148402 seconds (Warm-up) #&gt; 0.118888 seconds (Sampling) #&gt; 0.26729 seconds (Total) #&gt; #&gt; Tau = 0.354 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.162557 seconds (Warm-up) #&gt; 0.138546 seconds (Sampling) #&gt; 0.301103 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.167166 seconds (Warm-up) #&gt; 0.097878 seconds (Sampling) #&gt; 0.265044 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.155406 seconds (Warm-up) #&gt; 0.10972 seconds (Sampling) #&gt; 0.265126 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.156223 seconds (Warm-up) #&gt; 0.136394 seconds (Sampling) #&gt; 0.292617 seconds (Total) #&gt; #&gt; Tau = 0.25 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.176802 seconds (Warm-up) #&gt; 0.129965 seconds (Sampling) #&gt; 0.306767 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.183236 seconds (Warm-up) #&gt; 0.141074 seconds (Sampling) #&gt; 0.32431 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.163929 seconds (Warm-up) #&gt; 0.137625 seconds (Sampling) #&gt; 0.301554 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.208815 seconds (Warm-up) #&gt; 0.138363 seconds (Sampling) #&gt; 0.347178 seconds (Total) #&gt; #&gt; Tau = 0.177 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.202964 seconds (Warm-up) #&gt; 0.144704 seconds (Sampling) #&gt; 0.347668 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.183398 seconds (Warm-up) #&gt; 0.133954 seconds (Sampling) #&gt; 0.317352 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.207131 seconds (Warm-up) #&gt; 0.144178 seconds (Sampling) #&gt; 0.351309 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.189636 seconds (Warm-up) #&gt; 0.147323 seconds (Sampling) #&gt; 0.336959 seconds (Total) #&gt; #&gt; Tau = 0.125 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.216855 seconds (Warm-up) #&gt; 0.152283 seconds (Sampling) #&gt; 0.369138 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.240154 seconds (Warm-up) #&gt; 0.153379 seconds (Sampling) #&gt; 0.393533 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.241622 seconds (Warm-up) #&gt; 0.163411 seconds (Sampling) #&gt; 0.405033 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.276523 seconds (Warm-up) #&gt; 0.172288 seconds (Sampling) #&gt; 0.448811 seconds (Total) #&gt; #&gt; Tau = 0.0884 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.26097 seconds (Warm-up) #&gt; 0.168084 seconds (Sampling) #&gt; 0.429054 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.24219 seconds (Warm-up) #&gt; 0.170399 seconds (Sampling) #&gt; 0.412589 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.236765 seconds (Warm-up) #&gt; 0.161416 seconds (Sampling) #&gt; 0.398181 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.27547 seconds (Warm-up) #&gt; 0.160869 seconds (Sampling) #&gt; 0.436339 seconds (Total) #&gt; #&gt; Tau = 0.0625 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.296557 seconds (Warm-up) #&gt; 0.23422 seconds (Sampling) #&gt; 0.530777 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.259157 seconds (Warm-up) #&gt; 0.201577 seconds (Sampling) #&gt; 0.460734 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.388088 seconds (Warm-up) #&gt; 0.18296 seconds (Sampling) #&gt; 0.571048 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.282283 seconds (Warm-up) #&gt; 0.212218 seconds (Sampling) #&gt; 0.494501 seconds (Total) #&gt; #&gt; Tau = 0.0442 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.283107 seconds (Warm-up) #&gt; 0.203368 seconds (Sampling) #&gt; 0.486475 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.319521 seconds (Warm-up) #&gt; 0.175834 seconds (Sampling) #&gt; 0.495355 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.314637 seconds (Warm-up) #&gt; 0.222073 seconds (Sampling) #&gt; 0.53671 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.309884 seconds (Warm-up) #&gt; 0.174977 seconds (Sampling) #&gt; 0.484861 seconds (Total) #&gt; #&gt; Tau = 0.0312 #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.3e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.336272 seconds (Warm-up) #&gt; 0.205675 seconds (Sampling) #&gt; 0.541947 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.314891 seconds (Warm-up) #&gt; 0.192009 seconds (Sampling) #&gt; 0.5069 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.354932 seconds (Warm-up) #&gt; 0.219093 seconds (Sampling) #&gt; 0.574025 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-lasso-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 4.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.281833 seconds (Warm-up) #&gt; 0.195115 seconds (Sampling) #&gt; 0.476948 seconds (Total) plot_coefpaths(coefpath_lasso) plot_coefpaths(coefpath_lasso, &quot;mode&quot;) plot_coefpath_pars &lt;- function(coefpath) { ggplot(map_df(coefpath, &quot;summary&quot;), aes(x = log10(tau), y = mean)) + facet_wrap(~ parameter) + modelr::geom_ref_line(h = 0) + geom_ribbon(aes(ymin = `25%`, ymax = `75%`), alpha = 0.2) + geom_line() } plot_coefpath_pars(coefpath_lasso) Which is the “best” \\(tau\\)? get_best_tau(coefpath_lasso) #&gt; # A tibble: 1 × 3 #&gt; tau elpd p #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.125 -234 2.47 mod_lasso_2 &lt;- stan_model(&quot;stan/lm-coef-lasso-2.stan&quot;) #&gt; In file included from file27846d0ff3da.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:12: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/math/tools/config.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/config.hpp:39: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] #&gt; # define BOOST_NO_CXX11_RVALUE_REFERENCES #&gt; ^ #&gt; &lt;command line&gt;:6:9: note: previous definition is here #&gt; #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 #&gt; ^ #&gt; In file included from file27846d0ff3da.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:42: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] #&gt; static void set_zero_all_adjoints() { #&gt; ^ #&gt; In file included from file27846d0ff3da.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:43: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] #&gt; static void set_zero_all_adjoints_nested() { #&gt; ^ #&gt; In file included from file27846d0ff3da.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:59: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] #&gt; size_t fft_next_good_size(size_t N) { #&gt; ^ #&gt; In file included from file27846d0ff3da.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:298: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr.hpp:39: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint.hpp:61: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array.hpp:21: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/base.hpp:28: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; 8 warnings generated. fit_lasso &lt;- sampling(mod_lasso_2, data = prostate_data, refresh = -1, control = list(adapt_delta = 0.9)) #&gt; #&gt; Gradient evaluation took 3.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.520379 seconds (Warm-up) #&gt; 0.239833 seconds (Sampling) #&gt; 0.760212 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.578325 seconds (Warm-up) #&gt; 0.270174 seconds (Sampling) #&gt; 0.848499 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.413246 seconds (Warm-up) #&gt; 0.223016 seconds (Sampling) #&gt; 0.636262 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.470348 seconds (Warm-up) #&gt; 0.246839 seconds (Sampling) #&gt; 0.717187 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 4 #&gt; count #&gt; Exception thrown at line 38: double_exponential_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; Warning: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See #&gt; http://mc-stan.org/misc/warnings.html#bfmi-low #&gt; Warning: Examine the pairs() plot to diagnose sampling problems summary(fit_lasso, &quot;tau&quot;)$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; tau 0.216 0.00859 0.146 0.0173 0.12 0.193 0.282 0.576 288 1.01 loo(extract_log_lik(fit_lasso)) #&gt; Computed from 4000 by 97 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -234.6 3.0 #&gt; p_loo 3.7 0.4 #&gt; looic 469.2 6.0 #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5) #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. mcmc_dens(as.array(fit_lasso), &quot;tau&quot;) mcmc_dens(as.array(fit_lasso), regex_pars = &quot;^b&quot;) 13.6.2 Hierarchical Prior (HS) The Hierarchical or Horseshoe Prior is defined as as a scale mixture of normal distributions, \\[ \\begin{aligned}[t] \\lambda_i &amp;\\sim \\dt{\\nu}(0, 1) \\\\ \\end{aligned} \\] In the original formulation (Carvalho, Polson, and Scott 2009,Carvalho, Polson, and Scott (2010)) use a half-Cauchy (\\(\\nu = 1\\)), but Stan suggests and rstanarm uses a Student-t with \\(\\nu = 3\\), finding that it has better sampling performance than the half-Cauchy. mod_lm_coef_hs_1 &lt;- stan_model(&quot;stan/lm-coef-hs-1.stan&quot;) #&gt; In file included from file278478b1328d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:12: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/math/tools/config.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/config.hpp:39: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] #&gt; # define BOOST_NO_CXX11_RVALUE_REFERENCES #&gt; ^ #&gt; &lt;command line&gt;:6:9: note: previous definition is here #&gt; #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 #&gt; ^ #&gt; In file included from file278478b1328d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:42: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] #&gt; static void set_zero_all_adjoints() { #&gt; ^ #&gt; In file included from file278478b1328d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:43: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] #&gt; static void set_zero_all_adjoints_nested() { #&gt; ^ #&gt; In file included from file278478b1328d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:59: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] #&gt; size_t fft_next_good_size(size_t N) { #&gt; ^ #&gt; In file included from file278478b1328d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:298: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr.hpp:39: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint.hpp:61: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array.hpp:21: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/base.hpp:28: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; 8 warnings generated. coefpath_hs &lt;- map(tau_values, run_with_tau, mod = mod_lm_coef_hs_1, data = c(prostate_data, list(df_local = 3)), control = list(adapt_delta = 0.999, max_treedepth = 12)) #&gt; Tau = 4 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.87611 seconds (Warm-up) #&gt; 6.20805 seconds (Sampling) #&gt; 9.08416 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 3.52193 seconds (Warm-up) #&gt; 1.54883 seconds (Sampling) #&gt; 5.07076 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.86541 seconds (Warm-up) #&gt; 2.61092 seconds (Sampling) #&gt; 5.47633 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.47924 seconds (Warm-up) #&gt; 3.56892 seconds (Sampling) #&gt; 6.04816 seconds (Total) #&gt; Warning: There were 2 divergent transitions after warmup. Increasing adapt_delta above 0.999 may help. See #&gt; http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup #&gt; Warning: Examine the pairs() plot to diagnose sampling problems #&gt; Tau = 2.83 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.55465 seconds (Warm-up) #&gt; 5.27214 seconds (Sampling) #&gt; 7.82679 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.32708 seconds (Warm-up) #&gt; 1.8418 seconds (Sampling) #&gt; 4.16887 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.59388 seconds (Warm-up) #&gt; 2.63362 seconds (Sampling) #&gt; 5.2275 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.16754 seconds (Warm-up) #&gt; 3.22175 seconds (Sampling) #&gt; 5.38929 seconds (Total) #&gt; #&gt; Tau = 2 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.41352 seconds (Warm-up) #&gt; 1.06254 seconds (Sampling) #&gt; 3.47605 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.79026 seconds (Warm-up) #&gt; 1.07781 seconds (Sampling) #&gt; 2.86808 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.83395 seconds (Warm-up) #&gt; 2.11641 seconds (Sampling) #&gt; 3.95036 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.791 seconds (Warm-up) #&gt; 1.14265 seconds (Sampling) #&gt; 2.93365 seconds (Total) #&gt; Warning: There were 2 divergent transitions after warmup. Increasing adapt_delta above 0.999 may help. See #&gt; http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup #&gt; Warning: Examine the pairs() plot to diagnose sampling problems #&gt; Tau = 1.41 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 4.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.25397 seconds (Warm-up) #&gt; 1.20778 seconds (Sampling) #&gt; 2.46175 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.78616 seconds (Warm-up) #&gt; 1.15518 seconds (Sampling) #&gt; 2.94134 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.0846 seconds (Warm-up) #&gt; 0.736958 seconds (Sampling) #&gt; 2.82156 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 2.22619 seconds (Warm-up) #&gt; 0.638367 seconds (Sampling) #&gt; 2.86456 seconds (Total) #&gt; Warning: There were 3 divergent transitions after warmup. Increasing adapt_delta above 0.999 may help. See #&gt; http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup #&gt; Warning: Examine the pairs() plot to diagnose sampling problems #&gt; Tau = 1 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.15944 seconds (Warm-up) #&gt; 1.33754 seconds (Sampling) #&gt; 2.49697 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.18318 seconds (Warm-up) #&gt; 1.75236 seconds (Sampling) #&gt; 2.93555 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.11993 seconds (Warm-up) #&gt; 0.755497 seconds (Sampling) #&gt; 1.87542 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.63862 seconds (Warm-up) #&gt; 1.18794 seconds (Sampling) #&gt; 2.82656 seconds (Total) #&gt; #&gt; Tau = 0.707 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.13553 seconds (Warm-up) #&gt; 1.44398 seconds (Sampling) #&gt; 2.57951 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.22404 seconds (Warm-up) #&gt; 1.56945 seconds (Sampling) #&gt; 2.79349 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.908564 seconds (Warm-up) #&gt; 0.78698 seconds (Sampling) #&gt; 1.69554 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.45544 seconds (Warm-up) #&gt; 0.707461 seconds (Sampling) #&gt; 2.1629 seconds (Total) #&gt; #&gt; Tau = 0.5 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.873706 seconds (Warm-up) #&gt; 0.612853 seconds (Sampling) #&gt; 1.48656 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.821891 seconds (Warm-up) #&gt; 0.668755 seconds (Sampling) #&gt; 1.49065 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.4445 seconds (Warm-up) #&gt; 0.50983 seconds (Sampling) #&gt; 1.95433 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.882806 seconds (Warm-up) #&gt; 0.749524 seconds (Sampling) #&gt; 1.63233 seconds (Total) #&gt; #&gt; Tau = 0.354 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.904845 seconds (Warm-up) #&gt; 1.21261 seconds (Sampling) #&gt; 2.11746 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.967748 seconds (Warm-up) #&gt; 1.14564 seconds (Sampling) #&gt; 2.11338 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.824583 seconds (Warm-up) #&gt; 0.614315 seconds (Sampling) #&gt; 1.4389 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.751391 seconds (Warm-up) #&gt; 0.580313 seconds (Sampling) #&gt; 1.3317 seconds (Total) #&gt; #&gt; Tau = 0.25 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.958892 seconds (Warm-up) #&gt; 0.630532 seconds (Sampling) #&gt; 1.58942 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.819902 seconds (Warm-up) #&gt; 0.632382 seconds (Sampling) #&gt; 1.45228 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.707015 seconds (Warm-up) #&gt; 0.607349 seconds (Sampling) #&gt; 1.31436 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.792171 seconds (Warm-up) #&gt; 0.603033 seconds (Sampling) #&gt; 1.3952 seconds (Total) #&gt; #&gt; Tau = 0.177 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.37449 seconds (Warm-up) #&gt; 0.360441 seconds (Sampling) #&gt; 1.73493 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.710186 seconds (Warm-up) #&gt; 0.748483 seconds (Sampling) #&gt; 1.45867 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.33735 seconds (Warm-up) #&gt; 1.07732 seconds (Sampling) #&gt; 2.41467 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.661162 seconds (Warm-up) #&gt; 0.730974 seconds (Sampling) #&gt; 1.39214 seconds (Total) #&gt; #&gt; Tau = 0.125 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.25579 seconds (Warm-up) #&gt; 0.4392 seconds (Sampling) #&gt; 1.69499 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.78691 seconds (Warm-up) #&gt; 0.378782 seconds (Sampling) #&gt; 1.16569 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.868063 seconds (Warm-up) #&gt; 0.598435 seconds (Sampling) #&gt; 1.4665 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.95026 seconds (Warm-up) #&gt; 0.914547 seconds (Sampling) #&gt; 1.86481 seconds (Total) #&gt; #&gt; Tau = 0.0884 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 4.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.898391 seconds (Warm-up) #&gt; 0.627058 seconds (Sampling) #&gt; 1.52545 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.590551 seconds (Warm-up) #&gt; 0.639761 seconds (Sampling) #&gt; 1.23031 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.632818 seconds (Warm-up) #&gt; 0.425219 seconds (Sampling) #&gt; 1.05804 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.72216 seconds (Warm-up) #&gt; 0.629402 seconds (Sampling) #&gt; 1.35156 seconds (Total) #&gt; #&gt; Tau = 0.0625 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.586961 seconds (Warm-up) #&gt; 1.1543 seconds (Sampling) #&gt; 1.74126 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.61856 seconds (Warm-up) #&gt; 0.32033 seconds (Sampling) #&gt; 0.93889 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 2.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.607866 seconds (Warm-up) #&gt; 0.625017 seconds (Sampling) #&gt; 1.23288 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.515886 seconds (Warm-up) #&gt; 0.443247 seconds (Sampling) #&gt; 0.959133 seconds (Total) #&gt; #&gt; Tau = 0.0442 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 1.00163 seconds (Warm-up) #&gt; 0.45284 seconds (Sampling) #&gt; 1.45447 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.489896 seconds (Warm-up) #&gt; 0.544579 seconds (Sampling) #&gt; 1.03448 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.54948 seconds (Warm-up) #&gt; 0.589965 seconds (Sampling) #&gt; 1.13944 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.562241 seconds (Warm-up) #&gt; 0.527688 seconds (Sampling) #&gt; 1.08993 seconds (Total) #&gt; #&gt; Tau = 0.0312 #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.897725 seconds (Warm-up) #&gt; 0.414058 seconds (Sampling) #&gt; 1.31178 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.697267 seconds (Warm-up) #&gt; 0.458762 seconds (Sampling) #&gt; 1.15603 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.605598 seconds (Warm-up) #&gt; 0.328416 seconds (Sampling) #&gt; 0.934014 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm-coef-hs-1&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.576019 seconds (Warm-up) #&gt; 0.642967 seconds (Sampling) #&gt; 1.21899 seconds (Total) #&gt; Warning: Some Pareto k diagnostic values are slightly high. See #&gt; help(&#39;pareto-k-diagnostic&#39;) for details. plot_coefpaths(coefpath_hs) plot_coefpaths(coefpath_hs, &quot;mode&quot;) get_best_tau(coefpath_hs) #&gt; # A tibble: 1 × 3 #&gt; tau elpd p #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.125 -234 2.51 plot_coefpath_loo(coefpath_hs) mod_lm_coef_hs_2 &lt;- stan_model(&quot;stan/lm-coef-hs-2.stan&quot;) #&gt; In file included from file27845219544d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:12: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/math/tools/config.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/config.hpp:39: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] #&gt; # define BOOST_NO_CXX11_RVALUE_REFERENCES #&gt; ^ #&gt; &lt;command line&gt;:6:9: note: previous definition is here #&gt; #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 #&gt; ^ #&gt; In file included from file27845219544d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:42: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] #&gt; static void set_zero_all_adjoints() { #&gt; ^ #&gt; In file included from file27845219544d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:43: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] #&gt; static void set_zero_all_adjoints_nested() { #&gt; ^ #&gt; In file included from file27845219544d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:59: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] #&gt; size_t fft_next_good_size(size_t N) { #&gt; ^ #&gt; In file included from file27845219544d.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:298: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr.hpp:39: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint.hpp:61: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array.hpp:21: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/base.hpp:28: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; 8 warnings generated. fit_hs &lt;- sampling(mod_lm_coef_hs_2, refresh = -1, data = c(prostate_data, list(df_local = 3, df_global = 3)), control = list(adapt_delta = 0.995)) #&gt; #&gt; Gradient evaluation took 4.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 1.00399 seconds (Warm-up) #&gt; 0.582382 seconds (Sampling) #&gt; 1.58637 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 1 #&gt; count #&gt; Exception thrown at line 39: multiply: B[1] is nan, but must not be nan! 2 #&gt; Exception thrown at line 39: multiply: B[8] is nan, but must not be nan! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; Gradient evaluation took 2e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.988918 seconds (Warm-up) #&gt; 0.412526 seconds (Sampling) #&gt; 1.40144 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 2 #&gt; count #&gt; Exception thrown at line 39: multiply: B[1] is nan, but must not be nan! 5 #&gt; Exception thrown at line 39: multiply: B[5] is nan, but must not be nan! 1 #&gt; Exception thrown at line 39: multiply: B[7] is nan, but must not be nan! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 1.15877 seconds (Warm-up) #&gt; 1.16426 seconds (Sampling) #&gt; 2.32303 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.90871 seconds (Warm-up) #&gt; 0.473369 seconds (Sampling) #&gt; 1.38208 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 4 #&gt; count #&gt; Exception thrown at line 39: multiply: B[1] is nan, but must not be nan! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. summary(fit_hs, &quot;tau&quot;)$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; tau 0.298 0.00459 0.223 0.0361 0.147 0.24 0.382 0.885 2359 1 loo(extract_log_lik(fit_hs)) #&gt; Computed from 4000 by 97 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -234.5 3.0 #&gt; p_loo 3.7 0.4 #&gt; looic 469.0 5.9 #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5) #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. mcmc_dens(as.array(fit_hs), &quot;tau&quot;) mcmc_dens(as.array(fit_hs), regex_pars = &quot;^b\\\\[\\\\d+\\\\]$&quot;) mod_lm_coef_hs_3 &lt;- stan_model(&quot;stan/lm-coef-hs-3.stan&quot;) #&gt; In file included from file278473f93038.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:12: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/math/tools/config.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/config.hpp:39: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] #&gt; # define BOOST_NO_CXX11_RVALUE_REFERENCES #&gt; ^ #&gt; &lt;command line&gt;:6:9: note: previous definition is here #&gt; #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 #&gt; ^ #&gt; In file included from file278473f93038.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:42: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] #&gt; static void set_zero_all_adjoints() { #&gt; ^ #&gt; In file included from file278473f93038.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core.hpp:43: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] #&gt; static void set_zero_all_adjoints_nested() { #&gt; ^ #&gt; In file included from file278473f93038.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:59: #&gt; /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] #&gt; size_t fft_next_good_size(size_t N) { #&gt; ^ #&gt; In file included from file278473f93038.cpp:8: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/src/stan/model/model_header.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math.hpp:4: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/rev/mat.hpp:11: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/mat.hpp:298: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr.hpp:39: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint.hpp:61: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array.hpp:21: #&gt; In file included from /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/base.hpp:28: #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index_range index_range; #&gt; ^ #&gt; /Users/jrnold/Library/R/3.4/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] #&gt; typedef typename Array::index index; #&gt; ^ #&gt; 8 warnings generated. fit_hs3 &lt;- sampling(mod_lm_coef_hs_3, refresh = -1, data = c(prostate_data, list(df_local = 3, df_global = 3, p0 = 2)), control = list(adapt_delta = 0.995)) #&gt; #&gt; Gradient evaluation took 3.8e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 7.08115 seconds (Warm-up) #&gt; 9.70287 seconds (Sampling) #&gt; 16.784 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 1 #&gt; count #&gt; Exception thrown at line 51: student_t_lpdf: Scale parameter is inf, but must be finite! 4 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 5.45486 seconds (Warm-up) #&gt; 9.76004 seconds (Sampling) #&gt; 15.2149 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.9e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 9.2612 seconds (Warm-up) #&gt; 9.87074 seconds (Sampling) #&gt; 19.1319 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 3 #&gt; count #&gt; Exception thrown at line 44: multiply: B[1] is nan, but must not be nan! 1 #&gt; Exception thrown at line 51: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; Gradient evaluation took 2.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 2.91756 seconds (Warm-up) #&gt; 5.69903 seconds (Sampling) #&gt; 8.61659 seconds (Total) #&gt; Warning: There were 531 divergent transitions after warmup. Increasing adapt_delta above 0.995 may help. See #&gt; http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup #&gt; Warning: There were 3342 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See #&gt; http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded #&gt; Warning: Examine the pairs() plot to diagnose sampling problems 13.6.3 Comparison Let’s compare the various coefficient paths: all_coefpaths &lt;- bind_rows(mutate(map_df(coefpath_normal, &quot;summary&quot;), model = &quot;normal&quot;), mutate(map_df(coefpath_lasso, &quot;summary&quot;), model = &quot;lasso&quot;), mutate(map_df(coefpath_hs, &quot;summary&quot;), model = &quot;hs&quot;)) ggplot(all_coefpaths, aes(x = log2(tau), y = mean, colour = model)) + modelr::geom_ref_line(h = 0) + geom_line() + facet_wrap(~ parameter) 13.7 Shrinkage Parameters Given the linear Gaussian regression model \\[ y_i \\sim \\dnorm(\\vec{\\beta}\\T \\vec{x}, \\sigma^2) \\] for \\(i = 1, \\dots, n\\), where \\(\\vec{x}\\) is the \\(K\\) dimensional vector of predictors. Suppose a prior \\[ \\begin{aligned}[t] \\beta_j | \\lambda_j, \\tau &amp;\\sim N(0, \\lambda_j^2 \\tau^2) \\end{aligned} \\] The \\(\\lambda_j\\) are local scales - it allows some weights to escape the shrinkage. The global parameter \\(\\tau\\) pulls all weights towards zero, and effectively controls the sparsity. The posterior distribution is \\[ \\begin{aligned}[t] p(\\vec{\\beta} | \\mat{\\Lambda}, \\tau, \\sigma^2, \\mat{X}, \\vec{y}) &amp;= \\dnorm(\\vec{\\beta}, \\bar{\\vec{\\beta}}, \\mat{\\Sigma}) \\\\ \\bar{\\vec{\\beta}} &amp;= \\tau^2 \\mat{\\Lambda}(\\tau^2 \\mat{\\Lambda} + \\sigma^2 (\\mat{X}\\T \\mat{X})^{-1})^{-1} \\hat{\\vec{\\beta}} \\\\ \\mat{\\Sigma} &amp;= (\\tau^{-2} \\mat{\\Lambda}^{-1} + \\frac{1}{\\sigma^2} \\mat{X}\\T \\mat{X})^{-1}, \\end{aligned} \\] where \\[ \\mat{\\Lambda} = \\diag(\\lambda_1^2, \\dots, \\lambda_K^2) \\] and \\[ \\hat{\\vec{\\beta}} = (\\mat{X}\\T \\mat{X})^{-1} \\mat{X}\\T \\vec{y} \\] is the MLE solution if \\((\\mat{X}\\T \\mat{X})^{-1}\\) exists. It the predictors are uncorrelated with zero mean and unit variance, then \\(\\mat{X}\\T \\mat{X} \\approx n \\mat{I}\\), and approximate \\[ \\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j \\] where \\(\\kappa_j\\) is the shrinkage factor for coefficient \\(j\\), \\[ \\kappa_j = \\frac{1}{1 + n \\sigma^{-2} \\tau^2 \\lambda^2_j} \\] When \\(\\kappa = 1\\), it is complete shrinkage, and the coefficient is zero. When \\(\\kappa = 0\\), then there is no shrinkage, and the coefficient is equal to the MLE solution. As \\(\\tau \\to 0\\), then \\(\\bar{\\beta} \\to 0\\), and as \\(\\tau \\to \\infty\\), then \\(\\bar{\\beta} \\to \\hat{\\beta}\\). Note that for these distributions: Normal: prior puts weight only on a single point HS for df = 0: prior on shrinkage parameter puts weight on either completely shrunk (\\(\\kappa = 1\\)) or unshrunk (\\(\\kappa = 0\\)) HS for df = 3: prior on shrinkage parameter puts a lo of weight on it being completely shrunk (\\(\\kappa = 1\\)), but truncates the density for completely unshrunk. 13.8 Choice of Hyperparameter on \\(\\tau\\) The value of \\(\\tau\\) and the choice of its hyper-parameter has a big influence on the sparsity of the coefficients. Carvalho, Polson, and Scott (2009) suggest \\[ \\tau \\sim \\dhalfcauchy(0, \\sigma), \\] while Polson and Scott (2011) suggest, \\[ \\tau \\sim \\dhalfcauchy(0, 1) . \\] Pas, Kleijn, and Vaart (2014) suggest \\[ \\tau \\sim \\dhalfcauchy(0, p^* / n) \\] where \\(p^*\\) is the true number of non-zero parameters, and \\(n\\) is the number of observations. They suggest \\(\\tau = p^* / n\\) or \\(\\tau p^* / n \\sqrt{log(n / p^*)}\\). Additionally, they suggest restricting \\(\\tau\\) to \\([0, 1]\\). Piironen and Vehtari (2016) understand the choice of the prior on \\(\\tau\\) as the implied prior on the number of effective parameters. The shrinkage can be understood as its influence on the number of effective parameters, \\(m_{eff}\\), \\[ m_{eff} = \\sum_{j = 1}^K (1 - \\kappa_j) . \\] This is a measure of effective model size. The mean and variance of \\(m_{eff}\\) given \\(\\tau\\) and \\(\\sigma\\) are, \\[ \\begin{aligned}[t] \\E[m_{eff} | \\tau, \\sigma] &amp;= \\frac{\\sigma^{-1} \\tau \\sqrt{n}}{1 + \\sigma^{-1} \\tau \\sqrt{n}} K , \\\\ \\Var[m_{eff} | \\tau, \\sigma] &amp;= \\frac{\\sigma^{-1} \\tau \\sqrt{n}}{2 (1 + \\sigma^{-1} \\tau \\sqrt{n})2} K . \\end{aligned} \\] Based on this, a prior should be chosen so that the prior mass is located near, \\[ \\tau_0 = \\frac{p_0}{K - p_0}\\frac{\\sigma}{\\sqrt{n}} \\] Densities of the shrinkage parameter, \\(\\kappa\\), for various shrinkage distributions where \\(\\sigma^2 = 1\\), \\(\\tau = 1\\), for \\(n = 1\\). Datta and Ghosh (2013) warn against empirical Bayes estimators of \\(\\tau\\) for the horseshoe prior as it can collapse to 0. Scott and Berger (2010) consider marginal maximum likelihood estimates of \\(\\tau\\). Pas, Kleijn, and Vaart (2014) suggest that an empirical Bayes estimator truncated below at \\(1 / n\\). 13.9 R Implementations See rstanarm: estimates GLM regressions with various priors rmonomvn: estimates Bayesian ridge, lasso, horseshoe, and ridge regression. bayesreg: See Makalic and Schmidt (2016) for documentation and a good review of Bayesian regularized regression. fastHorseshoe 13.10 Bayesian Model Averaging Bayesian model averaging (BMA) is method that calculates a posterior distribution of parameters by averaging over a discrete set of models, weighting them by their model evidence. Suppose there are \\(K\\) models, \\(M_k\\), \\(k = 1, \\dots, K\\) with the likelihood function \\(L(y | \\theta_k, M_k)\\) for observed data \\(y\\). The posterior distribution of parameters \\(\\theta\\), conditional on each model is, \\[ p(\\theta_k | y, M_k) = \\frac{L(y | \\theta_k | M_k) p(\\theta_k | M_k)}{\\int L(y | \\theta_k, M_k) p(\\theta_k | M_k) d\\,\\theta_k} \\] The essential quality for BMA applications is the denominator of this equation is the the marginal likelihood or model evidence, \\[ p(y | M_k) = \\int L(y | \\theta_k, M_k) p(\\theta_k | M_k) d\\,\\theta_k . \\] From this, derive the posterior probability of models given the data, \\[ p(M_k | y) = \\frac{p(y | M_k) p(M_k)}{\\sum_{m = 1}^K p(y | M_m) p(M_m)} \\] The posterior probability of a model requires specifying a prior \\(p(M_k)\\) for each model. Bayes Factors can be used to calculate model probabilities for BMA and vice-versa. The Bayes Factor for models \\(l\\) and \\(m\\) is \\[ BF_{lm} = \\frac{p(M_l | y)}{p(M_m | y)} . \\] Given a baseline model, \\(M_1\\), the model evidence can be written in terms of Bayes Factors relative to that model, \\[ p(M_l|y) = \\frac{BF_{1l} p(M_l)}{\\sum_{m = 1}^K BF_{m1} p(M_m)} . \\] marginal probabilities of a parameter: The marginal probability of a parameter (\\(\\theta\\)), averaged across all models is, \\[ p(\\theta | y) = \\sum_{k = 1}^K p(\\theta | y, M_k) p(M_k | y) . \\] The posterior distribution of \\(\\Delta\\) averaged across all models is the average of \\(\\Delta\\) weighted by each posterior model probability. The mean and variance of the posterior models are, \\[ \\begin{aligned}[t] \\E[\\theta | y] &amp;= \\sum_{k = 1}^K \\bar{\\theta} p(M_k | y) \\\\ \\Var[\\theta | y] &amp;= \\sum_{k = 1}^K (\\Var(\\theta | y, M_k) + \\bar{\\theta}_k^2) p(M_k | y) - \\E(\\theta | y)^2 \\end{aligned} \\] Why is BMA difficult? The posterior is sensitive to the model prior, \\(p(M_k)\\). Calculating the model evidence, \\(p(y | M_k)\\), is computationally difficult, except in special cases The model space can be very large. In regression, it is \\(2^K\\). This means that it may be impossible to compute model probabilities for the full set of models. Thus, it may require sampling from the (discrete) model space. Uses of BMA: model selection or choice: select the most likely model average posterior estimates average prediction. Generally predictions from models using BMA have lower risk (Raftery) For the common case of linear regression, \\[ \\begin{aligned}[t] y &amp;= \\alpha + X \\beta + \\epsilon &amp; \\epsilon &amp;\\sim \\dnorm(0, \\sigma^2 I) \\end{aligned} \\] where \\(X\\) is a \\(N \\times K\\) matrix and \\(\\beta\\) is a \\(K \\times 1\\) vector. The model selection problem in this case is the choice of the \\(K\\) variables to include in the regression model. Thus, there are \\(2^K\\) models to consider. Very quickly, See Fragoso and Neto (2015) for a recent review. See Volinsky et al. (1999) for an earlier review. There are several R packages that implement BMA. See M. and F. (2011) for a review of R packages. BAS See its vignette Zeugner (2011). BMA See its vignette Raftery et al. (2017). BMS See its vignette Clyde (2017). ensembleBMA uses BMA to generates ensemble BMA forecasts 13.10.1 Zellner’s g-prior An alternative prior is the Zellner’s g-prior. Consider the regression, \\[ y_i | \\alpha, \\vec{\\beta}, \\sigma \\sim N(\\alpha + \\mat{X} \\vec{\\beta}, \\sigma^2) \\] The \\(g\\)-prior is a non-informative, data-dependent prior, \\[ \\vec{\\beta} \\sim N(0, \\sigma^2 g \\mat{X}\\T \\mat{X}) \\] It depends on only a single parameter \\(g\\). The prior for \\(g\\) must be proper. Some common choices include, \\[ \\begin{aligned} g &amp;= n \\\\ g &amp;= k^2 \\\\ g &amp;= \\max(n, k^2) \\end{aligned} \\] or putting a hyperprior on \\(g\\). See Ley and Steel (2012) for a recent overview of g-priors. 13.11 Slab and Spike Priors In the case of the linear regression, an alternative to BMA is to use a spike-and-slab prior (Mitchell and Beauchamp 1988, George and McCulloch (1993), Ishwaran and Rao (2005)), which is a prior that is a discrete mixture of a point mass at 0 and a non-informative distribution. The weight over these who alternatives is similar to a The spike and slab prior is a “two-group” solution \\[ p(\\beta_k) = (1 - w) \\delta_0 + w \\pi(\\beta_k) \\] where \\(\\delta_0\\) is a Dirac delta function putting a point mass at 0, and \\(\\pi(\\beta_k)\\) is an uninformative distribution, e.g. \\(\\pi(\\beta_k) = \\dnorm(\\beta_k | 0, \\sigma^2)\\) where \\(\\sigma\\) is large. The posterior distribution of \\(w\\) is the probability that \\(\\beta_k \\neq 0\\), and the conditional posterior distribution \\(p(\\beta_k | y, w = 1)\\) is the distribution of \\(\\beta_k\\) given that \\(\\beta_k \\neq 0\\). See the R package spikeslab and he accompanying article (Ishwaran, Kogalur, and Rao 2010) for an implementation and review of spike-and-slab regressions. 13.12 Technical Notes Marginal density of the horseshoe+ prior Carvalho, Polson, and Scott (2010) has no closed form but some bounds are available. If \\(\\tau^2 = 1\\), then the marginal density of the horseshoe+ prior has the following properties: \\[ \\begin{aligned}[t] \\frac{K}{2} \\log \\left(1 + \\frac{4}{\\theta^2} \\right) &lt; p_{HS}(\\theta) \\leq K \\log \\left(1 + \\frac{2}{\\theta^2} \\right) \\\\ \\lim_{|\\theta| \\to 0} p_{HS}(\\theta) = \\infty \\end{aligned} \\] where \\(K = 1 / \\sqrt{2 \\pi^3}\\). Marginal density of the horseshoe+ prior Bhadra et al. (2015): If \\(\\tau^2 = 1\\), then the marginal density of the horseshoe+ prior has the following properties: \\[ \\begin{aligned}[t] \\frac{1}{\\pi^2 \\sqrt{2 \\pi}} \\log \\left(1 + \\frac{4}{\\theta^2} \\right) &lt; p_{HS+}(\\theta) \\leq \\frac{1}{\\pi^2 |\\theta|} \\\\ \\lim_{|\\theta| \\to 0} p_{HS+}(\\theta) = \\infty \\end{aligned} \\] Prior for \\(\\theta_i\\) Density for \\(\\lambda_i\\) Density for \\(\\kappa_i\\) Double-exponential $_i (_i^2 / 2) \\(\\kappa_i^{-2} \\exp\\left( \\frac{- 1}{2 \\kappa_i} \\right)\\) Cauchy \\(\\lambda_i^{-2} \\exp(-1 / \\lambda_i^2)\\) \\(\\kappa_i^{-\\frac{1}{2}} (1 - \\kappa_i)^{- \\frac{3}{2}} \\exp \\left(\\frac{\\kappa_i}{2 (1 - \\kappa_i)}\\right)\\) Strawderman-Berger \\(\\lambda_i (1 + \\lambda_i^2)^{-\\frac{3}{2}}\\) \\(\\kappa_i^{-\\frac{1}{2}}\\) Normal-exponential-gamma \\(\\lambda_i (1 + \\lambda_i^2)^{-(c + 1)}\\) \\(\\kappa_i^{c - 1}\\) Normal-Jeffreys \\(1 / \\lambda_i\\) \\(\\kappa_i^{-1} (1 - \\kappa_i)^{-1}\\) Horseshoe \\((1 + \\lambda_i^2)^{-1}\\) \\(\\kappa_i^{-1/2} (1 - \\kappa_i)^{-1/2}\\) Thresh-holding. The horseshoe has an implicit threshold of \\(|T_\\tau(y) - y| &lt; \\sqrt{2 \\sigma ^ 2 \\log (1 / \\tau))\\) (Pas, Kleijn, and Vaart 2014). 13.13 Multiple Comparisons and Thresholding rules Multiple comparisons, family-wise error rate, and false discovery rates are frequentist concepts. There are some attempts to bridge these two worlds - see Efron in particular. However, even if methodologically different, shrinkage addresses some of broadest concerns about making multiple comparisons. Although discussing hierarchical models, Gelman, Hill, and Yajima (2012) compares the shrinkage in hierarchical models to multiple comparisons, also see this post. Another (related) issue is sparsification. The decision rule as to whether a variable is 0 (included), or not. The sparse-shrinkage priors from Carvalho, Polson, and Scott (2010) are motivated by a two-group model (either \\(\\beta = 0\\) or \\(\\beta \\neq 0\\)). They suggest a decision rule of considering \\(\\beta \\neq 0\\) when \\(E(\\kappa_j) &lt; 0.5\\) where \\(\\kappa_j\\) is a shrinkage parameter described in the paper. Hahn and Carvalho (2015) propose estimating the posterior distribution via shrinkage, and then summarizing the posterior distribution. Piironen and Vehtari (2015) propose something similar in spirit, in which a second step projects the initial shrinkage model to a sparse model 13.14 Examples of Applications of Sensitivity Analysis These are a few applied papers based on sensitivity or model analysis meaning primarily problem, rather than methodologically driven. The memorably titled “Let’s Take the Con Out of Econometrics” (Leamer 1983), that economic models fail to account for model uncertainty and suggests using an ensemble-like method called extreme-bounds. On economic growth: (confidence bounds) Sala-I-Martin (1997), (Bayesian model averaging) Fernández, Ley, and Steel (2001), Ley and Steel (2009), Eicher, Papageorgiou, and Raftery (2009), Brock, Durlauf, and West (2003) Wars: Hegre and Sambanis (2006) use extreme bounds for civil war onset. Ward, Greenhill, and Bakke (2010) use model comparison and a step-wise method, but are focused on the difference between note the difference between p-values and prediction; Goenner (2004) use BMA for inter-state wars (democratic peace). Montgomery, Hollenbach, and Ward (2012) and Montgomery and Nyhan (2010) apply BMA to multiple political science issues including voting, presidential elections, and civil war onset. Tobias and Li (2004) Returns to Schooling See Fragoso and Neto (2015) for a recent(ish) review of BMA applications. Also, not that many of these analyses are slightly older as empirical research in economics and political science has put less emphasis on model-based inference (all-cause regressions) and more on design-based (causal) inference methods As noted earlier, regularization techniques are also applicable in these cases, but need to be adjusted. "],
["hierarchical-models.html", "14 Hierarchical Models 14.1 Baseball Hitting 14.2 Equivalent Models 14.3 Miscellaneous Mathematical Background", " 14 Hierarchical Models Hierarchical models: often groups of parameters, \\(\\{\\theta_1, \\dots, \\theta_J\\}\\), are related. E.g. countries, states, counties, years, etc. Even the regression coefficients, \\(\\beta_1, \\dots, \\beta_k\\) seen the in the Shrinkage and Regularization chapter. We can treat those \\(\\theta_j\\) as drawn from a population distribution, \\(\\theta_j \\sim p(\\theta)\\). The prior distribution \\(p(\\theta)\\) is called a hyperprior and its parameters are hyperparameters Exchangeability: parameters \\((\\theta_1, \\dots, \\theta_J)\\) are exchangeable if \\(p(\\theta_1, \\dots, \\theta_J)\\) don’t depend on the indexes. i.i.d. models are a special case of exchangeability. 14.1 Baseball Hitting Efron and Morris (1975) analyzed data from 18 players in the 1970 season. The goal was to predict the batting average of these 18 players from their first 45 at-bats for the remainder of the 1970 season. The following example is based on Carpenter, Gabry, and Goodrich (2017) and the rstanarm vignette Hierarchical Partial Pooling for Repeated Binary Trials. The hitting data used in Efron and Morris (1975) is included in rstanarm as rstanarm: data(&quot;bball1970&quot;, package = &quot;rstanarm&quot;) bball1970 &lt;- mutate(bball1970, BatAvg1 = Hits / AB, BatAvg2 = RemainingHits / RemainingAB) bball1970 #&gt; Player AB Hits RemainingAB RemainingHits BatAvg1 BatAvg2 #&gt; 1 Clemente 45 18 367 127 0.400 0.346 #&gt; 2 Robinson 45 17 426 127 0.378 0.298 #&gt; 3 Howard 45 16 521 144 0.356 0.276 #&gt; 4 Johnstone 45 15 275 61 0.333 0.222 #&gt; 5 Berry 45 14 418 114 0.311 0.273 #&gt; 6 Spencer 45 14 466 126 0.311 0.270 #&gt; 7 Kessinger 45 13 586 155 0.289 0.265 #&gt; 8 Alvarado 45 12 138 29 0.267 0.210 #&gt; 9 Santo 45 11 510 137 0.244 0.269 #&gt; 10 Swaboda 45 11 200 46 0.244 0.230 #&gt; 11 Petrocelli 45 10 538 142 0.222 0.264 #&gt; 12 Rodriguez 45 10 186 42 0.222 0.226 #&gt; 13 Scott 45 10 435 132 0.222 0.303 #&gt; 14 Unser 45 10 277 73 0.222 0.264 #&gt; 15 Williams 45 10 591 195 0.222 0.330 #&gt; 16 Campaneris 45 9 558 159 0.200 0.285 #&gt; 17 Munson 45 8 408 129 0.178 0.316 #&gt; 18 Alvis 45 7 70 14 0.156 0.200 Let \\(y_i\\) be the number of hits in the first 45 at bats for player \\(i\\), \\[ \\begin{aligned}[t] y_i &amp; \\sim \\dbin(45, \\mu_i), \\] where \\(\\mu_i \\in (0, 1)\\) is the player-specific batting average. Priors will be placed on the log-odds parameter, \\(\\eta \\in \\R\\), \\[ \\begin{aligned}[t] \\mu_i &amp;\\sim \\frac{1}{1 + \\exp(-\\eta_i)} . \\\\ \\end{aligned} \\] This example considers three ways of modeling \\(\\mu_i\\): Complete Pooling: All players have the same batting average parameter. \\[ \\eta_i = \\eta . \\] The common (log-odds) batting average is given a weakly informative prior, \\[ \\eta \\sim \\dnorm(0, 2.5) \\] On the log odds scale, this places 95% of the probability mass between NaN and NaN on the proportion scale. Non-pooled: Each players (log-odds) batting average is independent, with each assigned a separate weak prior. \\[ \\begin{aligned}[t] \\eta_i &amp;\\sim \\dnorm(0, 2.5) \\end{aligned} \\] Partial-pooling: Each player has a separate (log-odds) batting average, but these batting average parameters are drawn from a common normal distribution. \\[ \\begin{aligned}[t] \\eta_i &amp;\\sim \\dnorm(0, \\tau) \\\\ \\tau &amp;\\sim \\dnorm(0, 1) \\end{aligned} \\] bball1970_data &lt;- list( N = nrow(bball1970), k = bball1970$AB, y = bball1970$Hits, k_new = bball1970$RemainingAB, y_new = bball1970$RemainingHits ) Create a list to store models: models &lt;- list() models[[&quot;nopool&quot;]] &lt;- stan_model(&quot;stan/binomial-no-pooling.stan&quot;) models[[&quot;nopool&quot;]] /* Binomial Model (No pooling) A binomial model for $i = 1, \\dots, N$, no pooling: $$ p(y_i | n_i, \\mu_i) &\\sim \\mathsf{Binomial}(y_i | n_i, \\mu_i) \\\\ \\mu_i &= \\logit^{-1}(\\eta_i) \\\\ p(\\eta_i) &\\sim \\mathsf{Normal}^+(0, 10) $$ */ data { int N; int y[N]; int k[N]; // new data int y_new[N]; int k_new[N]; } parameters { vector[N] eta; } model { eta ~ normal(0., 10.); y ~ binomial_logit(k, eta); } generated quantities { int y_rep[N]; vector[N] log_lik; vector[N] log_lik_new; vector[N] mu; mu = inv_logit(eta); for (n in 1:N) { y_rep[n] = binomial_rng(k[n], mu[n]); log_lik[n] = binomial_logit_lpmf(y[n] | k[n], eta[n]); log_lik_new[n] = binomial_logit_lpmf(y_new[n] | k_new[n], eta[n]); } } models[[&quot;pool&quot;]] &lt;- stan_model(&quot;stan/binomial-complete-pooling.stan&quot;) models[[&quot;pool&quot;]] /* Binomial Model A binomial model for $i = 1, \\dots, N$, with complete pooling $$ \\begin{aligned}[t] p(y_i | n_i, \\mu) &\\sim \\mathsf{Binomial}(n_i, \\mu) \\\\ \\mu &= \\logit^{-1}(\\eta) \\\\ p(\\eta) &\\sim \\mathsf{Normal}^+(0, 10) \\end{aligned} $$ */ data { int N; int y[N]; int k[N]; // new data int y_new[N]; int k_new[N]; } parameters { real eta; } model { eta ~ normal(0., 10.); y ~ binomial_logit(k, eta); } generated quantities { int y_rep[N]; vector[N] log_lik; vector[N] log_lik_new; real mu; mu = inv_logit(eta); for (n in 1:N) { // y_rep[n] = binomial_rng(k[n], mu); log_lik[n] = binomial_logit_lpmf(y[n] | k[n], eta); log_lik_new[n] = binomial_logit_lpmf(y_new[n] | k_new[n], eta); } } models[[&quot;partial&quot;]] &lt;- stan_model(&quot;stan/binomial-partial-pooling.stan&quot;) models[[&quot;partial&quot;]] /* Binomial Model A binomial model for $i = 1, \\dots, N$, with partial pooling $$ \\begin{aligned}[t] p(y_i | n_i, \\mu_i) &\\sim \\mathsf{Binomial}(y_i | n_i, \\mu_i) \\\\ \\mu_i &= \\logit^{-1}(\\eta_i) \\\\ p(\\eta_i | \\tau) &\\sim \\mathsf{Normal}(alpha, \\tau) \\\\ p(\\tau) &\\sim \\mathsf{Normal}^+(0, 1) \\\\ p(alpha) & \\sim \\mathsf{Normal}(0, 2.5) \\\\ \\end{aligned} $$ */ data { int N; int y[N]; int k[N]; // new data int y_new[N]; int k_new[N]; } parameters { vector[N] eta; real alpha; real tau; } model { alpha ~ normal(0., 10.); tau ~ normal(0., 1); eta ~ normal(alpha, tau); y ~ binomial_logit(k, eta); } generated quantities { int y_rep[N]; vector[N] log_lik; vector[N] log_lik_new; vector[N] mu; mu = inv_logit(eta); for (n in 1:N) { // y_rep[n] = binomial_rng(k[n], mu[n]); log_lik[n] = binomial_logit_lpmf(y[n] | k[n], eta[n]); log_lik_new[n] = binomial_logit_lpmf(y_new[n] | k_new[n], eta[n]); } } Sample from all three models a fits &lt;- map(models, sampling, data = bball1970_data, refresh = -1) %&gt;% set_names(names(models)) #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.081766 seconds (Warm-up) #&gt; 0.063807 seconds (Sampling) #&gt; 0.145573 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 6e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.078533 seconds (Warm-up) #&gt; 0.063834 seconds (Sampling) #&gt; 0.142367 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 7e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.080248 seconds (Warm-up) #&gt; 0.063807 seconds (Sampling) #&gt; 0.144055 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 6e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.081984 seconds (Warm-up) #&gt; 0.064391 seconds (Sampling) #&gt; 0.146375 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.030174 seconds (Warm-up) #&gt; 0.030791 seconds (Sampling) #&gt; 0.060965 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 4e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.03112 seconds (Warm-up) #&gt; 0.02919 seconds (Sampling) #&gt; 0.06031 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 3e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.030345 seconds (Warm-up) #&gt; 0.03132 seconds (Sampling) #&gt; 0.061665 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 5e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.031033 seconds (Warm-up) #&gt; 0.029282 seconds (Sampling) #&gt; 0.060315 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 1.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.115527 seconds (Warm-up) #&gt; 0.168691 seconds (Sampling) #&gt; 0.284218 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 8e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.108639 seconds (Warm-up) #&gt; 0.107101 seconds (Sampling) #&gt; 0.21574 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 2 #&gt; count #&gt; Exception thrown at line 31: normal_lpdf: Scale parameter is 0, but must be &gt; 0! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; Gradient evaluation took 9e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.154264 seconds (Warm-up) #&gt; 0.145727 seconds (Sampling) #&gt; 0.299991 seconds (Total) #&gt; #&gt; #&gt; Gradient evaluation took 8e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; #&gt; Elapsed Time: 0.130128 seconds (Warm-up) #&gt; 0.077703 seconds (Sampling) #&gt; 0.207831 seconds (Total) #&gt; Warning: There were 387 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See #&gt; http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup #&gt; Warning: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See #&gt; http://mc-stan.org/misc/warnings.html#bfmi-low #&gt; Warning: Examine the pairs() plot to diagnose sampling problems For each model calculate the posterior mean of \\(\\mu\\) for each player: bball1970 &lt;- map2_df(names(fits), fits, function(nm, fit) { mu &lt;- broom::tidy(fit) %&gt;% filter(str_detect(term, &quot;^mu&quot;)) if (nrow(mu) == 1) { out &lt;- tibble(estimate = rep(mu$estimate, 18L)) } else { out &lt;- select(mu, estimate) } out$model &lt;- nm out$.id &lt;- seq_len(nrow(out)) out }) %&gt;% spread(model, estimate) %&gt;% bind_cols(bball1970) The partially pooled estiamtes are shrunk towards the overall average, and are between the no-pooling and pooled estimates. select(bball1970, Player, nopool, partial, pool) %&gt;% mutate(Player = factor(Player, levels = Player)) %&gt;% gather(variable, value, -Player) %&gt;% ggplot(aes(y = value, x = factor(variable), group = Player)) + geom_point() + geom_line() + labs(x = &quot;&quot;, y = expression(mu)) We can plot the actual batting averages (BatAvg1 and BatAvg2) and the model estimates: select(bball1970, Player, nopool, partial, pool, BatAvg1, BatAvg2) %&gt;% mutate(Player = factor(Player, levels = Player)) %&gt;% gather(variable, value, -Player) %&gt;% ggplot(aes(y = Player, x = value, colour = variable)) + geom_point() The estimates of the no-pooling model is almost exactly the same as BatAvg1. The out-of-sample batting averages BatAvg2 show regression to the mean. For these models, compare the overall out-of-sample performance by calculating the actual average out-of-sample log-pointwise predictive density (lppd), and the expected lppd using LOO-PSIS. The LOO-PSIS estimates of the out-of-sample lppd are optimistic. However, they still show the pooling and partial estimates as superior to the no-pooling estimates. The actual out-of-sample average lppd for the partial pooled model is the best fitting. map2_df(names(fits), fits, function(nm, fit) { loo &lt;- loo(extract_log_lik(fit, &quot;log_lik&quot;)) ll_new &lt;- rstan::extract(fit)[[&quot;log_lik_new&quot;]] tibble(model = nm, loo = loo$elpd_loo / bball1970_data$N, ll_out = mean(log(colMeans(exp(ll_new))))) }) #&gt; Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k- #&gt; diagnostic&#39;) for details. #&gt; Warning: Some Pareto k diagnostic values are slightly high. See #&gt; help(&#39;pareto-k-diagnostic&#39;) for details. #&gt; # A tibble: 3 × 3 #&gt; model loo ll_out #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 nopool -3.20 -4.60 #&gt; 2 pool -2.58 -4.06 #&gt; 3 partial -2.59 -4.02 To see why this is the case, plot the average errors for each observation in- and out-of-sample. In-sample for the no-pooling model is zero, but it over-estimates (under-estimates) the players with the highest (lowest) batting averages in their first 45 at bats—this is regression to the mean. In sample, the partially pooling model shrinks the estimates towards the mean and reducing error. Out of sample, the errors of the partially pooled model are not much different than the no-pooling model, except that the extreme observations have lower errors. select(bball1970, Player, nopool, partial, pool, BatAvg1, BatAvg2) %&gt;% mutate(Player = as.integer(factor(Player, levels = Player))) %&gt;% gather(variable, value, -Player, -matches(&quot;BatAvg&quot;)) %&gt;% mutate(`In-sample Errors` = value - BatAvg1, `Out-of-sample Errors` = value - BatAvg2) %&gt;% select(-matches(&quot;BatAvg&quot;), -value) %&gt;% gather(sample, error, -variable, -Player) %&gt;% ggplot(aes(y = error, x = Player, colour = variable)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 2) + geom_point() + geom_line() + facet_wrap(~ sample, ncol = 1) + theme(legend.position = &quot;bottom&quot;) References: Albert, Jim. Revisiting Efron and Morris’s Baseball Study Feb 15, 2016 Bob Carpenter. Hierarchical Bayesian Batting Ability, with Multiple Comparisons. November 4, 2009. John Kruschke. Shrinkage in multi-level hierarchical models. November 27, 2012. See Jensen, McShane, and Wyner (2009) for an updated hierarchical model of baseball hitting 14.1.1 Other Examples Rat Tumors - BDA Eight Schools - BDA 14.2 Equivalent Models 14.2.1 Group Varying Intercepts This is a regression, with a different intercept per group: \\[ \\begin{aligned}[t] y_i &amp;\\sim N(\\alpha_j[i] + \\beta x_i, \\sigma_y^2) \\\\ \\end{aligned} \\] The second level model of the group intercepts models them as distributed around a common mean, \\(\\mu_\\alpha\\), with error: \\[ \\alpha_j = \\mu_\\alpha + \\eta_j \\\\ \\eta_j \\sim N(0, \\sigma_\\alpha^2) \\] 14.2.2 Separate local regressions For each group, run a regression, \\[ \\begin{aligned}[t] y_i \\sim N(\\alpha_j + \\beta x_i, \\sigma_y^2) &amp; \\text{for all $i$ in group $j$} \\end{aligned} \\] And now model the group-level means, \\[ \\begin{aligned}[t] \\alpha &amp;= \\gamma u_j + \\eta_j \\\\ \\eta_j &amp;\\sim N(0, \\sigma^2_\\alpha) \\end{aligned} \\] 14.2.3 Modeling the coefficients of a large regression model Suppose that \\(X\\) includes all predictors and \\(J\\) indicators for the \\(J\\) groups. We could also put the constant in the second distribution. The coefficients \\(\\beta\\) for the coefficients on the group indicators are centered around a \\(\\mu_\\alpha\\). \\[ \\begin{aligned}[t] y_i &amp;\\sim N(x_i \\beta, \\sigma_y^2) \\\\ \\beta_j &amp;\\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2) \\end{aligned} \\] 14.2.4 Regression with multiple error terms \\[ \\begin{aligned}[t] y_i &amp;\\sim N(x_i \\beta + \\eta_{j[i]}, \\sigma_y^2) \\\\ \\eta_j &amp;\\sim N(0, \\sigma_{\\alpha}^2) \\end{aligned} \\] 14.2.5 Regresion with correlated errors \\[ \\begin{aligned}[t] y_i = X_i \\beta + \\omega_i, &amp; \\omega \\sim N(0, \\Sigma) \\end{aligned} \\] The errors have an \\(n \\times n\\) covariance matrix, and are equivalent to the sum of individual and group errors. \\[ \\omega_i = \\eta_{j[i]} + \\epsilon_i \\] The variances and covariances in \\(\\Sigma\\) are: for unit \\(i\\): \\(\\Sigma_{ii} = \\var(\\omega_i) = \\sigma_y^2 + \\sigma_{\\alpha}^2\\) for units \\(i\\), \\(k\\) in same group \\(j\\): \\(\\Sigma_{ik} = \\cov(\\omega_i, \\omega_k) = \\sigma_{\\alpha}^2\\) for units \\(i\\), \\(k\\) in the same group \\(j\\): \\(\\Sigma_{ik} = 0\\) library(&quot;stringr&quot;) 14.3 Miscellaneous Mathematical Background 14.3.1 Location-Scale Families In a location-scale family of distributions, if the random variable \\(X\\) is distributed with mean 0 and standard deviation 1, then the random variable \\(Y\\), \\[ Y = \\mu + \\sigma X , \\] has mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Normal distribution: Suppose \\(X \\sim \\dnorm(0, 1)\\), then \\[ Y = \\mu + \\sigma X, \\] is equivalent to \\(Y \\sim \\dnorm(\\mu, \\sigma)\\) (normal with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)). ** Student-t distribution** (including Cauchy): \\[ \\begin{aligned}[t] X &amp;\\sim \\dt{\\nu}(0, 1) \\\\ Y &amp;= \\mu + \\sigma X \\end{aligned} \\] implies \\[ Y \\sim \\dt{\\nu}(\\mu, \\sigma), \\] i.e. \\(Y\\) is distributed Student-\\(t\\) with location \\(\\mu\\) and scale \\(\\sigma\\). In Stan, it can be useful parameterize distributions in terms of a mean 0, scale 1 parameters, and separate parameters for the locations and scales. E.g. with normal distributions, parameters { real mu; real&lt;lower = 0.0&gt; sigma; vector[n] eps; } transformed parameters { vector[n] y; y = mu + sigma * eps; } model { eps ~ normal(0.0, 1.0); } 14.3.2 Scale Mixtures of Normal Distributions Some commonly used distributions can be represented as scale mixtures of normal distributions. For formal details of scale mixtures of normal distributions see West (1987). Distributions that are scale-mixtures of normal distributions can be written as, \\[ Y \\sim \\dnorm(\\mu, \\sigma_i^2) \\\\ \\sigma_i \\sim \\pi(\\sigma_i) \\] As its name suggests, the individual variances (scales) themselves, have a distribution. Some examples: Student-t Double Exponential Horseshoe or Hierarchical Shrinkage (HS) Horseshoe Plus or Hierarchical Shrinkage Plus (HS+) Even when analytic forms of the distribution are available, representing them as scale mixtures of normal distributions may be convenient in modeling. In particular, it may allow for drawing samples from the distribution easily. And in HMC, it may induce a more tractable posterior density. 14.3.3 Covariance-Correlation Matrix Decomposition The suggested method for modeling covariance matrices in Stan is the separation strategy which decomposes a covariance matrix \\(\\Sigma\\) can be decomposed into a standard deviation vector \\(\\sigma\\), and a correlation matrix \\(R\\) (Barnard, McCulloch, and Meng 2000), \\[ \\Sigma = \\diag(\\sigma) R \\diag(\\sigma) . \\] This is useful for setting priors on covariance because separate priors can be set for the scales of the variables via \\(\\sigma\\), and the correlation between them, via \\(R\\). The rstanarm decov prior goes further and decomposes the covariance matrix into a correlation matrix, \\(\\mat{R}\\), a diagonal variance matrix \\(\\mat{\\Omega}\\) with trace \\(n \\sigma^2\\), a scalar global variance \\(\\sigma^2\\), and a simplex \\(\\vec{\\pi}\\) (proportion of total variance for each variable): \\[ \\begin{aligned}[t] \\mat{\\Sigma} &amp;= \\mat{\\Omega} \\mat{R} \\\\ \\diag(\\mat{\\Omega}) &amp;= n \\vec{\\pi} \\sigma^2 \\end{aligned} \\] Separate and interpretable priors can be put on \\(\\mat{R}\\), \\(\\vec{\\pi}\\), and \\(\\sigma^2\\). The LKJ (Lewandowski, ) distribution is a distribution over correlation coefficients, \\[ R \\sim \\dlkjcorr(\\eta) , \\] where \\[ \\dlkjcorr(\\Sigma | \\eta) \\propto \\det(\\Sigma)^{(\\eta - 1)} . \\] This distribution has the following properties: \\(\\eta = 1\\): uniform correlations \\(\\eta \\to \\infty\\): approaches the identity matrix \\(0 &lt; \\eta &lt; 1\\): there is a trough at the identity matrix with higher probabilities placed on non-zero correlations. For all positive \\(\\eta\\) (\\(\\eta &gt; 0\\)), \\(\\E(R) = \\mat{I}\\). lkjcorr_df &lt;- function(eta, n = 2) { out &lt;- as.data.frame(rlkjcorr(n, eta)) out$.row &lt;- seq_len(nrow(out)) out &lt;- gather(out, .col, value, -.row) out$.col &lt;- as.integer(str_replace(out$.col, &quot;^V&quot;, &quot;&quot;)) out$eta &lt;- eta out } lkjsims &lt;- purrr::map_df(c(0.01, 0.1, 1, 2, 50, 1000), lkjcorr_df, n = 50) This simulates a single matrix from the LKJ distribution with different values of \\(\\eta\\). As \\(\\eta \\to \\infty\\), the off-diagonal correlations tend towards 0, and the correlation matrix to the identity matrix. ggplot(lkjsims, aes(x = .row, y = .col, fill = value)) + facet_wrap(~ eta, ncol = 2) + scale_fill_distiller(limits = c(-1, 1), type = &quot;div&quot;, palette = &quot;RdYlBu&quot;) + geom_raster() + theme_minimal() + theme(panel.grid = element_blank(), axis.text = element_blank()) + labs(x = &quot;&quot;, y = &quot;&quot;) The density of the off-diagonal correlations. lkjsims %&gt;% filter(.row &lt; .col) %&gt;% ggplot(aes(x = value, colour = factor(eta))) + geom_density() For other discussions of the LKJ correlation distribution, see these: https://stats.stackexchange.com/questions/2746/how-to-efficiently-generate-random-positive-semidefinite-correlation-matrices/125017#125017 http://www.zinkov.com/posts/2015-06-09-where-priors-come-from/ http://www.psychstatistics.com/2014/12/27/d-lkj-priors/ 14.3.4 QR Factorization For a full-rank \\(N \\times K\\) matrix, the QR factorization is \\[ \\mat{X} = \\mat{Q} \\mat{R} \\] where \\(\\mat{Q}\\) is an orthonormal matrix such that \\(\\mat{Q}\\T \\mat{Q}\\) and \\(\\mat{R}\\) is an upper triangular matrix. Stan function Team (2016) suggest writing it is \\[ \\begin{aligned}[t] \\mat{Q}^* = \\mat{Q} \\times \\sqrt{N - 1} \\\\ \\mat{R}^* = \\frac{1}{\\sqrt{N - 1}} \\mat{R} \\end{aligned} \\] This is used for solving linear model. Suppose \\(\\vec{\\beta}\\) is a \\(K \\times 1\\) vector, then \\[ \\vec{eta} = \\mat{x} \\vec{\\beta} = \\mat{Q} \\mat{R} \\vec{\\beta} = \\mat{Q}^* \\mat{R}^* \\vec{\\beta} . \\] Suppose \\(\\mat{theta} = \\mat{R}^* \\vec{\\beta}\\), then \\(\\vec{eta} = \\mat{Q}^* \\mat{\\theta}\\) and \\(\\vec{beta} = {\\mat{R}^*}^{-1} \\mat{\\theta}\\). rstanarm provides a prior for a normal linear model which uses the QR decomposition to parameterize a prior in terms of \\(R^2\\). Stan functions: qr_Q(matrix A) qr_R(matrix A) See Team (2016 Sec 8.2) 14.3.5 Cholesky Decomposition The Cholesky decomposition of a positive definite matrix \\(A\\) is, \\[ \\mat{A} = \\mat{L} \\mat{L}\\T , \\] where \\(\\mat{L}\\) is a lower-triangular matrix. It is similar to a square root for a matrix. It often more numerically stable or efficient to work with the Cholesky decomposition, than with a covariance matrix. When working with the covariance matrix, numerical precision can result in a non positive definite matrix. However, working with \\(\\mat{L}\\) will ensure that \\(\\mat{A} = \\mat{L} \\mat{L}\\T\\) will be positive definite. In Stan Types types cholesky_factor_cov, and cholesky_factor_corr represent the Cholesky factor of covariance and correlation matrices, respectively. Cholesky decomposition function is cholesky_decompose(matrix A) Multiple functions in Stan are parameterized with Cholesky decompositions instead of or in addition to covariance matrices. Use them if possible; they are more numerically stable. lkj_corr_chol_lpdf multi_normal_cholesky_lpdf The Cholesky factor is used for sampling from a multivariate normal distribution using i.i.d. standard normal distributions. Suppose \\(X_1, \\dots, X_N\\) are \\(N\\) i.i.d. standard normal distributions, \\(\\mat{\\Omega}\\) is an \\(N \\times N\\) lower-triangular matrix such that \\(\\mat{\\Omega} \\mat{Omega}\\T = \\mat{\\Sigma}\\), and \\(\\mu\\) is an \\(N \\times 1\\) vector, then \\[ \\vec{\\mu} + \\mat{\\Omega} X \\sim \\dnorm(\\vec{\\mu}, \\mat{\\Sigma}) \\] See Team (2016, 40, 147, 241, 246) "],
["notes.html", "15 Notes 15.1 Syllabi 15.2 Textbooks 15.3 Topics 15.4 Computation Methods 15.5 Model Checking 15.6 General Applications and Models 15.7 Hierarchical Modeling 15.8 Shrinkage/Regularization 15.9 Bayes Theorem Examples 15.10 Good-Turing Estimator 15.11 Reproducibility 15.12 Empirical Bayes 15.13 Things to cover", " 15 Notes 15.1 Syllabi Ryan Bakker and Johannes Karreth, “Introduction to Applied Bayesian Modeling” ICPSR. Summer 2016. Syllabus code Justin Esarey. “Advanced Topics in Political Methodology: Bayesian Statistics” Winter 2015. Syllabus Lectures Kruschke. Doing Bayesian Data Analysis site Nick Beauchamp. “Bayesian Methods.” NYU syllabus Alex Tanhk. “Bayesian Methods for the Social Sciences” U of Wisconsin. Spring 2017. syllabus MTH225 Statistics for Science Spring 2016 github website Ben Goodrich, “Bayesian Statistics for Social Sciences” Columbia University. Spring 2016. Bakker. “ntroduction to Applied Bayesian Analysis” University of Georgia. syllabus site Myimoto. “Advances in Quantitative Psychology: Bayesian Statistics, Modeling &amp; Reasoning” U of Washington. Winter 2017. site Kruschke. “Bayesian Data Analysis” Indiana University. Spring 2016. PyMC code Blackwell and Spirling. 2002. “Topics in Political Methodology” Harvard. Fall 2014. Syllabus. It has a couple of classes on Bayesian methods. Neil Frazer. Bayesian Data Analysis. Hawaii. Spring 2017. syllabus Lopes. 2016. Bayesian Statistical Learning: Readings in Statistics and Econometrics. http://hedibert.org/current-teaching/ Lopes. 2012 Simulation-based approaches to modern Bayesian econometrics. Short course. Lopes. 2015. Bayesian Econometrics. http://hedibert.org/current-teaching/ 15.2 Textbooks Gelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. CRC Press. Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. “Understanding Predictive Information Criteria for Bayesian Models.” Statistics and Computing 24 (6). Springer: 997–1016. Gill, Jeff. 2008. Bayesian Methods : A Social and Behavioral Sciences Approach. Second. Boca Raton: Chapman &amp; Hall/CRC. Jackman, Simon. 2009. Bayesian Analysis for the Social Sciences. Chichester, UK: Wiley. Kruschke, John. 2010. Doing Bayesian Data Analysis: A Tutorial Introduction with R. Academic Press. Lynch, Scott M. 2007. Introduction to Applied Bayesian Statistics and Estimation for Social Scientists. New York: Springer. McElreath, Richard. 2016. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Vol. 122. CRC Press. github page for Statistical Rethinking https://github.com/rmcelreath/rethinking http://xcelab.net/rm/statistical-rethinking/ Lunn, David, Chris Jackson, Nicky Best, Andrew Thomas, and David Spiegelhalter. 2012. The BUGS Book: A Practical Introduction to Bayesian Analysis. Boca Raton, FL: Chapman; Hall/CRC. Suess, Eric A. and Bruce E. Trumbo. 2010. Introduction to Probability Simulation and Gibbs Sampling with R. New York: Springer. Suess, Eric A. and Bruce E. Trumbo. 2010. Introduction to Probability Simulation and Gibbs Sampling with R. New York: Springer. Peter Hoff. 2009. A First Course in Bayesian Statistical Methods Jaynes. 2003. Probability Theory: The Logic of Science. Congdon. 2014. Applied Bayesian Modeling. Wakefield. 2013. Bayesian and Frequentist Regression Methods Casella and Roberts. 2004. Monte Carlo Statistical Methods Marin and Roberts. 2014. Bayesian Essentials with R. http://www.springer.com/us/book/9781461486862 15.3 Topics 15.3.1 Overviews Michael Clarke Bayesian Basics Jackman. 2004. Bayesian Analysis for Political Research. Annual Review of Political Science DOI: 10.1146/annurev.polisci.7.012003.104706 Kruschke, J.K. &amp; Liddell, T.M. Psychon Bull Rev (2017). doi:10.3758/s13423-016-1221-4 - Cumming, G. (2014). The new statistics why and how. Psychological Science, 25(1), 7–29. 15.3.2 Bayesian Philosophy Efron. 2010. The Future of Indirect Evidence. Stat Sci doi:10.1214/09-STS308 Berger. 2006. The case for objective Bayesian analysis. Bayesian Anal doi:10.1214/06-BA115 Brad Efron “Why Isn’t Everyone a Bayesian?” The American Statistician, Vol. 40, No. 1 (Feb., 1986) [include following discussion of Efron’s article] Chernoff. http://dx.doi.org/10.1080/00031305.1986.10475343 Lindley. http://dx.doi.org/10.1080/00031305.1986.10475344 Morris. http://dx.doi.org/10.1080/00031305.1986.10475345 Press. http://dx.doi.org/10.1080/00031305.1986.10475346 Smith. http://dx.doi.org/10.1080/00031305.1986.10475347 Efron. Reply. http://dx.doi.org/10.1080/00031305.1986.10475348 Philosophy and the practice of Bayesian statistics in the social sciences1. tp://www.stat.columbia.edu/~gelman/research/published/philosophy_chapter.pdf Aris Spanos “Revisiting data mining: ‘hunting’ with or without a license” Rubin (1984) Rubin, Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician. Ann. Statist. 12 (1984), no. 4, 1151–1172. doi:10.1214/aos/1176346785. http://projecteuclid.org/euclid.aos/1176346785. Andrew Gelman Induction and Deduction in Bayesian Data Analysis Berger, James O. Could Fisher, Jeffreys and Neyman Have Agreed on Testing?. Statist. Sci. 18 (2003), no. 1, 1–32. doi:10.1214/ss/1056397485. http://projecteuclid.org/euclid.ss/1056397485. Gross2014a: Gross, J. H. (2015), Testing What Matters (If You Must Test at All): A Context-Driven Approach to Substantive and Statistical Significance. American Journal of Political Science, 59: 775–788. doi:10.1111/ajps.12149 Ng. and Jordan. On Discriminative vs. Generative classifiers: A Comparison of logistic regression and Naive Bayes: http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf 15.3.3 Bayesian Frequentist Debates Casella and Berger. 1987. Reconciling Bayesian and Frequentist Evidence in the One-Sided Testing Problem. JASA. doi:10.1080/01621459.1987.10478396 Bayesians and Frequentists : Models, Assumptions, and Inference slides Kasss Statitsical Inference: The Big Picture https://arxiv.org/pdf/1106.2895v2.pdf Noah Smith Bayesian vs. Frequentist: Is there any “there” there? Kass Kinds of Bayesians http://www.stat.cmu.edu/~kass/papers/kinds.pdf Anthony O’Hagan. Science, Subjectivity and Software (Comments on the articles by Berger and Goldstein) Good, I.J. (1971) 46656 varieties of Bayesians. Letter in American Statistician, 25: 62– 63. Reprinted in Good Thinking, University of Minnesota Press, 1982, pp. 20–21. 15.3.4 Categorical Agresti. Bayesian Inference for Categorical Data Analysis. http://www.stat.ufl.edu/~aa/cda2/bayes.pdf Perfect Separation Gelman. 2008. “A weakly informative default prior distribution for logistic and other regression models” Ann Applied Stat doi:10.1214/08-AOAS191 Rainey. 2016. “Dealing with Separation in Logistic Regression Models” Political Analysisa Rare Events King and Zheng. 2001. “Explaining Rare Events in International Relations” Int Org https://doi.org/10.1162/00208180152507597 King, Gary, and Langche Zeng. 2001. “Logistic Regression in Rare Events Data.” Political Analysis http://www.jstor.org/stable/25791637. 15.3.5 Identifiability Weschler et al. 2013. A. Bayesian Look at Nonidentifiability: A Simple Example. Am stat http://dx.doi.org/10.1080/00031305.2013.778787 15.3.6 Time Series Park, “Changepoint analysis of binary and ordinal probit models: An application to bank rate policy under the interwar gold standard” 15.3.7 Topic Models Grimmer and Stewart, “Text as data: Te promise and pitfalls of automatic content analysis methods for political texts” Quinn, Monroe, Colaresi, Crespin and Radev, “How to analyze political attention with minimal assumptions and costs” 15.3.8 Nonparametric Bayesian Methods Gill and Casella, “Nonparametric priors for ordinal Bayesian social science models” Spirling and Quinn, “Identifying intraparty voting blocs in the U.K. House of Commons” 15.3.9 Prior Elicitation Gill, J. and Walker, L. D. (2005). Elicited Priors for Bayesian Model Specifications in Political Science Research. Journal of Politics 15.3.10 Variable Selection Ghosh and Ghattas. 2015. Bayesian Variable Selection Under Collinearity. Am Stat http://dx.doi.org/10.1080/00031305.2015.1031827 15.3.11 Shrinkage Efron, B. &amp; Morris, C. 1975. “Data Analysis Using Stein’s Estimator and its Generalizations” JASA doi:10.1080/01621459.1975.10479864 https://baseballwithr.wordpress.com/2016/02/15/revisiting-efron-and-morriss-baseball-study/ 15.3.12 Applied Bayes Rule Mostly examples of naive Bayes 15.4 Computation Methods 15.4.0.1 Animations https://chi-feng.github.io/mcmc-demo/ https://mimno.infosci.cornell.edu/hmc/; http://www.mimno.org/articles/hmc/ http://twiecki.github.io/blog/2014/01/02/visualizing-mcmc/ https://ridlow.wordpress.com/category/animation/ http://people.math.aau.dk/~kkb/Undervisning/Bayes14/sorenh/docs/sampling-notes.pdf https://rpubs.com/mv2521/mcmc-animation http://blog.revolutionanalytics.com/2013/09/an-animated-peek-into-the-workings-of-bayesian-statistics.html https://people.duke.edu/~ccc14/sta-663/Animation.html https://artax.karlin.mff.cuni.cz/r-help/library/asbio/html/anm.mc.bvn.html https://groups.google.com/forum/#!topic/stan-users/nOk80xTlSyE https://www.youtube.com/watch?v=Vv3f0QNWvWQ https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/ https://www.youtube.com/watch?v=pHsuIaPbNbY&amp;list=PLqdbxUnkqOw2nKn7VxYqIrKWcqRkQYOsF&amp;index=11 http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html 15.4.0.2 Gibbs Gelfand et. al. 1986. “Illustration of Bayesian Inference in Normal Data Models Using Gibbs Sampling” doi: 10.1080/01621459.1990.10474968 Chib and Greenberg. “Understanding the Metropolis-Hastings Algorithm” doi:10.1080/00031305.1995.10476177 15.4.0.3 MCMC Casella Berger Jackman 2000 Allison and Dunkley. 2013. Comparison of sampling techniques for Bayesian parameter estimation. https://arxiv.org/pdf/1308.2675.pdf https://courses.cs.washington.edu/courses/cse577/04sp/notes/dellaertUW.pdf Geyer. MCMC: Does it work? How can we tell?http://users.stat.umn.edu/~geyer/jsm09.pdf 15.4.0.4 HMCM Neal. 2011. MCMC using Hamiltonian dynamics. https://arxiv.org/pdf/1206.1901.pdf https://www.youtube.com/watch?v=xWQpEAyI5s8&amp;index=12&amp;list=PLqdbxUnkqOw2nKn7VxYqIrKWcqRkQYOsF https://arxiv.org/pdf/1701.02434.pdf http://deeplearning.net/tutorial/hmc.html 15.4.0.5 SMC Liu and Chen. 1998. Sequential Monte Carlo Methods for Dynamic Systems. JASA 10.1080/01621459.1998.10473765 15.4.0.6 Variational Grimmer, “An Introduction to Bayesian Inference via Variational Approximations” Raganath et al. 2015. “Black Box Variational Inference” https://arxiv.org/abs/1401.0118 15.4.0.7 Expectation Propogation Gelman et. al. 2017. “Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data.” https://arxiv.org/pdf/1412.4869.pdf 15.4.0.8 Importance Resampling Smith and Gelfand. 1992. “Bayesian Statistics without Tears: A Sampling–Resampling Perspective” Am Stat 10.1080/00031305.1992.10475856. Gelfand and Smith. “Sampling-Based Approaches to Calculating Marginal Densities” JASA 10.1080/01621459.1990.10476213 Lopes, Hedibert F., Nicholas G. Polson, and Carlos M. Carvalho. “Bayesian Statistics with a Smile: A Resampling-sampling Perspective.” Brazilian Journal of Probability and Statistics http://www.jstor.org/stable/43601224. [Simulation-based approaches to modern Bayesian 15.4.0.9 Approximate Bayesian Marin, Pudlo, Robert and Ryder, “Approximate Bayesian computational methods” 15.4.0.10 Author attribution Mosteller. 1964. Inference in an Authorship Problem. JASA Arefin, A. S.; Vimieiro, R.; Riveros, C.; Craig, H. &amp; Moscato, P. Berwick, R. C. (Ed.) An Information Theoretic Clustering Approach for Unveiling Authorship Affinities in Shakespearean Era Plays and Poems PLoS ONE, Public Library of Science (PLoS), 2014. 10.1371/journal.pone.0111445. Not Bayesian per se, but has the corpus of Shakespeare and other plays. 15.4.1 Software Sofware for general purpose Bayesian computation is called probablistic programming, though the term is used in CS and not so much in stats, or social science. Stan Joseph Rickert. 2016. R Stan and Statistics BUGS modeling language. Models are specified in a different language. NIMBLE A very new BUGS-like lanugage that works with R. JAGS Gibbs/MCMC based WinBUGS Gibbs and MCMC based software. It was one of the first but is now obsolete and unmaintained. Use JAGS or Stan instead. OpenBUGS The continuation of the WinBUGS project. Also no longer well maintained. Use JAGS or Stan instead. R has multiple packages that implement some Bayesian methods. See the Bayesian Task View LearnBayes TeachBayes Python PyMC Very complete general-purpose Python package for Bayesian Analysis The various Machine learning packages like [SciKit] Edward. By David Blei. Deep generative models, variational inference. Runs on Tensorflow. Implements variational and HMC methods, as well as optimization. Church and others. Lisp-based inference programs. These are from the CS side. Church Anglican Stata: Since Stata 14 it has some Bayesian capabilities. It is mostly MH with Gibbs for a few models. Julia Mamba MCMC supporting multiple methods including Gibbs, MH, HMC, slice 15.4.2 Stan Some R packages. Official stan-dev packages: rstan rstanarm bayesplot ShinyStan loo Others: brms Bayesian generalized non-linear multilevel models using Stan ggmcmc 15.4.3 Diagrams 15.4.3.1 DAGs and Plate Notation See Plate notation tikz-bayesnet A TiKZ library for drawing Bayesian networks Daf A python package to draw DAGs Relevant Stackoverflow questions: [Software for drawing bayesian networks (graphical models)] (http://stats.stackexchange.com/questions/16750/software-for-drawing-bayesian-networks-graphical-models) Stackoverflow. Tikz Example how to draw plate indices in graphical model by tikz Stackexchange Can I have automatically adjusted plates in a graphical model? 15.4.3.2 Kruschke Diagrams Diagrams in the style of Kruschke’s Doing Bayesian Analysis LibreOffice Draw Templates: http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/ Blog posts http://doingbayesiandataanalysis.blogspot.se/2012/05/graphical-model-diagrams-in-doing.html http://doingbayesiandataanalysis.blogspot.se/2012/05/hierarchical-diagrams-read-bottom-to.html http://doingbayesiandataanalysis.blogspot.se/2013/10/diagrams-for-hierarchical-models-we.html R scripts: https://github.com/rasmusab/distribution_diagrams Tikz scripts: https://github.com/yozw/bayesdiagram 15.4.3.3 Venn Diagrams/Eikosograms Oldford and W.H. Cherry. 2006. “Picturing Probability: the poverty of Venn diagrams, the richness of Eikosograms” 15.4.4 Political Science Bayesian Works Darmofal2009a: Darmofal, D. (2009), Bayesian Spatial Survival Models for Political Event Processes. American Journal of Political Science, 53: 241–257. doi:10.1111/j.1540-5907.2008.00368.x RosasShomerHaptonstahl2014a: Rosas, G., Shomer, Y. and Haptonstahl, S. R. (2015), No News Is News: Nonignorable Nonresponse in Roll-Call Data Analysis. American Journal of Political Science, 59: 511–528. doi:10.1111/ajps.12148 Joseph Bafumi, Andrew Gelman, David K. Park, Noah Kaplan; Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation. Polit Anal 2005; 13 (2): 171-187. doi: 10.1093/pan/mpi010 Arthur Spirling; Bayesian Approaches for Limited Dependent Variable Change Point Problems. Polit Anal 2007; 15 (4): 387-405. doi: 10.1093/pan/mpm022 Kari Lock, Andrew Gelman; Bayesian Combination of State Polls and Election Forecasts. Polit Anal 2010; 18 (3): 337-348. doi: 10.1093/pan/mpq002 Jacob M. Montgomery, Brendan Nyhan; Bayesian Model Averaging: Theoretical Developments and Practical Applications. Polit Anal 2010; 18 (2): 245-270. doi: 10.1093/pan/mpq001 Kevin M. Quinn; Bayesian Factor Analysis for Mixed Ordinal and Continuous Responses. Polit Anal 2004; 12 (4): 338-353. doi: 10.1093/pan/mph022 Ryan Bakker, Keith T. Poole; Bayesian Metric Multidimensional Scaling. Polit Anal 2013; 21 (1): 125-140. doi: 10.1093/pan/mps039 Clinton Joshua D, Jackman Simon D, Rivers Douglas. The statistical analysis of roll call data: A unified approach, American Political Science Review , 2004, vol. 98 (pg. 355-70) Pope Jeremy C, Treier Shawn A. Reconsidering the great compromise at the federal convention of 1787: Deliberation and agenda effects on the senate and slavery, American Journal of Political Science , 2011, vol. 55 (pg. 289-306) Martin Andrew D, Quinn Kevin M. Dynamic ideal point estimation via Markov chain Monte Carlo for the U.S. Supreme Court, 1953–1999, Political Analysis , 2002, vol. 10 (pg. 134-53) Justin Grimmer; A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases. Polit Anal 2010; 18 (1): 1-35. doi: 10.1093/pan/mpp034 Jacob M. Montgomery, Florian M. Hollenbach, Michael D. Ward; Improving Predictions Using Ensemble Bayesian Model Averaging. Polit Anal 2012; 20 (3): 271-291. doi: 10.1093/pan/mps002 Stegmueller2013a: Stegmueller, D. (2013), How Many Countries for Multilevel Modeling? A Comparison of Frequentist and Bayesian Approaches. American Journal of Political Science, 57: 748–761. doi:10.1111/ajps.12001 HareArmstrongBakkerEtAl2014a: Hare, C., Armstrong, D. A., Bakker, R., Carroll, R. and Poole, K. T. (2015), Using Bayesian Aldrich-McKelvey Scaling to Study Citizens’ Ideological Preferences and Perceptions. American Journal of Political Science, 59: 759–774. doi:10.1111/ajps.12151 HonakerKing2010a: Honaker, J. and King, G. (2010), What to Do about Missing Values in Time-Series Cross-Section Data. American Journal of Political Science, 54: 561–581. doi:10.1111/j.1540-5907.2010.00447.x ImaiTingley2011a: Imai, K. and Tingley, D. (2012), A Statistical Method for Empirical Testing of Competing Theories. American Journal of Political Science, 56: 218–236. doi:10.1111/j.1540-5907.2011.00555.x Park2010aL Hee Park, J. (2010), Structural Change in U.S. Presidents’ Use of Force. American Journal of Political Science, 54: 766–782. doi:10.1111/j.1540-5907.2010.00459.x 10.1111/j.1540-5907.2012.00590.x Park2012a: Park, J. H. (2012), A Unified Method for Dynamic and Cross-Sectional Heterogeneity: Introducing Hidden Markov Panel Models. American Journal of Political Science, 56: 1040–1054. doi:10.1111/j.1540- WawroKatznelson2013a: Wawro, G. J. and Katznelson, I. (2014), Designing Historical Social Scientific Inquiry: How Parameter Heterogeneity Can Bridge the Methodological Divide between Quantitative and Qualitative Approaches. American Journal of Political Science, 58: 526–546. doi:10.1111/ajps.12041 Western, B., &amp; Jackman, S. (1994). Bayesian Inference for Comparative Research. American Political Science Review, 88(2), 412-423. doi:10.2307/2944713 15.5 Model Checking Gelman, Andrew, and Iain Pardoe. 2006. “Bayesian Measures of Explained Variance and Pooling in Multilevel (Hierarchical) Models.” Technometrics 48 (2). Taylor &amp; Francis: 241–51. Gelman, Andrew. A Bayesian Formulation of Exploratory Data Analysis and Goodness-of-fit Testing. Internat. Statist. Rev. 71 (2003), no. 2, 369–382. http://projecteuclid.org/euclid.isr/1069172304. Kruschke, J. K. (2011). Bayesian assessment of null values via parameter estimation and model comparison. Perspectives on Psychological Science, 6(3) 299–312. Andrew Gelman Jessica Hwang and Aki Vehtari. 2013. Understanding predictive information criteria for Bayesian models. http://www.stat.columbia.edu/~gelman/research/published/waic_understand3.pdf Vehtari, Gelman, and Gabry. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. 2016. http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf WAID. LOO package in R: https://github.com/stan-dev/loo Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely application information criterion in singular learning theory. Journal of Machine Learning Research 11, 3571-3594. Gelfand, A. E. (1996). Model determination using sampling-based methods. In Markov Chain Monte Carlo in Practice, ed. W. R. Gilks, S. Richardson, D. J. Spiegelhalter, 145-162. London: Chapman and Hall. Gelfand, A. E., Dey, D. K., and Chang, H. (1992). Model determination using predictive distributions with implementation via sampling-based methods. In Bayesian Statistics 4, ed. J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith, 147-167. Oxford University Press. Gelman, A., Hwang, J., and Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing 24, 997-1016. Vehtari and Lampinen. 2002. Bayesian model assessment and comparison using cross-validation predictive densities. https://doi.org/10.1162/08997660260293292 Vehtari and Ojanen. 2012. A survey of Bayesian predictive methods for model assessment, selection and comparison. doi:10.1214/12-SS102 Cook, Gelman, and Rubin. 2006. Validation of Software for Bayesian Models Using Posterior Quantiles. J of Comp. and Graphical Stat DOI:10.1198/106186006X136976 15.6 General Applications and Models 15.6.1 Mixed Methods and Qualitative Research Macartan Humphreys and Alan M. Jacobs, 2015, “Mixing Methods: A Bayesian Approach”, American Political Science Review 15.7 Hierarchical Modeling Kruschke and Vanpaeml “Bayesian Estimation in Hierarchical Models” http://www.indiana.edu/~kruschke/articles/KruschkeVanpaemel2015.pdf David K. Park, Andrew Gelman, Joseph Bafumi. 2004. “Bayesian Multilevel Estimation with Poststratification: State-Level Estimates from National Polls.” Polit Anal doi:10.1093/pan/mph024 Lax, Jeffrey and Justin Phillips. 2009. “How Should We Estimate Public Opinion in the States?” AJPS 15.8 Shrinkage/Regularization Piironen and Vehtari. 2016. On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior. https://arxiv.org/abs/1610.05559 Lopes. 2015. Bayesian Regularization slides. 15.8.1 Examples Monroe, B. L.; Colaresi, M. P. &amp; Quinn™, K. M. Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict Political Analysis, Cambridge University Press (CUP), 2008, https://doi.org/10.1093/pan/mpn018 Beauchamp. 2016. Predicting and Interpolating State-Level Polls Using Twitter Textual Data: Juho Piironen, Aki Vehtari. Projection predictive model selection for Gaussian processes. https://arxiv.org/abs/1510.04813 forecasting and predictiing civil war (Fearon / Laitin) Goldstone et al. 2009. “A Global Model for Forecasting Political Instability” AJPS 10.1111/j.1540-5907.2009.00426.x Ward et al. 2017/ Lessons from near real-time forecasting of irregular leadership changes. JPR http://dx.doi.org/10.1177%2F0022343316680858 Andy Berger. Coup forecasts for 2017. http://andybeger.com/2017/02/10/coup-forecasts-2017/ http://imai.princeton.edu/research/files/afghan.pdf 15.8.2 Latent Variable Models CLINTON, J., JACKMAN, S., &amp; RIVERS, D. (2004). The Statistical Analysis of Roll Call Data. American Political Science Review, 98(2), 355-370. doi:10.1017/S0003055404001194 Pope, J. C. and Treier, S. (2011), Reconsidering the Great Compromise at the Federal Convention of 1787: Deliberation and Agenda Effects on the Senate and Slavery. American Journal of Political Science, 55: 289–306. doi:10.1111/j.1540-5907.2010.00490.x Cai et al. 2016. Item Response Theory. Ann rev of Stat and Its Application DOI: 10.1146/annurev-statistics-041715-033702 15.9 Bayes Theorem Examples 15.9.1 Miscallaneous Monty Hall Problem: http://marilynvossavant.com/game-show-problem/ Examples from Kahnehman Fenton, Neil, and Berger. 2016. “Bayes and the Law” Ann Rev of Stat and Its Application DOI: 10.1146/annurev-statistics-041715-033428 Taddy. 2013. Multinomial Inverse Regression for Text Analysis. JASA http://dx.doi.org/10.1080/01621459.2012.734168 Taddy. 2015. Document Classification by Inversion of Distributed Language Representations. Laver et al. 2003. Extracting Policy Positions from Political Texts Using Words as Data. Laver, Michael, Kenneth Benoit, and John Garry. “Extracting Policy Positions from Political Texts Using Words as Data.” The American Political Science Review. http://www.jstor.org/stable/3118211. 15.9.2 German Tank Problem https://en.wikipedia.org/wiki/German_tank_problem Goodman 1954. Some Practical Techniques in Serial Number Analysis. JASA doi:10.1080/01621459.1954.10501218 Johnson. 1994. Estimating the Size of a Population. Teaching Stats DOI:10.1111/j.1467-9639.1994.tb00688.x Ruggles and Brodie. 1947. An Empirical Approach to Economic Intelligence in World War II. doi:10.1080/01621459.1947.10501915 Other applications Gill and Spirling. 2015. Estimating the Severity of the WikiLeaks U.S. Diplomatic Cables Disclosure. https://doi.org/10.1093/pan/mpv005. Political Analysis doi:10.1093/pan/mpv005 15.10 Good-Turing Estimator Mosteller. 1964. Inference in an Authorship Problem. JASA 15.11 Reproducibility Jon Zelner: Docker package of an R and Stan project https://github.com/kjhealy/lintscreen https://msalganik.wordpress.com/2015/06/09/rapid-feedback-on-code-with-lintr/ https://msalganik.wordpress.com/2015/06/07/git-and-github-in-a-data-analysis-class/ http://astrofrog.github.io/blog/2013/04/10/how-to-conduct-a-full-code-review-on-github/ http://www.princeton.edu/~mjs3/soc504_s2015/submitting_homework.shtml https://education.github.com/guide https://msalganik.wordpress.com/2015/05/04/replication-and-extension-projects-making-class-more-interesting-and-useful/ https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation https://dx.doi.org/10.1093%2Fbiomet%2F40.3-4.237 http://rstudio-pubs-static.s3.amazonaws.com/165358_78fd356d6e124331bd66981c51f7ad7c.html https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec11.pdf https://www.rdocumentation.org/packages/edgeR/versions/3.14.0/topics/goodTuring http://kochanski.org/gpk/teaching/0401Oxford/GoodTuring.pdf http://www.cs.dartmouth.edu/~lorenzo/teaching/cs134/Archive/Spring2010/milestone/20100511-134-milestone-cooley/node5.html https://simons.berkeley.edu/events/openlectures2015-spring-1 http://www.grsampson.net/D_SGT.c https://courses.engr.illinois.edu/cs498jh/Slides/Lecture03HO.pdf http://ic.epfl.ch/files/content/sites/ic/files/Inka/Orlitsky%20Talk%202016.pdf 15.11.1 Uncategorized Travelling Politician Example: https://github.com/ctufts/metropolis_hastings_example/tree/master/claydavis 15.12 Empirical Bayes Efron. 2015. Frequentist accuracy of Bayesian estimates. JRSS B https://dx.doi.org/10.1111%2Frssb.12080 15.13 Things to cover Lindley’s paradox "],
["references-9.html", "References", " References "]
]

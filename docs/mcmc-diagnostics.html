<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Updating: A Set of Bayesian Notes</title>
  <meta name="description" content="Updating: A Set of Bayesian Notes">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Updating: A Set of Bayesian Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://jrnold.github.io/bayesian_notes" />
  
  
  <meta name="github-repo" content="jrnold/bayesian_notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Updating: A Set of Bayesian Notes" />
  <meta name="twitter:site" content="@jrnld" />
  
  

<meta name="author" content="Jeffrey B. Arnold">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="bayesian-computation.html">
<link rel="next" href="model-checking.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<script src="libs/grViz-binding-1.0.0/grViz.js"></script>



<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Bayesian Notes</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>1</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-analysis"><i class="fa fa-check"></i><b>1.1</b> Bayesian Analysis</a></li>
<li class="chapter" data-level="1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>1.2</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="part"><span><b>I Theory</b></span></li>
<li class="chapter" data-level="2" data-path="bayes-theorem.html"><a href="bayes-theorem.html"><i class="fa fa-check"></i><b>2</b> Bayes Theorem</a><ul>
<li class="chapter" data-level="" data-path="bayes-theorem.html"><a href="bayes-theorem.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="2.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#introduction-to-bayes-theorem"><i class="fa fa-check"></i><b>2.1</b> Introduction to Bayes’ Theorem</a></li>
<li class="chapter" data-level="2.2" data-path="bayes-theorem.html"><a href="bayes-theorem.html#examples"><i class="fa fa-check"></i><b>2.2</b> Examples</a><ul>
<li class="chapter" data-level="2.2.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#taxi-cab-problem"><i class="fa fa-check"></i><b>2.2.1</b> Taxi-Cab Problem</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html#why-most-research-findings-are-false"><i class="fa fa-check"></i><b>2.3</b> Why most research findings are false</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#questions"><i class="fa fa-check"></i><b>2.3.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="bayes-theorem.html"><a href="bayes-theorem.html#measurement-error-and-rare-events-in-surveys"><i class="fa fa-check"></i><b>2.4</b> Measurement Error and Rare Events in Surveys</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html"><i class="fa fa-check"></i><b>3</b> Example: Predicting Names from Ages</a><ul>
<li class="chapter" data-level="" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#prerequisites-1"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="3.1" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#statement-of-the-problem"><i class="fa fa-check"></i><b>3.1</b> Statement of the problem</a></li>
<li class="chapter" data-level="3.2" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#data-wrangling"><i class="fa fa-check"></i><b>3.2</b> Data Wrangling</a></li>
<li class="chapter" data-level="3.3" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#probability-of-age-given-name-and-sex"><i class="fa fa-check"></i><b>3.3</b> Probability of age given name and sex</a><ul>
<li class="chapter" data-level="3.3.1" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#questions-1"><i class="fa fa-check"></i><b>3.3.1</b> Questions</a></li>
<li class="chapter" data-level="3.3.2" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#references"><i class="fa fa-check"></i><b>3.3.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>4</b> Naive Bayes</a><ul>
<li class="chapter" data-level="" data-path="naive-bayes.html"><a href="naive-bayes.html#prerequisites-2"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="4.1" data-path="naive-bayes.html"><a href="naive-bayes.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="naive-bayes.html"><a href="naive-bayes.html#examples-1"><i class="fa fa-check"></i><b>4.2</b> Examples</a><ul>
<li class="chapter" data-level="4.2.1" data-path="naive-bayes.html"><a href="naive-bayes.html#federalist-papers"><i class="fa fa-check"></i><b>4.2.1</b> Federalist Papers</a></li>
<li class="chapter" data-level="4.2.2" data-path="naive-bayes.html"><a href="naive-bayes.html#extensions"><i class="fa fa-check"></i><b>4.2.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="naive-bayes.html"><a href="naive-bayes.html#details"><i class="fa fa-check"></i><b>4.3</b> Details</a><ul>
<li class="chapter" data-level="4.3.1" data-path="naive-bayes.html"><a href="naive-bayes.html#generative-vs.discriminative-models"><i class="fa fa-check"></i><b>4.3.1</b> Generative vs. Discriminative Models</a></li>
<li class="chapter" data-level="4.3.2" data-path="naive-bayes.html"><a href="naive-bayes.html#estimation"><i class="fa fa-check"></i><b>4.3.2</b> Estimation</a></li>
<li class="chapter" data-level="4.3.3" data-path="naive-bayes.html"><a href="naive-bayes.html#prediction"><i class="fa fa-check"></i><b>4.3.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="naive-bayes.html"><a href="naive-bayes.html#references-1"><i class="fa fa-check"></i><b>4.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="priors.html"><a href="priors.html"><i class="fa fa-check"></i><b>5</b> Priors</a><ul>
<li class="chapter" data-level="5.1" data-path="priors.html"><a href="priors.html#levels-of-priors"><i class="fa fa-check"></i><b>5.1</b> Levels of Priors</a></li>
<li class="chapter" data-level="5.2" data-path="priors.html"><a href="priors.html#conjugate-priors"><i class="fa fa-check"></i><b>5.2</b> Conjugate Priors</a><ul>
<li class="chapter" data-level="5.2.1" data-path="priors.html"><a href="priors.html#binomial-beta"><i class="fa fa-check"></i><b>5.2.1</b> Binomial-Beta</a></li>
<li class="chapter" data-level="5.2.2" data-path="priors.html"><a href="priors.html#categorical-dirichlet"><i class="fa fa-check"></i><b>5.2.2</b> Categorical-Dirichlet</a></li>
<li class="chapter" data-level="5.2.3" data-path="priors.html"><a href="priors.html#poisson-gamma"><i class="fa fa-check"></i><b>5.2.3</b> Poisson-Gamma</a></li>
<li class="chapter" data-level="5.2.4" data-path="priors.html"><a href="priors.html#normal-with-known-variance"><i class="fa fa-check"></i><b>5.2.4</b> Normal with known variance</a></li>
<li class="chapter" data-level="5.2.5" data-path="priors.html"><a href="priors.html#exponential-family"><i class="fa fa-check"></i><b>5.2.5</b> Exponential Family</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="priors.html"><a href="priors.html#improper-priors"><i class="fa fa-check"></i><b>5.3</b> Improper Priors</a></li>
<li class="chapter" data-level="5.4" data-path="priors.html"><a href="priors.html#cromwells-rule"><i class="fa fa-check"></i><b>5.4</b> Cromwell’s Rule</a></li>
<li class="chapter" data-level="5.5" data-path="priors.html"><a href="priors.html#asymptotics"><i class="fa fa-check"></i><b>5.5</b> Asymptotics</a></li>
<li class="chapter" data-level="5.6" data-path="priors.html"><a href="priors.html#proper-and-improper-priors"><i class="fa fa-check"></i><b>5.6</b> Proper and Improper Priors</a></li>
<li class="chapter" data-level="5.7" data-path="priors.html"><a href="priors.html#hyperpriors-and-hyperparameters"><i class="fa fa-check"></i><b>5.7</b> Hyperpriors and Hyperparameters</a></li>
<li class="chapter" data-level="5.8" data-path="priors.html"><a href="priors.html#references-2"><i class="fa fa-check"></i><b>5.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>6</b> Estimation</a><ul>
<li class="chapter" data-level="6.1" data-path="estimation-1.html"><a href="estimation-1.html#point-estimates"><i class="fa fa-check"></i><b>6.1</b> Point Estimates</a></li>
<li class="chapter" data-level="6.2" data-path="estimation-1.html"><a href="estimation-1.html#credible-intervals"><i class="fa fa-check"></i><b>6.2</b> Credible Intervals</a><ul>
<li class="chapter" data-level="6.2.1" data-path="estimation-1.html"><a href="estimation-1.html#compared-to-confidence-intervals"><i class="fa fa-check"></i><b>6.2.1</b> Compared to confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimation-1.html"><a href="estimation-1.html#bayesian-decision-theory"><i class="fa fa-check"></i><b>6.3</b> Bayesian Decision Theory</a></li>
</ul></li>
<li class="part"><span><b>II Computation</b></span></li>
<li class="chapter" data-level="7" data-path="bayesian-computation.html"><a href="bayesian-computation.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#how-to-calculate-a-posterior"><i class="fa fa-check"></i><b>7.1</b> How to calculate a posterior?</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#example-globe-tossing-model"><i class="fa fa-check"></i><b>7.2</b> Example: Globe-tossing model</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#quadrature"><i class="fa fa-check"></i><b>7.3</b> Quadrature</a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#grid-approximation"><i class="fa fa-check"></i><b>7.3.1</b> Grid approximation</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian-computation.html"><a href="bayesian-computation.html#functional-approximations"><i class="fa fa-check"></i><b>7.4</b> Functional Approximations</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#maximum-a-posteriori"><i class="fa fa-check"></i><b>7.4.1</b> Maximum A Posteriori</a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#laplace-approximation"><i class="fa fa-check"></i><b>7.4.2</b> Laplace Approximation</a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#variational-inference"><i class="fa fa-check"></i><b>7.4.3</b> Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian-computation.html"><a href="bayesian-computation.html#sampling-methods"><i class="fa fa-check"></i><b>7.5</b> Sampling Methods</a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#numerical-integration"><i class="fa fa-check"></i><b>7.5.1</b> Numerical Integration</a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>7.5.2</b> Inverse transform sampling</a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#direct-approximation"><i class="fa fa-check"></i><b>7.5.3</b> Direct approximation</a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian-computation.html"><a href="bayesian-computation.html#rejection-sampling"><i class="fa fa-check"></i><b>7.5.4</b> Rejection sampling</a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian-computation.html"><a href="bayesian-computation.html#importance-sampling"><i class="fa fa-check"></i><b>7.5.5</b> Importance Sampling</a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian-computation.html"><a href="bayesian-computation.html#mcmc-methods"><i class="fa fa-check"></i><b>7.5.6</b> MCMC Methods</a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian-computation.html"><a href="bayesian-computation.html#discarding-early-iterations"><i class="fa fa-check"></i><b>7.5.7</b> Discarding early iterations</a></li>
<li class="chapter" data-level="7.5.8" data-path="bayesian-computation.html"><a href="bayesian-computation.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>7.5.8</b> Monte Carlo Sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>8</b> MCMC Diagnostics</a><ul>
<li class="chapter" data-level="" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#prerequisites-3"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="8.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#reparameterize-models"><i class="fa fa-check"></i><b>8.1</b> Reparameterize Models</a></li>
<li class="chapter" data-level="8.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#convergence-diagnostics"><i class="fa fa-check"></i><b>8.2</b> Convergence Diagnostics</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#potential-scale-reduction-hatr"><i class="fa fa-check"></i><b>8.2.1</b> Potential Scale Reduction (<span class="math inline">\(\hat{R}\)</span>)</a></li>
<li class="chapter" data-level="8.2.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#references-3"><i class="fa fa-check"></i><b>8.2.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation-effective-sample-size-and-mcse"><i class="fa fa-check"></i><b>8.3</b> Autocorrelation, Effective Sample Size, and MCSE</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>8.3.1</b> Effective Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#thinning"><i class="fa fa-check"></i><b>8.4</b> Thinning</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#traceplots"><i class="fa fa-check"></i><b>8.4.1</b> Traceplots</a></li>
<li class="chapter" data-level="8.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#monte-carlo-standard-error-mcse"><i class="fa fa-check"></i><b>8.4.2</b> Monte Carlo Standard Error (MCSE)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#hmc-nut-specific-diagnostics"><i class="fa fa-check"></i><b>8.5</b> HMC-NUT Specific Diagnostics</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#divergent-transitions"><i class="fa fa-check"></i><b>8.5.1</b> Divergent transitions</a></li>
<li class="chapter" data-level="8.5.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#maximum-tree-depth"><i class="fa fa-check"></i><b>8.5.2</b> Maximum Tree-depth</a></li>
<li class="chapter" data-level="8.5.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#bayesian-fraction-of-missing-information"><i class="fa fa-check"></i><b>8.5.3</b> Bayesian Fraction of Missing Information</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#debugging-bayesian-computing"><i class="fa fa-check"></i><b>8.6</b> Debugging Bayesian Computing</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>9</b> Model Checking</a><ul>
<li class="chapter" data-level="9.1" data-path="model-checking.html"><a href="model-checking.html#why-check-models"><i class="fa fa-check"></i><b>9.1</b> Why check models?</a></li>
<li class="chapter" data-level="9.2" data-path="model-checking.html"><a href="model-checking.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>9.2</b> Posterior Predictive Checks</a><ul>
<li class="chapter" data-level="9.2.1" data-path="model-checking.html"><a href="model-checking.html#bayesian-p-values"><i class="fa fa-check"></i><b>9.2.1</b> Bayesian p-values</a></li>
<li class="chapter" data-level="9.2.2" data-path="model-checking.html"><a href="model-checking.html#test-quantities"><i class="fa fa-check"></i><b>9.2.2</b> Test quantities</a></li>
<li class="chapter" data-level="9.2.3" data-path="model-checking.html"><a href="model-checking.html#p-values-vs.u-values"><i class="fa fa-check"></i><b>9.2.3</b> p-values vs. u-values</a></li>
<li class="chapter" data-level="9.2.4" data-path="model-checking.html"><a href="model-checking.html#marginal-predictive-checks"><i class="fa fa-check"></i><b>9.2.4</b> Marginal predictive checks</a></li>
<li class="chapter" data-level="9.2.5" data-path="model-checking.html"><a href="model-checking.html#outliers"><i class="fa fa-check"></i><b>9.2.5</b> Outliers</a></li>
<li class="chapter" data-level="9.2.6" data-path="model-checking.html"><a href="model-checking.html#graphical-posterior-predictive-checks"><i class="fa fa-check"></i><b>9.2.6</b> Graphical Posterior Predictive Checks</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="model-checking.html"><a href="model-checking.html#references-4"><i class="fa fa-check"></i><b>9.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>10</b> Model Comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="model-comparison.html"><a href="model-comparison.html#models"><i class="fa fa-check"></i><b>10.1</b> Models</a></li>
<li class="chapter" data-level="10.2" data-path="model-comparison.html"><a href="model-comparison.html#classes-of-model-spaces"><i class="fa fa-check"></i><b>10.2</b> Classes of Model Spaces</a></li>
<li class="chapter" data-level="10.3" data-path="model-comparison.html"><a href="model-comparison.html#continuous-model-expansion"><i class="fa fa-check"></i><b>10.3</b> Continuous model expansion</a></li>
<li class="chapter" data-level="10.4" data-path="model-comparison.html"><a href="model-comparison.html#discrete-model-expansion"><i class="fa fa-check"></i><b>10.4</b> Discrete Model Expansion</a></li>
<li class="chapter" data-level="10.5" data-path="model-comparison.html"><a href="model-comparison.html#out-of-sample-predictive-accuracy"><i class="fa fa-check"></i><b>10.5</b> Out-of-sample predictive accuracy</a></li>
<li class="chapter" data-level="10.6" data-path="model-comparison.html"><a href="model-comparison.html#stacking"><i class="fa fa-check"></i><b>10.6</b> Stacking</a></li>
<li class="chapter" data-level="10.7" data-path="model-comparison.html"><a href="model-comparison.html#posterior-predictive-criteria"><i class="fa fa-check"></i><b>10.7</b> Posterior Predictive Criteria</a><ul>
<li class="chapter" data-level="10.7.1" data-path="model-comparison.html"><a href="model-comparison.html#summary-and-advice"><i class="fa fa-check"></i><b>10.7.1</b> Summary and Advice</a></li>
<li class="chapter" data-level="10.7.2" data-path="model-comparison.html"><a href="model-comparison.html#expected-log-predictive-density"><i class="fa fa-check"></i><b>10.7.2</b> Expected Log Predictive Density</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="model-comparison.html"><a href="model-comparison.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>10.8</b> Bayesian Model Averaging</a></li>
<li class="chapter" data-level="10.9" data-path="model-comparison.html"><a href="model-comparison.html#pseudo-bma"><i class="fa fa-check"></i><b>10.9</b> Pseudo-BMA</a></li>
<li class="chapter" data-level="10.10" data-path="model-comparison.html"><a href="model-comparison.html#loo-cv-via-importance-sampling"><i class="fa fa-check"></i><b>10.10</b> LOO-CV via importance sampling</a></li>
<li class="chapter" data-level="10.11" data-path="model-comparison.html"><a href="model-comparison.html#selection-induced-bias"><i class="fa fa-check"></i><b>10.11</b> Selection induced Bias</a></li>
</ul></li>
<li class="part"><span><b>III Models</b></span></li>
<li class="chapter" data-level="11" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Introduction to Stan and Linear Regression</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#prerequisites-4"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="11.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#ols-and-mle-linear-regression"><i class="fa fa-check"></i><b>11.1</b> OLS and MLE Linear Regression</a><ul>
<li class="chapter" data-level="11.1.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#bayesian-model-with-improper-priors"><i class="fa fa-check"></i><b>11.1.1</b> Bayesian Model with Improper priors</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#stan-model"><i class="fa fa-check"></i><b>11.2</b> Stan Model</a></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling-model-with-stan"><i class="fa fa-check"></i><b>11.3</b> Sampling Model with Stan</a><ul>
<li class="chapter" data-level="11.3.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling"><i class="fa fa-check"></i><b>11.3.1</b> Sampling</a></li>
<li class="chapter" data-level="11.3.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#convergence-diagnostics-and-model-fit"><i class="fa fa-check"></i><b>11.3.2</b> Convergence Diagnostics and Model Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#prerequisites-5"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="12.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#introduction-1"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-models"><i class="fa fa-check"></i><b>12.2</b> Count Models</a><ul>
<li class="chapter" data-level="12.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson"><i class="fa fa-check"></i><b>12.2.1</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example-3"><i class="fa fa-check"></i><b>12.3</b> Example</a></li>
<li class="chapter" data-level="12.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#negative-binomial"><i class="fa fa-check"></i><b>12.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="12.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#multinomial-categorical-models"><i class="fa fa-check"></i><b>12.5</b> Multinomial / Categorical Models</a></li>
<li class="chapter" data-level="12.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#gamma-regression"><i class="fa fa-check"></i><b>12.6</b> Gamma Regression</a></li>
<li class="chapter" data-level="12.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#beta-regression"><i class="fa fa-check"></i><b>12.7</b> Beta Regression</a></li>
<li class="chapter" data-level="12.8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-5"><i class="fa fa-check"></i><b>12.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="binomial-models.html"><a href="binomial-models.html"><i class="fa fa-check"></i><b>13</b> Binomial Models</a><ul>
<li class="chapter" data-level="" data-path="binomial-models.html"><a href="binomial-models.html#prerequisites-6"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="13.1" data-path="binomial-models.html"><a href="binomial-models.html#introduction-2"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="binomial-models.html"><a href="binomial-models.html#link-functions-link-function"><i class="fa fa-check"></i><b>13.2</b> Link Functions {link-function}</a><ul>
<li class="chapter" data-level="13.2.1" data-path="binomial-models.html"><a href="binomial-models.html#stan"><i class="fa fa-check"></i><b>13.2.1</b> Stan</a></li>
<li class="chapter" data-level="13.2.2" data-path="binomial-models.html"><a href="binomial-models.html#example-vote-turnout"><i class="fa fa-check"></i><b>13.2.2</b> Example: Vote Turnout</a></li>
<li class="chapter" data-level="13.2.3" data-path="binomial-models.html"><a href="binomial-models.html#stan-1"><i class="fa fa-check"></i><b>13.2.3</b> Stan</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="binomial-models.html"><a href="binomial-models.html#references-6"><i class="fa fa-check"></i><b>13.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="separtion.html"><a href="separtion.html"><i class="fa fa-check"></i><b>14</b> Separation</a><ul>
<li class="chapter" data-level="" data-path="separtion.html"><a href="separtion.html#prerequisites-7"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="14.1" data-path="separtion.html"><a href="separtion.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="separtion.html"><a href="separtion.html#complete-separation"><i class="fa fa-check"></i><b>14.2</b> Complete Separation</a></li>
<li class="chapter" data-level="14.3" data-path="separtion.html"><a href="separtion.html#quasi-separation"><i class="fa fa-check"></i><b>14.3</b> Quasi-Separation</a></li>
<li class="chapter" data-level="14.4" data-path="separtion.html"><a href="separtion.html#weak-priors"><i class="fa fa-check"></i><b>14.4</b> Weak Priors</a></li>
<li class="chapter" data-level="14.5" data-path="separtion.html"><a href="separtion.html#example-support-of-aca-medicaid-expansion"><i class="fa fa-check"></i><b>14.5</b> Example: Support of ACA Medicaid Expansion</a></li>
<li class="chapter" data-level="14.6" data-path="separtion.html"><a href="separtion.html#questions-2"><i class="fa fa-check"></i><b>14.6</b> Questions</a></li>
<li class="chapter" data-level="14.7" data-path="separtion.html"><a href="separtion.html#references-7"><i class="fa fa-check"></i><b>14.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>15</b> Robust Regression</a><ul>
<li class="chapter" data-level="" data-path="robust-regression.html"><a href="robust-regression.html#prerequisites-8"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="15.1" data-path="robust-regression.html"><a href="robust-regression.html#wide-tailed-distributions"><i class="fa fa-check"></i><b>15.1</b> Wide Tailed Distributions</a></li>
<li class="chapter" data-level="15.2" data-path="robust-regression.html"><a href="robust-regression.html#student-t-distribution"><i class="fa fa-check"></i><b>15.2</b> Student-t distribution</a><ul>
<li class="chapter" data-level="15.2.1" data-path="robust-regression.html"><a href="robust-regression.html#examples-2"><i class="fa fa-check"></i><b>15.2.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="robust-regression.html"><a href="robust-regression.html#robit"><i class="fa fa-check"></i><b>15.3</b> Robit</a></li>
<li class="chapter" data-level="15.4" data-path="robust-regression.html"><a href="robust-regression.html#quantile-regression"><i class="fa fa-check"></i><b>15.4</b> Quantile regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="robust-regression.html"><a href="robust-regression.html#questions-3"><i class="fa fa-check"></i><b>15.4.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="robust-regression.html"><a href="robust-regression.html#references-8"><i class="fa fa-check"></i><b>15.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html"><i class="fa fa-check"></i><b>16</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#prerequisites-9"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="16.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#introduction-4"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#weighted-regression"><i class="fa fa-check"></i><b>16.2</b> Weighted Regression</a></li>
<li class="chapter" data-level="16.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#modeling-the-scale-with-covariates"><i class="fa fa-check"></i><b>16.3</b> Modeling the Scale with Covariates</a></li>
<li class="chapter" data-level="16.4" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#prior-distributions"><i class="fa fa-check"></i><b>16.4</b> Prior Distributions</a><ul>
<li class="chapter" data-level="16.4.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#examples-duncan"><i class="fa fa-check"></i><b>16.4.1</b> Examples: Duncan</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#exercises"><i class="fa fa-check"></i><b>16.5</b> Exercises</a></li>
<li class="chapter" data-level="16.6" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#references-9"><i class="fa fa-check"></i><b>16.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="rare-events.html"><a href="rare-events.html"><i class="fa fa-check"></i><b>17</b> Rare Events</a><ul>
<li class="chapter" data-level="" data-path="rare-events.html"><a href="rare-events.html#prerequisites-10"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="17.1" data-path="rare-events.html"><a href="rare-events.html#introduction-5"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="rare-events.html"><a href="rare-events.html#finite-sample-bias"><i class="fa fa-check"></i><b>17.2</b> Finite-Sample Bias</a></li>
<li class="chapter" data-level="17.3" data-path="rare-events.html"><a href="rare-events.html#case-control"><i class="fa fa-check"></i><b>17.3</b> Case Control</a></li>
<li class="chapter" data-level="17.4" data-path="rare-events.html"><a href="rare-events.html#questions-4"><i class="fa fa-check"></i><b>17.4</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html"><i class="fa fa-check"></i><b>18</b> Shrinkage and Hierarchical Models</a><ul>
<li class="chapter" data-level="18.1" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#hierarchical-models"><i class="fa fa-check"></i><b>18.1</b> Hierarchical Models</a></li>
<li class="chapter" data-level="18.2" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#baseball-hits"><i class="fa fa-check"></i><b>18.2</b> Baseball Hits</a><ul>
<li class="chapter" data-level="18.2.1" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#references-10"><i class="fa fa-check"></i><b>18.2.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html"><i class="fa fa-check"></i><b>19</b> Shrinkage and Regularized Regression</a><ul>
<li class="chapter" data-level="" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#prerequisites-11"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="19.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#introduction-6"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#penalized-maximum-likelihood-regression"><i class="fa fa-check"></i><b>19.2</b> Penalized Maximum Likelihood Regression</a><ul>
<li class="chapter" data-level="19.2.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#ridge-regression"><i class="fa fa-check"></i><b>19.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="19.2.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>19.2.2</b> Lasso</a></li>
<li class="chapter" data-level="19.2.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#constrained-optimization-interpretation"><i class="fa fa-check"></i><b>19.2.3</b> Constrained Optimization Interpretation</a></li>
<li class="chapter" data-level="19.2.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#bayesian-interpretation"><i class="fa fa-check"></i><b>19.2.4</b> Bayesian Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#bayesian-shrinkage"><i class="fa fa-check"></i><b>19.3</b> Bayesian Shrinkage</a><ul>
<li class="chapter" data-level="19.3.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#priors-1"><i class="fa fa-check"></i><b>19.3.1</b> Priors</a></li>
<li class="chapter" data-level="19.3.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#spike-and-slab-prior"><i class="fa fa-check"></i><b>19.3.2</b> Spike and Slab prior</a></li>
<li class="chapter" data-level="19.3.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#normal-distribution"><i class="fa fa-check"></i><b>19.3.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="19.3.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#laplace-distribution"><i class="fa fa-check"></i><b>19.3.4</b> Laplace Distribution</a></li>
<li class="chapter" data-level="19.3.5" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#student-t-and-cauchy-distributions"><i class="fa fa-check"></i><b>19.3.5</b> Student-t and Cauchy Distributions</a></li>
<li class="chapter" data-level="19.3.6" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#horseshore-prior"><i class="fa fa-check"></i><b>19.3.6</b> Horseshore Prior</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#understanding-shrinkage-models"><i class="fa fa-check"></i><b>19.4</b> Understanding Shrinkage Models</a></li>
<li class="chapter" data-level="19.5" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#choice-of-hyperparameter-on-tau"><i class="fa fa-check"></i><b>19.5</b> Choice of Hyperparameter on <span class="math inline">\(\tau\)</span></a></li>
<li class="chapter" data-level="19.6" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#differences-between-bayesian-and-penalized-ml"><i class="fa fa-check"></i><b>19.6</b> Differences between Bayesian and Penalized ML</a></li>
<li class="chapter" data-level="19.7" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#examples-3"><i class="fa fa-check"></i><b>19.7</b> Examples</a></li>
</ul></li>
<li class="part"><span><b>IV Appendix</b></span><ul>
<li class="chapter" data-level="" data-path=""><a href="#prerequisites-12"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="19.8" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#parameters"><i class="fa fa-check"></i><b>19.8</b> Parameters</a></li>
<li class="chapter" data-level="19.9" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#miscellaneous-mathematical-background"><i class="fa fa-check"></i><b>19.9</b> Miscellaneous Mathematical Background</a><ul>
<li class="chapter" data-level="19.9.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#location-scale-families"><i class="fa fa-check"></i><b>19.9.1</b> Location-Scale Families</a></li>
<li class="chapter" data-level="19.9.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#scale-mixtures-of-normal-distributions"><i class="fa fa-check"></i><b>19.9.2</b> Scale Mixtures of Normal Distributions</a></li>
<li class="chapter" data-level="19.9.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#covariance-correlation-matrix-decomposition"><i class="fa fa-check"></i><b>19.9.3</b> Covariance-Correlation Matrix Decomposition</a></li>
<li class="chapter" data-level="19.9.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#qr-factorization"><i class="fa fa-check"></i><b>19.9.4</b> QR Factorization</a></li>
<li class="chapter" data-level="19.9.5" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#cholesky-decomposition"><i class="fa fa-check"></i><b>19.9.5</b> Cholesky Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="19.10" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#scaled-and-unscaled-variables"><i class="fa fa-check"></i><b>19.10</b> Scaled and Unscaled Variables</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>20</b> Distributions</a></li>
<li class="chapter" data-level="21" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html"><i class="fa fa-check"></i><b>21</b> Annotated Bibliography</a><ul>
<li class="chapter" data-level="21.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#textbooks"><i class="fa fa-check"></i><b>21.1</b> Textbooks</a></li>
<li class="chapter" data-level="21.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#syllabi"><i class="fa fa-check"></i><b>21.2</b> Syllabi</a></li>
<li class="chapter" data-level="21.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#topics"><i class="fa fa-check"></i><b>21.3</b> Topics</a></li>
<li class="chapter" data-level="21.4" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayes-theorem-1"><i class="fa fa-check"></i><b>21.4</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="21.5" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#article-length-introductions-to-bayesian-statistics"><i class="fa fa-check"></i><b>21.5</b> Article Length Introductions to Bayesian Statistics</a><ul>
<li class="chapter" data-level="21.5.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#why-bayesian"><i class="fa fa-check"></i><b>21.5.1</b> Why Bayesian</a></li>
<li class="chapter" data-level="21.5.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#modern-statistical-workflow"><i class="fa fa-check"></i><b>21.5.2</b> Modern Statistical Workflow</a></li>
<li class="chapter" data-level="21.5.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-philosophy"><i class="fa fa-check"></i><b>21.5.3</b> Bayesian Philosophy</a></li>
<li class="chapter" data-level="21.5.4" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>21.5.4</b> Bayesian Hypothesis Testing</a></li>
<li class="chapter" data-level="21.5.5" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-frequentist-debates"><i class="fa fa-check"></i><b>21.5.5</b> Bayesian Frequentist Debates</a></li>
<li class="chapter" data-level="21.5.6" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#categorical"><i class="fa fa-check"></i><b>21.5.6</b> Categorical</a></li>
<li class="chapter" data-level="21.5.7" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#variable-selection"><i class="fa fa-check"></i><b>21.5.7</b> Variable Selection</a></li>
<li class="chapter" data-level="21.5.8" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#multiple-testing"><i class="fa fa-check"></i><b>21.5.8</b> Multiple Testing</a></li>
<li class="chapter" data-level="21.5.9" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#rare-events-1"><i class="fa fa-check"></i><b>21.5.9</b> Rare Events</a></li>
<li class="chapter" data-level="21.5.10" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#identifiability"><i class="fa fa-check"></i><b>21.5.10</b> Identifiability</a></li>
<li class="chapter" data-level="21.5.11" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#shrinkage"><i class="fa fa-check"></i><b>21.5.11</b> Shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#software"><i class="fa fa-check"></i><b>21.6</b> Software</a><ul>
<li class="chapter" data-level="21.6.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#stan-2"><i class="fa fa-check"></i><b>21.6.1</b> Stan</a></li>
<li class="chapter" data-level="21.6.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#diagrams"><i class="fa fa-check"></i><b>21.6.2</b> Diagrams</a></li>
<li class="chapter" data-level="21.6.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#priors-2"><i class="fa fa-check"></i><b>21.6.3</b> Priors</a></li>
</ul></li>
<li class="chapter" data-level="21.7" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-model-averaging-1"><i class="fa fa-check"></i><b>21.7</b> Bayesian Model Averaging</a></li>
<li class="chapter" data-level="21.8" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#multilevel-modeling"><i class="fa fa-check"></i><b>21.8</b> Multilevel Modeling</a></li>
<li class="chapter" data-level="21.9" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#mixture-models"><i class="fa fa-check"></i><b>21.9</b> Mixture Models</a></li>
<li class="chapter" data-level="21.10" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#inference"><i class="fa fa-check"></i><b>21.10</b> Inference</a><ul>
<li class="chapter" data-level="21.10.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#discussion-of-bayesian-inference"><i class="fa fa-check"></i><b>21.10.1</b> Discussion of Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="21.11" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#model-checking-1"><i class="fa fa-check"></i><b>21.11</b> Model Checking</a><ul>
<li class="chapter" data-level="21.11.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#posterior-predictive-checks-1"><i class="fa fa-check"></i><b>21.11.1</b> Posterior Predictive Checks</a></li>
<li class="chapter" data-level="21.11.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#prediction-criteria"><i class="fa fa-check"></i><b>21.11.2</b> Prediction Criteria</a></li>
<li class="chapter" data-level="21.11.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#software-validation"><i class="fa fa-check"></i><b>21.11.3</b> Software Validation</a></li>
</ul></li>
<li class="chapter" data-level="21.12" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#hierarchical-modeling"><i class="fa fa-check"></i><b>21.12</b> Hierarchical Modeling</a></li>
<li class="chapter" data-level="21.13" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#shrinkageregularization"><i class="fa fa-check"></i><b>21.13</b> Shrinkage/Regularization</a></li>
<li class="chapter" data-level="21.14" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#empirical-bayes"><i class="fa fa-check"></i><b>21.14</b> Empirical Bayes</a></li>
<li class="chapter" data-level="21.15" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#history-of-bayesian-statistics"><i class="fa fa-check"></i><b>21.15</b> History of Bayesian Statistics</a></li>
<li class="chapter" data-level="21.16" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#sampling-difficulties"><i class="fa fa-check"></i><b>21.16</b> Sampling Difficulties</a></li>
<li class="chapter" data-level="21.17" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#complicated-estimation-and-testing"><i class="fa fa-check"></i><b>21.17</b> Complicated Estimation and Testing</a></li>
<li class="chapter" data-level="21.18" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#pooling-polls"><i class="fa fa-check"></i><b>21.18</b> Pooling Polls</a></li>
<li class="chapter" data-level="21.19" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#visualizing-mcmc-methods"><i class="fa fa-check"></i><b>21.19</b> Visualizing MCMC Methods</a></li>
<li class="chapter" data-level="21.20" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-point-estimation-decision"><i class="fa fa-check"></i><b>21.20</b> Bayesian point estimation / Decision</a></li>
<li class="chapter" data-level="21.21" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#stan-modeling-language"><i class="fa fa-check"></i><b>21.21</b> Stan Modeling Language</a></li>
<li class="chapter" data-level="21.22" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayes-factors"><i class="fa fa-check"></i><b>21.22</b> Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-11.html"><a href="references-11.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Updating: A Set of Bayesian Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\median}{median}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\logistic}{Logistic}
\DeclareMathOperator{\logit}{Logit}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

% This follows BDA
\newcommand{\dunif}{\mathrm{U}}
\newcommand{\dnorm}{\mathrm{N}}
\newcommand{\dlnorm}{\mathrm{lognormal}}
\newcommand{\dmvnorm}{\mathrm{N}}
\newcommand{\dgamma}{\mathrm{Gamma}}
\newcommand{\dinvgamma}{\mathrm{Inv-Gamma}}
\newcommand{\dchisq}[1]{\chi^2_{#1}}
\newcommand{\dinvchisq}[1]{\mathrm{Inv-}\chi^2_{#1}}
\newcommand{\dexp}{\mathrm{Expon}}
\newcommand{\dlaplace}{\mathrm{Laplace}}
\newcommand{\dweibull}{\mathrm{Weibull}}
\newcommand{\dwishart}[1]{\mathrm{Wishart}_{#1}}
\newcommand{\dinvwishart}[1]{\mathrm{Inv-Wishart}_{#1}}
\newcommand{\dlkj}{\mathrm{LkjCorr}}
\newcommand{\dt}{\mathrm{Student-t}}
\newcommand{\dbeta}{\mathrm{Beta}}
\newcommand{\ddirichlet}{\mathrm{Dirichlet}}
\newcommand{\dlogistic}{\mathrm{Logistic}}
\newcommand{\dllogistic}{\mathrm{Log-logistic}}
\newcommand{\dpois}{\mathrm{Poisson}}
\newcommand{\dBinom}{\mathrm{Bin}}
\newcommand{\dBinom}{\mathrm{Bin}}
\newcommand{\dmultinom}{\mathrm{Multinom}}
\newcommand{\dnbinom}{\mathrm{Neg-bin}}
\newcommand{\dnbinomalt}{\mathrm{Neg-bin2}}
\newcommand{\dbetabinom}{\mathrm{Beta-bin}}
\newcommand{\dcauchy}{\mathrm{Cauchy}}
\newcommand{\dhalfcauchy}{\mathrm{Cauchy}^{+}}
\newcommand{\dlkjcorr}{\mathrm{LKJ}^{+}}
\newcommand{\dbernoulli}{\mathrm{Bernoulli}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Reals}{\R}
\newcommand{\RealPos}{\R^{+}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Nats}{\N}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}

\DeclareMathOperator{\invlogit}{Inv-Logit}
\DeclareMathOperator{\logit}{Logit}

\]
<div id="mcmc-diagnostics" class="section level1">
<h1><span class="header-section-number">8</span> MCMC Diagnostics</h1>
<p>There are two parts of checking a Bayesian model:</p>
<ol style="list-style-type: decimal">
<li>diagnostics: Is the sampler working? Is it adequately approximating the specified posterior distribution: <span class="math inline">\(p(\theta | D)\)</span>.</li>
<li>model fit: Does the model adequately represent the data?</li>
</ol>
<p>This chapter covers the former.</p>
<p>Also see the <strong><a href="https://cran.r-project.org/package=bayesplot">bayesplot</a></strong> vignette <a href="https://cran.r-project.org/web/packages/bayesplot/vignettes/MCMC-diagnostics.html">Visual MCMC diagnostics using the bayesplot package</a>, which though specific to the provides, provides a good overview of these diagnostics.</p>
<div id="prerequisites-3" class="section level2 unnumbered">
<h2>Prerequisites</h2>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;rstan&quot;</span>)</a>
<a class="sourceLine" id="cb64-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)</a></code></pre></div>
</div>
<div id="reparameterize-models" class="section level2">
<h2><span class="header-section-number">8.1</span> Reparameterize Models</h2>
<ol style="list-style-type: decimal">
<li>Reduce correlation between parameters (e.g. see <code>mcmc_pairs</code>)</li>
<li>Put parameters on the same scale. The samplers work best when all parameters are roughly on the same scale, e.g. <span class="math inline">\(\approx 1\)</span>. Try to avoid situations where parameters are orders of magnitude different, e.g. 1e-5 and 1e+10.</li>
<li>Increase the informativeness of priors. If parameters are too uninformative, the posterior distribution may have wide tails that hamper sampling. One way of thinking about it is that the model is only “weakly identified” and requires either more data or more informative priors to estimate.</li>
</ol>
</div>
<div id="convergence-diagnostics" class="section level2">
<h2><span class="header-section-number">8.2</span> Convergence Diagnostics</h2>
<p>Under certain conditions, MCMC algorithms will draw a sample from the target posterior distribution after it has converged to equilibrium.
However, since in practice, any sample is finite, there is no guarantee about whether its converged, or is close enough to the posterior distribution.</p>
<p>In general there is no way to prove that the sampler has converged.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>
However, there are several statistics that indicate that a sampler has not converged.</p>
<div id="potential-scale-reduction-hatr" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Potential Scale Reduction (<span class="math inline">\(\hat{R}\)</span>)</h3>
<p>In equilibrium, the distribution of samples from chains should be the same regardless of the initial starting
values of the chains <span class="citation">(Stan Development Team 2016, Sec 28.2)</span>.</p>
<p>One way to check this is to compare the distributions of multiple chains—in equilibrium they should all have the same mean.
Additionally, the split <span class="math inline">\(\hat{R}\)</span> tests for convergence by splitting the chain in half, and testing the hypothesis that the means are the same in each half. This tests for non-stationarity within a chain.</p>
<p>See <span class="citation">Stan Development Team (2016 Sec 28.2)</span> for the equations to calculate these.</p>
<p><strong>TODO:</strong> Examples of passing and non-passing <span class="math inline">\(\hat{R}\)</span> chains using fake data generated from known functions with a given autocorrelation.</p>
<p><strong>Rule of Thumb:</strong> The rule of thumb is that R-hat values for all less than 1.1 <a href="https://cran.r-project.org/web/packages/rstanarm/vignettes/rstanarm.html">source</a>.
Note that <strong>all</strong> parameters must show convergence.
This is a necessary but not sufficient condition for convergence.</p>
</div>
<div id="references-3" class="section level3">
<h3><span class="header-section-number">8.2.2</span> References</h3>
<ul>
<li><span class="citation">A. Gelman et al. (2013, 267)</span></li>
<li><span class="citation">Stan Development Team (2016 Ch 28.)</span> for how Stan calculates Hat, autocorrelations, and ESS.</li>
<li><span class="citation">Gelman and Rubin (1992)</span> introduce the R-hat statistic</li>
</ul>
</div>
</div>
<div id="autocorrelation-effective-sample-size-and-mcse" class="section level2">
<h2><span class="header-section-number">8.3</span> Autocorrelation, Effective Sample Size, and MCSE</h2>
<p>MCMC samples are dependent. This does not effect the validity of inference on the posterior if the samplers has time to explore the posterior distribution, but it does affect the efficiency of the sampler.</p>
<p>In other words, highly correlated MCMC samplers requires more samples to produce the same level of Monte Carlo error for an estimate.</p>
<div id="effective-sample-size" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Effective Sample Size</h3>
<p>The effective sample size (ESS) measures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample.
Suppose that the <span class="math inline">\(\rho^2_t\)</span> is the ACF function of a sample of size <span class="math inline">\(N\)</span>, the effective sample size, <span class="math inline">\(N_eff\)</span>, is
<span class="math display">\[
N_{eff} = \frac{N}{\sum_{t = -\infty}^\infty \rho_t} = \frac{N}{1 + 2 \sum_{t = -\infty}^\infty \rho_t}.
\]</span>
<strong>TODO</strong> show that if <span class="math inline">\(\rho_t = 1\)</span> for all <span class="math inline">\(t\)</span> then <span class="math inline">\(N_eff = 1\)</span>, and if <span class="math inline">\(\rho_t = 0\)</span> for all <span class="math inline">\(t\)</span> then <span class="math inline">\(N_eff = N\)</span></p>
<p>Computing the effective sample size requires calculating an autocorrelation.
A multi-chain estimate of the autocorrelation is found by computing the <em>variogram</em> with the correlations for all lags,
<span class="math display">\[
V_t = \frac{1}{m(n - t)} \sum_{j = 1}^m \sum_{i = t + 1}^n (\psi_{i,j} - \psi_{i - t,j})^2
\]</span>
The estimate of the autocorrelations <span class="math inline">\(\hat{\rho}_t\)</span> is
<span class="math display">\[
\hat{\rho}_t = 1 - \frac{V_t}{2 \widehat{\mathrm{var}}^+}
\]</span>
The estimates of the autocorrelations can be noisy, so <span class="math inline">\(\hat{\rho}_t\)</span> are summed from 0, to the last <span class="math inline">\(t\)</span> such that <span class="math inline">\(\rho\)</span> is positive (<span class="math inline">\(T\)</span>),
<span class="math display">\[
\hat{n}_{eff} = \frac{mn}{1 + 2 \sum_{t = 1}^T \hat{\rho}_t}
\]</span></p>
<ul>
<li><p>See also <span class="citation">Stan Development Team (2016 Sec 28.4)</span>, <span class="citation">Geyer (2011)</span>, and <span class="citation">A. Gelman et al. (2013 Sec 11.5)</span>.</p></li>
<li><p>This isn’t the only way to calculate the effective sample size. The
<strong><a href="https://cran.r-project.org/package=coda">coda</a></strong> package function <a href="https://www.rdocumentation.org/packages/coda/topics/effectiveSize">coda</a>
uses a different method. The differences are due to how the autocorrelation is calculated.</p></li>
</ul>
<p><strong>Example:</strong> Comparison of the effective sample sizes for data generated with various levels of autocorrelation.
The package <code>rstan</code> does not directly expose the function it uses to calculate ESS, so this <code>ess</code> function does so (for a single chain).</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1">ess &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb65-2" data-line-number="2">  N &lt;-<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb65-3" data-line-number="3">  V &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(<span class="kw">seq_len</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb65-4" data-line-number="4">          <span class="cf">function</span>(t) {</a>
<a class="sourceLine" id="cb65-5" data-line-number="5">             <span class="kw">mean</span>(<span class="kw">diff</span>(x, <span class="dt">lag =</span> t) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb65-6" data-line-number="6">          })</a>
<a class="sourceLine" id="cb65-7" data-line-number="7">  rho &lt;-<span class="st"> </span><span class="kw">head_while</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>V <span class="op">/</span><span class="st"> </span><span class="kw">var</span>(x), <span class="op">~</span><span class="st"> </span>. <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb65-8" data-line-number="8">  N <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(rho))</a>
<a class="sourceLine" id="cb65-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb65-10" data-line-number="10">n &lt;-<span class="st"> </span><span class="dv">1024</span></a>
<a class="sourceLine" id="cb65-11" data-line-number="11">sims &lt;-<span class="st"> </span><span class="kw">map_df</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.99</span>),</a>
<a class="sourceLine" id="cb65-12" data-line-number="12">  <span class="cf">function</span>(ar) {</a>
<a class="sourceLine" id="cb65-13" data-line-number="13">    <span class="kw">tibble</span>(<span class="dt">ar =</span> ar,</a>
<a class="sourceLine" id="cb65-14" data-line-number="14">           <span class="dt">y =</span> <span class="cf">if</span> (ar <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb65-15" data-line-number="15">             <span class="kw">rnorm</span>(n)</a>
<a class="sourceLine" id="cb65-16" data-line-number="16">           } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb65-17" data-line-number="17">             <span class="kw">as.numeric</span>(<span class="kw">arima.sim</span>(<span class="kw">list</span>(<span class="dt">ar =</span> ar), n))</a>
<a class="sourceLine" id="cb65-18" data-line-number="18">           },</a>
<a class="sourceLine" id="cb65-19" data-line-number="19">           <span class="dt">x =</span> <span class="kw">seq_along</span>(y),</a>
<a class="sourceLine" id="cb65-20" data-line-number="20">           <span class="dt">n_eff =</span> <span class="kw">ess</span>(y),</a>
<a class="sourceLine" id="cb65-21" data-line-number="21">           <span class="dt">label =</span> <span class="kw">sprintf</span>(<span class="st">&quot;AR = %.2f (n_eff = %.0f)&quot;</span>, ar, n_eff))</a>
<a class="sourceLine" id="cb65-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb65-23" data-line-number="23">)</a></code></pre></div>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="kw">ggplot</span>(sims, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb66-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb66-3" data-line-number="3"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>label, <span class="dt">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb66-4" data-line-number="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;&quot;</span>)</a></code></pre></div>
<p><img src="diagnostics_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="thinning" class="section level2">
<h2><span class="header-section-number">8.4</span> Thinning</h2>
<p>Since the autocorrelation tends to decrease as the lag increases, thinning samples will reduce the final autocorrelation in the sample while also reducing the total number of samples saved.
Due to the autocorrelation, the reduction in the number of effective samples will often be less than number of samples removed in thinning.</p>
<p>Both of these will produce 1,000 samples from the posterior, but effective sample size of <span class="math inline">\(B\)</span> will be greater than the effective sample size of <span class="math inline">\(A\)</span>, since after thinning g the autocorrelation in <span class="math inline">\(B\)</span> will be lower.</p>
<ul>
<li><em>A</em> Generating 1,000 samples after convergence and save all of them</li>
<li><em>B</em> Generating 10,000 samples after convergence and save every 10th sample</li>
</ul>
<p>In this case, A produces 10,000 samples, and B produces 1,000.
The effective sample size of A will be higher than B.
However, due to autocorrelation, the proportional reduction in the effective sample size in B will be less than the thinning: <span class="math inline">\(N_{eff}(A) / N_{eff}(B) &lt; 10\)</span>.</p>
<ul>
<li><em>A</em> Generating 10,000 samples after convergence and save all of them</li>
<li><em>B</em> Generating 10,000 samples after convergence and save every 10th sample</li>
</ul>
<p>Thinning trades off sample size for memory, and due to autocorrelation in samples, loss in effective sample size is less than the loss in sample size.</p>
<p>Thinning has become less of an issue as memory has become less of a computational constraint, and samplers have become more efficient.</p>
<p>The following example simulates random values from an autocorrelated series, and applies different levels of thinning.
Thinning is always decreasing the effective sample size.
However, the number of effective samples per sample (<code>n_eff / n</code>) increases until the thinning is large enough that the thinned samples are uncorrelated.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1">thin_ess &lt;-<span class="st"> </span><span class="cf">function</span>(thin, x) {</a>
<a class="sourceLine" id="cb67-2" data-line-number="2">  <span class="cf">if</span> (thin <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb67-3" data-line-number="3">    <span class="co"># keep only thinned rows</span></a>
<a class="sourceLine" id="cb67-4" data-line-number="4">    x_thinned &lt;-<span class="st"> </span>x[(<span class="kw">seq_len</span>(<span class="kw">length</span>(x)) <span class="op">%%</span><span class="st"> </span>thin) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb67-5" data-line-number="5">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb67-6" data-line-number="6">    x_thinned &lt;-<span class="st"> </span>x</a>
<a class="sourceLine" id="cb67-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb67-8" data-line-number="8">  <span class="kw">tibble</span>(<span class="dt">thin =</span> thin,</a>
<a class="sourceLine" id="cb67-9" data-line-number="9">         <span class="dt">n =</span> <span class="kw">length</span>(x_thinned),</a>
<a class="sourceLine" id="cb67-10" data-line-number="10">         <span class="dt">n_eff =</span> <span class="kw">ess</span>(x_thinned),</a>
<a class="sourceLine" id="cb67-11" data-line-number="11">         <span class="st">`</span><span class="dt">n_eff / n</span><span class="st">`</span> =<span class="st"> </span>n_eff <span class="op">/</span><span class="st"> </span>n)</a>
<a class="sourceLine" id="cb67-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb67-13" data-line-number="13"></a>
<a class="sourceLine" id="cb67-14" data-line-number="14"><span class="kw">map_df</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>), thin_ess,</a>
<a class="sourceLine" id="cb67-15" data-line-number="15">       <span class="dt">x =</span> <span class="kw">arima.sim</span>(<span class="kw">list</span>(<span class="dt">ar =</span> <span class="fl">.9</span>), <span class="dv">4096</span>))</a>
<a class="sourceLine" id="cb67-16" data-line-number="16"><span class="co">#&gt; # A tibble: 6 x 4</span></a>
<a class="sourceLine" id="cb67-17" data-line-number="17"><span class="co">#&gt;    thin     n n_eff `n_eff / n`</span></a>
<a class="sourceLine" id="cb67-18" data-line-number="18"><span class="co">#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;</span></a>
<a class="sourceLine" id="cb67-19" data-line-number="19"><span class="co">#&gt; 1     1  4096 1266.       0.309</span></a>
<a class="sourceLine" id="cb67-20" data-line-number="20"><span class="co">#&gt; 2     2  2048 1087.       0.531</span></a>
<a class="sourceLine" id="cb67-21" data-line-number="21"><span class="co">#&gt; 3     4  1024  791.       0.773</span></a>
<a class="sourceLine" id="cb67-22" data-line-number="22"><span class="co">#&gt; 4     8   512  512        1    </span></a>
<a class="sourceLine" id="cb67-23" data-line-number="23"><span class="co">#&gt; 5    16   256  256        1    </span></a>
<a class="sourceLine" id="cb67-24" data-line-number="24"><span class="co">#&gt; 6    32   128  128        1</span></a></code></pre></div>
<ul>
<li><span class="citation">A. Gelman et al. (2013, 282–83)</span></li>
<li><span class="citation">Stan Development Team (2016, 354–55)</span></li>
</ul>
<div id="traceplots" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Traceplots</h3>
<p>Trace plots are a time series of sampler iterations, e.g. as produced by <a href="https://www.rdocumentation.org/packages/bayesplot/topics/mcmc_trace">bayesplot</a>.</p>
<p>These can, but <strong>should not</strong>, be used to assess convergence,</p>
<ul>
<li>such visual inspection is ‘notoriously unreliable’ <span class="citation">(A. Gelman et al. 2013, 285)</span></li>
<li>it cannot scale to many parameters</li>
</ul>
<p>Trace plots may be useful for diagnosing convergence problems <em>after</em> <span class="math inline">\(\hat{R}\)</span> or or <span class="math inline">\(n_eff\)</span> indicates problems. Some possible issues to check in these plots are</p>
<ul>
<li>multimodality (the traceplot jumps between different distributions)</li>
<li>wide posterior tails (the traceplot shows regions where the sampler will reach and have difficulty returning to the main distribution)</li>
</ul>
</div>
<div id="monte-carlo-standard-error-mcse" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Monte Carlo Standard Error (MCSE)</h3>
<p>The Monte Carlo standard error is the uncertainty about a statistic in the sample due to sampling error.
With a independent sample of size <span class="math inline">\(N\)</span>, the MCSE for the sample mean is
<span class="math display">\[
MCSE(\bar{\theta}) = \frac{s}{\sqrt{N}}
\]</span>
where <span class="math inline">\(s\)</span> is the sample standard deviation.</p>
<p>However, MCMC are generally not independent, and the MCSE will be higher than that
of an independent sample. One way to calculate the MCSE with autocorrelated samples is to use the effective sample size instead of the sample size,
<span class="math display">\[
MCSE(\bar{\theta}) = \frac{s}{\sqrt{N_{eff}}}
\]</span></p>
<p>An MCSE estimator for the mean is
<span class="math display">\[
\mathrm{MCSE}(\hat{\theta}) = \frac{\sd(\theta)}{\sqrt{n_{neff}}}
\]</span>
An MCSE estimator for any posterior probability, where <span class="math inline">\(\hat{p} = \Pr(f(\theta))\)</span>, follows from the standard error of a proportion, but using the effective sample size,
<span class="math display">\[
MCSE(\hat{p}) = \sqrt{\hat{p} (1 - \hat{p}) / n_{eff}}
\]</span></p>
<p>See <span class="citation">Flegal, Haran, and Jones (2008)</span> and the <strong><a href="https://cran.r-project.org/package=mcmcse">mcmcse</a></strong> for methods to calculate MCMC standard errors for means and quantiles using sub-sampling methods.</p>
<p><span class="citation">Flegal, Haran, and Jones (2008)</span> argues for using ESS as a stopping rule and convergence diagnostic for Bayesian inference.</p>
<p>The estimation of standard errors for quantiles, as would be used in is more complicated. See the package <strong><a href="https://cran.r-project.org/package=mcmcse">mcmcse</a></strong> for Monte Carlo standard errors of quantiles (though calculated in a different method than <strong>rstan</strong>).</p>
<ul>
<li><span class="citation">A. Gelman et al. (2013 Sec. 10.5)</span></li>
<li><span class="citation">Flegal, Haran, and Jones (2008)</span></li>
<li><a href="http://www.stat.umn.edu/geyer/mcmc/talk/mcmc.pdf">Talk by Geyer on MCSE</a></li>
</ul>
</div>
</div>
<div id="hmc-nut-specific-diagnostics" class="section level2">
<h2><span class="header-section-number">8.5</span> HMC-NUT Specific Diagnostics</h2>
<p>Hamiltonian Monte Carlo (HMC), and the No-U-Turn Sampler (HMC-NUTS) in particular, produce several diagnostics that indicate that the sampler is breaking and, thus, not sampling from the posterior distribution.
This is unusual, as most Bayesian sampling methods do not give indication of whether they are working well, and all that can be checked are the properties of the samples themselves with methods such <span class="math inline">\(\hat{R}\)</span>.</p>
<p>Three specific HMC-NUTS diagnostics are</p>
<ol style="list-style-type: decimal">
<li>divergent transitions</li>
<li>maximum tree-depth</li>
<li>Bayesian fraction of missing information</li>
</ol>
<p>The general way to fix these issues is the manually adjust the HMC-NUTS sampler parameters.n</p>
<ol style="list-style-type: decimal">
<li>Stepsize: Length of the steps to take</li>
<li>Tree-depth: Number of steps to take</li>
</ol>
<p>During the warmup period, Stan tunes these values, however these auto-tuned parameters may not always be optimal.
The other alternative is to reparameterize the models.</p>
<div id="divergent-transitions" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Divergent transitions</h3>
<p>The details of the HMC are technical and can be found <strong>TODO</strong>. The gist of the problem is that Stan is using a discrete approximation of a continuous function when integrating.
If the step sizes are too large, the discrete approximation does not work.
Helpfully, when the approximation is poor it does not fail without any indication but will produce “divergent transitions”.</p>
<p>If there are too many divergent transitions, then the sampler is not drawing samples from the entire posterior and inferences will be biased.</p>
<p>Reduce the step size. This can be done by increasing the the <code>adapt_delta</code> parameter.
This is the target average proposal acceptance probability in the adaptation, which is used to determine the step size during warmup.
A higher desired acceptance probability (closer to 1) reduces the the step size. A smaller step size means that it will require more steps to explore the posterior distribution.</p>
<p>See <span class="citation">Stan Development Team (2016, 380)</span></p>
</div>
<div id="maximum-tree-depth" class="section level3">
<h3><span class="header-section-number">8.5.2</span> Maximum Tree-depth</h3>
<p>NUTS is an intelligent method to select the number of steps to take in each iteration. However, there is still a maximum number of steps that NUTS will try.
If the sampler is often hitting the maximum number of steps, it means that the optimal number of steps to take in each iteration is higher than the maximum.
While divergent transitions bias inference, a too-small maximum tree-depth only affects efficiency.
The sampler is still exploring the posterior distribution, but the exploration will be slower and the autocorrelation higher (effective sample size lower) than if the maximum tree-depth were set higher.</p>
<p>Increase the maximum tree-depth.</p>
</div>
<div id="bayesian-fraction-of-missing-information" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Bayesian Fraction of Missing Information</h3>
<p>This is rather technical. See <span class="citation">Betancourt (2016)</span>.</p>
</div>
</div>
<div id="debugging-bayesian-computing" class="section level2">
<h2><span class="header-section-number">8.6</span> Debugging Bayesian Computing</h2>
<p>See <span class="citation">A. Gelman et al. (2013, 270)</span>, …</p>
<ol style="list-style-type: decimal">
<li>Pick a reasonable value for the “true” parameter vector <span class="math inline">\(\theta^* \sim p(\theta)\)</span>. This should be a random draw from the prior, but if <span class="math inline">\(\theta\)</span> is uninformative, then any reasonable value of <span class="math inline">\(\theta\)</span> should work.</li>
<li>Draw all other parameters that are conditional on <span class="math inline">\(\theta^*\)</span>.</li>
<li>Simulate a large fake dataset <span class="math inline">\(y^{fake}\)</span> from the data distribution <span class="math inline">\(p(y | \theta^*)\)</span></li>
<li>Calculate the posterior <span class="math inline">\(p(\theta | y^{rep})\)</span>.</li>
<li>Compare the posterior distribution <span class="math inline">\(p(\theta | y)\)</span> to its true value <span class="math inline">\(\theta^*\)</span>. For example there should be a 50% probability that the 50% posterior interval of <span class="math inline">\(p(\theta | y)\)</span> contains <span class="math inline">\(\theta^*\)</span>.</li>
</ol>
<p>Notes</p>
<ul>
<li><p>for large number of parameters: calculate the proportion of 50% credible intervals that contain the true value.</p></li>
<li><p>residual plot: for all scalar parameters <span class="math inline">\(\theta_j\)</span>, calculate the residual <span class="math inline">\(\E[\theta_j] - \theta^*\)</span>. The residuals should have mean 0.</p></li>
<li><p>for models with few parameters, perform many fake data simulations.</p></li>
<li><p>convergence and posterior predictive tests may also indicate issues with the model.</p>
<ul>
<li>it could be either a problem with the model, or a problem in computation.</li>
<li>simplify the model by removing parameters or setting them to fixed values</li>
</ul></li>
<li><p>Start with a simple model and build complexity. This is useful both because the simple model may be “good enough” for your purposes, and because adding one part at a time makes it easier to catch and fix bugs.</p></li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>This is also the case in optimization with non-convex objective functions.<a href="mcmc-diagnostics.html#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-computation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-checking.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/bayesian_notes/edit/master/diagnostics.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Updating: A Set of Bayesian Notes</title>
  <meta name="description" content="Updating: A Set of Bayesian Notes">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Updating: A Set of Bayesian Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://jrnold.github.io/bayesian_notes" />
  
  
  <meta name="github-repo" content="jrnold/bayesian_notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Updating: A Set of Bayesian Notes" />
  <meta name="twitter:site" content="@jrnld" />
  
  

<meta name="author" content="Jeffrey B. Arnold">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="shrinkage-and-hierarchical-models.html">
<link rel="next" href="distributions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<script src="libs/grViz-binding-1.0.0/grViz.js"></script>



<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Bayesian Notes</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>1</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-analysis"><i class="fa fa-check"></i><b>1.1</b> Bayesian Analysis</a></li>
<li class="chapter" data-level="1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>1.2</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="part"><span><b>I Theory</b></span></li>
<li class="chapter" data-level="2" data-path="bayes-theorem.html"><a href="bayes-theorem.html"><i class="fa fa-check"></i><b>2</b> Bayes Theorem</a><ul>
<li class="chapter" data-level="" data-path="bayes-theorem.html"><a href="bayes-theorem.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="2.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#introduction-to-bayes-theorem"><i class="fa fa-check"></i><b>2.1</b> Introduction to Bayes’ Theorem</a></li>
<li class="chapter" data-level="2.2" data-path="bayes-theorem.html"><a href="bayes-theorem.html#examples"><i class="fa fa-check"></i><b>2.2</b> Examples</a><ul>
<li class="chapter" data-level="2.2.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#taxi-cab-problem"><i class="fa fa-check"></i><b>2.2.1</b> Taxi-Cab Problem</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html#why-most-research-findings-are-false"><i class="fa fa-check"></i><b>2.3</b> Why most research findings are false</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#questions"><i class="fa fa-check"></i><b>2.3.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="bayes-theorem.html"><a href="bayes-theorem.html#measurement-error-and-rare-events-in-surveys"><i class="fa fa-check"></i><b>2.4</b> Measurement Error and Rare Events in Surveys</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html"><i class="fa fa-check"></i><b>3</b> Example: Predicting Names from Ages</a><ul>
<li class="chapter" data-level="" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#prerequisites-1"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="3.1" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#statement-of-the-problem"><i class="fa fa-check"></i><b>3.1</b> Statement of the problem</a></li>
<li class="chapter" data-level="3.2" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#data-wrangling"><i class="fa fa-check"></i><b>3.2</b> Data Wrangling</a></li>
<li class="chapter" data-level="3.3" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#probability-of-age-given-name-and-sex"><i class="fa fa-check"></i><b>3.3</b> Probability of age given name and sex</a><ul>
<li class="chapter" data-level="3.3.1" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#questions-1"><i class="fa fa-check"></i><b>3.3.1</b> Questions</a></li>
<li class="chapter" data-level="3.3.2" data-path="example-predicting-names-from-ages.html"><a href="example-predicting-names-from-ages.html#references"><i class="fa fa-check"></i><b>3.3.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>4</b> Naive Bayes</a><ul>
<li class="chapter" data-level="" data-path="naive-bayes.html"><a href="naive-bayes.html#prerequisites-2"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="4.1" data-path="naive-bayes.html"><a href="naive-bayes.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="naive-bayes.html"><a href="naive-bayes.html#examples-1"><i class="fa fa-check"></i><b>4.2</b> Examples</a><ul>
<li class="chapter" data-level="4.2.1" data-path="naive-bayes.html"><a href="naive-bayes.html#federalist-papers"><i class="fa fa-check"></i><b>4.2.1</b> Federalist Papers</a></li>
<li class="chapter" data-level="4.2.2" data-path="naive-bayes.html"><a href="naive-bayes.html#extensions"><i class="fa fa-check"></i><b>4.2.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="naive-bayes.html"><a href="naive-bayes.html#details"><i class="fa fa-check"></i><b>4.3</b> Details</a><ul>
<li class="chapter" data-level="4.3.1" data-path="naive-bayes.html"><a href="naive-bayes.html#generative-vs.discriminative-models"><i class="fa fa-check"></i><b>4.3.1</b> Generative vs. Discriminative Models</a></li>
<li class="chapter" data-level="4.3.2" data-path="naive-bayes.html"><a href="naive-bayes.html#estimation"><i class="fa fa-check"></i><b>4.3.2</b> Estimation</a></li>
<li class="chapter" data-level="4.3.3" data-path="naive-bayes.html"><a href="naive-bayes.html#prediction"><i class="fa fa-check"></i><b>4.3.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="naive-bayes.html"><a href="naive-bayes.html#references-1"><i class="fa fa-check"></i><b>4.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="priors.html"><a href="priors.html"><i class="fa fa-check"></i><b>5</b> Priors</a><ul>
<li class="chapter" data-level="5.1" data-path="priors.html"><a href="priors.html#levels-of-priors"><i class="fa fa-check"></i><b>5.1</b> Levels of Priors</a></li>
<li class="chapter" data-level="5.2" data-path="priors.html"><a href="priors.html#conjugate-priors"><i class="fa fa-check"></i><b>5.2</b> Conjugate Priors</a><ul>
<li class="chapter" data-level="5.2.1" data-path="priors.html"><a href="priors.html#binomial-beta"><i class="fa fa-check"></i><b>5.2.1</b> Binomial-Beta</a></li>
<li class="chapter" data-level="5.2.2" data-path="priors.html"><a href="priors.html#categorical-dirichlet"><i class="fa fa-check"></i><b>5.2.2</b> Categorical-Dirichlet</a></li>
<li class="chapter" data-level="5.2.3" data-path="priors.html"><a href="priors.html#poisson-gamma"><i class="fa fa-check"></i><b>5.2.3</b> Poisson-Gamma</a></li>
<li class="chapter" data-level="5.2.4" data-path="priors.html"><a href="priors.html#normal-with-known-variance"><i class="fa fa-check"></i><b>5.2.4</b> Normal with known variance</a></li>
<li class="chapter" data-level="5.2.5" data-path="priors.html"><a href="priors.html#exponential-family"><i class="fa fa-check"></i><b>5.2.5</b> Exponential Family</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="priors.html"><a href="priors.html#improper-priors"><i class="fa fa-check"></i><b>5.3</b> Improper Priors</a></li>
<li class="chapter" data-level="5.4" data-path="priors.html"><a href="priors.html#cromwells-rule"><i class="fa fa-check"></i><b>5.4</b> Cromwell’s Rule</a></li>
<li class="chapter" data-level="5.5" data-path="priors.html"><a href="priors.html#asymptotics"><i class="fa fa-check"></i><b>5.5</b> Asymptotics</a></li>
<li class="chapter" data-level="5.6" data-path="priors.html"><a href="priors.html#proper-and-improper-priors"><i class="fa fa-check"></i><b>5.6</b> Proper and Improper Priors</a></li>
<li class="chapter" data-level="5.7" data-path="priors.html"><a href="priors.html#hyperpriors-and-hyperparameters"><i class="fa fa-check"></i><b>5.7</b> Hyperpriors and Hyperparameters</a></li>
<li class="chapter" data-level="5.8" data-path="priors.html"><a href="priors.html#references-2"><i class="fa fa-check"></i><b>5.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>6</b> Estimation</a><ul>
<li class="chapter" data-level="6.1" data-path="estimation-1.html"><a href="estimation-1.html#point-estimates"><i class="fa fa-check"></i><b>6.1</b> Point Estimates</a></li>
<li class="chapter" data-level="6.2" data-path="estimation-1.html"><a href="estimation-1.html#credible-intervals"><i class="fa fa-check"></i><b>6.2</b> Credible Intervals</a><ul>
<li class="chapter" data-level="6.2.1" data-path="estimation-1.html"><a href="estimation-1.html#compared-to-confidence-intervals"><i class="fa fa-check"></i><b>6.2.1</b> Compared to confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimation-1.html"><a href="estimation-1.html#bayesian-decision-theory"><i class="fa fa-check"></i><b>6.3</b> Bayesian Decision Theory</a></li>
</ul></li>
<li class="part"><span><b>II Computation</b></span></li>
<li class="chapter" data-level="7" data-path="bayesian-computation.html"><a href="bayesian-computation.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#how-to-calculate-a-posterior"><i class="fa fa-check"></i><b>7.1</b> How to calculate a posterior?</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#example-globe-tossing-model"><i class="fa fa-check"></i><b>7.2</b> Example: Globe-tossing model</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#quadrature"><i class="fa fa-check"></i><b>7.3</b> Quadrature</a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#grid-approximation"><i class="fa fa-check"></i><b>7.3.1</b> Grid approximation</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian-computation.html"><a href="bayesian-computation.html#functional-approximations"><i class="fa fa-check"></i><b>7.4</b> Functional Approximations</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#maximum-a-posteriori"><i class="fa fa-check"></i><b>7.4.1</b> Maximum A Posteriori</a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#laplace-approximation"><i class="fa fa-check"></i><b>7.4.2</b> Laplace Approximation</a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#variational-inference"><i class="fa fa-check"></i><b>7.4.3</b> Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian-computation.html"><a href="bayesian-computation.html#sampling-methods"><i class="fa fa-check"></i><b>7.5</b> Sampling Methods</a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#numerical-integration"><i class="fa fa-check"></i><b>7.5.1</b> Numerical Integration</a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>7.5.2</b> Inverse transform sampling</a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#direct-approximation"><i class="fa fa-check"></i><b>7.5.3</b> Direct approximation</a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian-computation.html"><a href="bayesian-computation.html#rejection-sampling"><i class="fa fa-check"></i><b>7.5.4</b> Rejection sampling</a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian-computation.html"><a href="bayesian-computation.html#importance-sampling"><i class="fa fa-check"></i><b>7.5.5</b> Importance Sampling</a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian-computation.html"><a href="bayesian-computation.html#mcmc-methods"><i class="fa fa-check"></i><b>7.5.6</b> MCMC Methods</a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian-computation.html"><a href="bayesian-computation.html#discarding-early-iterations"><i class="fa fa-check"></i><b>7.5.7</b> Discarding early iterations</a></li>
<li class="chapter" data-level="7.5.8" data-path="bayesian-computation.html"><a href="bayesian-computation.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>7.5.8</b> Monte Carlo Sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>8</b> MCMC Diagnostics</a><ul>
<li class="chapter" data-level="" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#prerequisites-3"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="8.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#reparameterize-models"><i class="fa fa-check"></i><b>8.1</b> Reparameterize Models</a></li>
<li class="chapter" data-level="8.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#convergence-diagnostics"><i class="fa fa-check"></i><b>8.2</b> Convergence Diagnostics</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#potential-scale-reduction-hatr"><i class="fa fa-check"></i><b>8.2.1</b> Potential Scale Reduction (<span class="math inline">\(\hat{R}\)</span>)</a></li>
<li class="chapter" data-level="8.2.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#references-3"><i class="fa fa-check"></i><b>8.2.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation-effective-sample-size-and-mcse"><i class="fa fa-check"></i><b>8.3</b> Autocorrelation, Effective Sample Size, and MCSE</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>8.3.1</b> Effective Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#thinning"><i class="fa fa-check"></i><b>8.4</b> Thinning</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#traceplots"><i class="fa fa-check"></i><b>8.4.1</b> Traceplots</a></li>
<li class="chapter" data-level="8.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#monte-carlo-standard-error-mcse"><i class="fa fa-check"></i><b>8.4.2</b> Monte Carlo Standard Error (MCSE)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#hmc-nut-specific-diagnostics"><i class="fa fa-check"></i><b>8.5</b> HMC-NUT Specific Diagnostics</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#divergent-transitions"><i class="fa fa-check"></i><b>8.5.1</b> Divergent transitions</a></li>
<li class="chapter" data-level="8.5.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#maximum-tree-depth"><i class="fa fa-check"></i><b>8.5.2</b> Maximum Tree-depth</a></li>
<li class="chapter" data-level="8.5.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#bayesian-fraction-of-missing-information"><i class="fa fa-check"></i><b>8.5.3</b> Bayesian Fraction of Missing Information</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#debugging-bayesian-computing"><i class="fa fa-check"></i><b>8.6</b> Debugging Bayesian Computing</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>9</b> Model Checking</a><ul>
<li class="chapter" data-level="9.1" data-path="model-checking.html"><a href="model-checking.html#why-check-models"><i class="fa fa-check"></i><b>9.1</b> Why check models?</a></li>
<li class="chapter" data-level="9.2" data-path="model-checking.html"><a href="model-checking.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>9.2</b> Posterior Predictive Checks</a><ul>
<li class="chapter" data-level="9.2.1" data-path="model-checking.html"><a href="model-checking.html#bayesian-p-values"><i class="fa fa-check"></i><b>9.2.1</b> Bayesian p-values</a></li>
<li class="chapter" data-level="9.2.2" data-path="model-checking.html"><a href="model-checking.html#test-quantities"><i class="fa fa-check"></i><b>9.2.2</b> Test quantities</a></li>
<li class="chapter" data-level="9.2.3" data-path="model-checking.html"><a href="model-checking.html#p-values-vs.u-values"><i class="fa fa-check"></i><b>9.2.3</b> p-values vs. u-values</a></li>
<li class="chapter" data-level="9.2.4" data-path="model-checking.html"><a href="model-checking.html#marginal-predictive-checks"><i class="fa fa-check"></i><b>9.2.4</b> Marginal predictive checks</a></li>
<li class="chapter" data-level="9.2.5" data-path="model-checking.html"><a href="model-checking.html#outliers"><i class="fa fa-check"></i><b>9.2.5</b> Outliers</a></li>
<li class="chapter" data-level="9.2.6" data-path="model-checking.html"><a href="model-checking.html#graphical-posterior-predictive-checks"><i class="fa fa-check"></i><b>9.2.6</b> Graphical Posterior Predictive Checks</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="model-checking.html"><a href="model-checking.html#references-4"><i class="fa fa-check"></i><b>9.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>10</b> Model Comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="model-comparison.html"><a href="model-comparison.html#models"><i class="fa fa-check"></i><b>10.1</b> Models</a></li>
<li class="chapter" data-level="10.2" data-path="model-comparison.html"><a href="model-comparison.html#classes-of-model-spaces"><i class="fa fa-check"></i><b>10.2</b> Classes of Model Spaces</a></li>
<li class="chapter" data-level="10.3" data-path="model-comparison.html"><a href="model-comparison.html#continuous-model-expansion"><i class="fa fa-check"></i><b>10.3</b> Continuous model expansion</a></li>
<li class="chapter" data-level="10.4" data-path="model-comparison.html"><a href="model-comparison.html#discrete-model-expansion"><i class="fa fa-check"></i><b>10.4</b> Discrete Model Expansion</a></li>
<li class="chapter" data-level="10.5" data-path="model-comparison.html"><a href="model-comparison.html#out-of-sample-predictive-accuracy"><i class="fa fa-check"></i><b>10.5</b> Out-of-sample predictive accuracy</a></li>
<li class="chapter" data-level="10.6" data-path="model-comparison.html"><a href="model-comparison.html#stacking"><i class="fa fa-check"></i><b>10.6</b> Stacking</a></li>
<li class="chapter" data-level="10.7" data-path="model-comparison.html"><a href="model-comparison.html#posterior-predictive-criteria"><i class="fa fa-check"></i><b>10.7</b> Posterior Predictive Criteria</a><ul>
<li class="chapter" data-level="10.7.1" data-path="model-comparison.html"><a href="model-comparison.html#summary-and-advice"><i class="fa fa-check"></i><b>10.7.1</b> Summary and Advice</a></li>
<li class="chapter" data-level="10.7.2" data-path="model-comparison.html"><a href="model-comparison.html#expected-log-predictive-density"><i class="fa fa-check"></i><b>10.7.2</b> Expected Log Predictive Density</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="model-comparison.html"><a href="model-comparison.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>10.8</b> Bayesian Model Averaging</a></li>
<li class="chapter" data-level="10.9" data-path="model-comparison.html"><a href="model-comparison.html#pseudo-bma"><i class="fa fa-check"></i><b>10.9</b> Pseudo-BMA</a></li>
<li class="chapter" data-level="10.10" data-path="model-comparison.html"><a href="model-comparison.html#loo-cv-via-importance-sampling"><i class="fa fa-check"></i><b>10.10</b> LOO-CV via importance sampling</a></li>
<li class="chapter" data-level="10.11" data-path="model-comparison.html"><a href="model-comparison.html#selection-induced-bias"><i class="fa fa-check"></i><b>10.11</b> Selection induced Bias</a></li>
</ul></li>
<li class="part"><span><b>III Models</b></span></li>
<li class="chapter" data-level="11" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Introduction to Stan and Linear Regression</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#prerequisites-4"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="11.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#ols-and-mle-linear-regression"><i class="fa fa-check"></i><b>11.1</b> OLS and MLE Linear Regression</a><ul>
<li class="chapter" data-level="11.1.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#bayesian-model-with-improper-priors"><i class="fa fa-check"></i><b>11.1.1</b> Bayesian Model with Improper priors</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#stan-model"><i class="fa fa-check"></i><b>11.2</b> Stan Model</a></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling-model-with-stan"><i class="fa fa-check"></i><b>11.3</b> Sampling Model with Stan</a><ul>
<li class="chapter" data-level="11.3.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling"><i class="fa fa-check"></i><b>11.3.1</b> Sampling</a></li>
<li class="chapter" data-level="11.3.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#convergence-diagnostics-and-model-fit"><i class="fa fa-check"></i><b>11.3.2</b> Convergence Diagnostics and Model Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#prerequisites-5"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="12.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#introduction-1"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-models"><i class="fa fa-check"></i><b>12.2</b> Count Models</a><ul>
<li class="chapter" data-level="12.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson"><i class="fa fa-check"></i><b>12.2.1</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example-3"><i class="fa fa-check"></i><b>12.3</b> Example</a></li>
<li class="chapter" data-level="12.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#negative-binomial"><i class="fa fa-check"></i><b>12.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="12.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#multinomial-categorical-models"><i class="fa fa-check"></i><b>12.5</b> Multinomial / Categorical Models</a></li>
<li class="chapter" data-level="12.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#gamma-regression"><i class="fa fa-check"></i><b>12.6</b> Gamma Regression</a></li>
<li class="chapter" data-level="12.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#beta-regression"><i class="fa fa-check"></i><b>12.7</b> Beta Regression</a></li>
<li class="chapter" data-level="12.8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-5"><i class="fa fa-check"></i><b>12.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="binomial-models.html"><a href="binomial-models.html"><i class="fa fa-check"></i><b>13</b> Binomial Models</a><ul>
<li class="chapter" data-level="" data-path="binomial-models.html"><a href="binomial-models.html#prerequisites-6"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="13.1" data-path="binomial-models.html"><a href="binomial-models.html#introduction-2"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="binomial-models.html"><a href="binomial-models.html#link-functions-link-function"><i class="fa fa-check"></i><b>13.2</b> Link Functions {link-function}</a><ul>
<li class="chapter" data-level="13.2.1" data-path="binomial-models.html"><a href="binomial-models.html#stan"><i class="fa fa-check"></i><b>13.2.1</b> Stan</a></li>
<li class="chapter" data-level="13.2.2" data-path="binomial-models.html"><a href="binomial-models.html#example-vote-turnout"><i class="fa fa-check"></i><b>13.2.2</b> Example: Vote Turnout</a></li>
<li class="chapter" data-level="13.2.3" data-path="binomial-models.html"><a href="binomial-models.html#stan-1"><i class="fa fa-check"></i><b>13.2.3</b> Stan</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="binomial-models.html"><a href="binomial-models.html#references-6"><i class="fa fa-check"></i><b>13.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="separtion.html"><a href="separtion.html"><i class="fa fa-check"></i><b>14</b> Separation</a><ul>
<li class="chapter" data-level="" data-path="separtion.html"><a href="separtion.html#prerequisites-7"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="14.1" data-path="separtion.html"><a href="separtion.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="separtion.html"><a href="separtion.html#complete-separation"><i class="fa fa-check"></i><b>14.2</b> Complete Separation</a></li>
<li class="chapter" data-level="14.3" data-path="separtion.html"><a href="separtion.html#quasi-separation"><i class="fa fa-check"></i><b>14.3</b> Quasi-Separation</a></li>
<li class="chapter" data-level="14.4" data-path="separtion.html"><a href="separtion.html#weak-priors"><i class="fa fa-check"></i><b>14.4</b> Weak Priors</a></li>
<li class="chapter" data-level="14.5" data-path="separtion.html"><a href="separtion.html#example-support-of-aca-medicaid-expansion"><i class="fa fa-check"></i><b>14.5</b> Example: Support of ACA Medicaid Expansion</a></li>
<li class="chapter" data-level="14.6" data-path="separtion.html"><a href="separtion.html#questions-2"><i class="fa fa-check"></i><b>14.6</b> Questions</a></li>
<li class="chapter" data-level="14.7" data-path="separtion.html"><a href="separtion.html#references-7"><i class="fa fa-check"></i><b>14.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>15</b> Robust Regression</a><ul>
<li class="chapter" data-level="" data-path="robust-regression.html"><a href="robust-regression.html#prerequisites-8"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="15.1" data-path="robust-regression.html"><a href="robust-regression.html#wide-tailed-distributions"><i class="fa fa-check"></i><b>15.1</b> Wide Tailed Distributions</a></li>
<li class="chapter" data-level="15.2" data-path="robust-regression.html"><a href="robust-regression.html#student-t-distribution"><i class="fa fa-check"></i><b>15.2</b> Student-t distribution</a><ul>
<li class="chapter" data-level="15.2.1" data-path="robust-regression.html"><a href="robust-regression.html#examples-2"><i class="fa fa-check"></i><b>15.2.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="robust-regression.html"><a href="robust-regression.html#robit"><i class="fa fa-check"></i><b>15.3</b> Robit</a></li>
<li class="chapter" data-level="15.4" data-path="robust-regression.html"><a href="robust-regression.html#quantile-regression"><i class="fa fa-check"></i><b>15.4</b> Quantile regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="robust-regression.html"><a href="robust-regression.html#questions-3"><i class="fa fa-check"></i><b>15.4.1</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="robust-regression.html"><a href="robust-regression.html#references-8"><i class="fa fa-check"></i><b>15.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html"><i class="fa fa-check"></i><b>16</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#prerequisites-9"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="16.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#introduction-4"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#weighted-regression"><i class="fa fa-check"></i><b>16.2</b> Weighted Regression</a></li>
<li class="chapter" data-level="16.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#modeling-the-scale-with-covariates"><i class="fa fa-check"></i><b>16.3</b> Modeling the Scale with Covariates</a></li>
<li class="chapter" data-level="16.4" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#prior-distributions"><i class="fa fa-check"></i><b>16.4</b> Prior Distributions</a><ul>
<li class="chapter" data-level="16.4.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#examples-duncan"><i class="fa fa-check"></i><b>16.4.1</b> Examples: Duncan</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#exercises"><i class="fa fa-check"></i><b>16.5</b> Exercises</a></li>
<li class="chapter" data-level="16.6" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#references-9"><i class="fa fa-check"></i><b>16.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="rare-events.html"><a href="rare-events.html"><i class="fa fa-check"></i><b>17</b> Rare Events</a><ul>
<li class="chapter" data-level="" data-path="rare-events.html"><a href="rare-events.html#prerequisites-10"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="17.1" data-path="rare-events.html"><a href="rare-events.html#introduction-5"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="rare-events.html"><a href="rare-events.html#finite-sample-bias"><i class="fa fa-check"></i><b>17.2</b> Finite-Sample Bias</a></li>
<li class="chapter" data-level="17.3" data-path="rare-events.html"><a href="rare-events.html#case-control"><i class="fa fa-check"></i><b>17.3</b> Case Control</a></li>
<li class="chapter" data-level="17.4" data-path="rare-events.html"><a href="rare-events.html#questions-4"><i class="fa fa-check"></i><b>17.4</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html"><i class="fa fa-check"></i><b>18</b> Shrinkage and Hierarchical Models</a><ul>
<li class="chapter" data-level="18.1" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#hierarchical-models"><i class="fa fa-check"></i><b>18.1</b> Hierarchical Models</a></li>
<li class="chapter" data-level="18.2" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#baseball-hits"><i class="fa fa-check"></i><b>18.2</b> Baseball Hits</a><ul>
<li class="chapter" data-level="18.2.1" data-path="shrinkage-and-hierarchical-models.html"><a href="shrinkage-and-hierarchical-models.html#references-10"><i class="fa fa-check"></i><b>18.2.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html"><i class="fa fa-check"></i><b>19</b> Shrinkage and Regularized Regression</a><ul>
<li class="chapter" data-level="" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#prerequisites-11"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="19.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#introduction-6"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#penalized-maximum-likelihood-regression"><i class="fa fa-check"></i><b>19.2</b> Penalized Maximum Likelihood Regression</a><ul>
<li class="chapter" data-level="19.2.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#ridge-regression"><i class="fa fa-check"></i><b>19.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="19.2.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>19.2.2</b> Lasso</a></li>
<li class="chapter" data-level="19.2.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#constrained-optimization-interpretation"><i class="fa fa-check"></i><b>19.2.3</b> Constrained Optimization Interpretation</a></li>
<li class="chapter" data-level="19.2.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#bayesian-interpretation"><i class="fa fa-check"></i><b>19.2.4</b> Bayesian Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#bayesian-shrinkage"><i class="fa fa-check"></i><b>19.3</b> Bayesian Shrinkage</a><ul>
<li class="chapter" data-level="19.3.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#priors-1"><i class="fa fa-check"></i><b>19.3.1</b> Priors</a></li>
<li class="chapter" data-level="19.3.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#spike-and-slab-prior"><i class="fa fa-check"></i><b>19.3.2</b> Spike and Slab prior</a></li>
<li class="chapter" data-level="19.3.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#normal-distribution"><i class="fa fa-check"></i><b>19.3.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="19.3.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#laplace-distribution"><i class="fa fa-check"></i><b>19.3.4</b> Laplace Distribution</a></li>
<li class="chapter" data-level="19.3.5" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#student-t-and-cauchy-distributions"><i class="fa fa-check"></i><b>19.3.5</b> Student-t and Cauchy Distributions</a></li>
<li class="chapter" data-level="19.3.6" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#horseshore-prior"><i class="fa fa-check"></i><b>19.3.6</b> Horseshore Prior</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#understanding-shrinkage-models"><i class="fa fa-check"></i><b>19.4</b> Understanding Shrinkage Models</a></li>
<li class="chapter" data-level="19.5" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#choice-of-hyperparameter-on-tau"><i class="fa fa-check"></i><b>19.5</b> Choice of Hyperparameter on <span class="math inline">\(\tau\)</span></a></li>
<li class="chapter" data-level="19.6" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#differences-between-bayesian-and-penalized-ml"><i class="fa fa-check"></i><b>19.6</b> Differences between Bayesian and Penalized ML</a></li>
<li class="chapter" data-level="19.7" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#examples-3"><i class="fa fa-check"></i><b>19.7</b> Examples</a></li>
</ul></li>
<li class="part"><span><b>IV Appendix</b></span><ul>
<li class="chapter" data-level="" data-path=""><a href="#prerequisites-12"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="19.8" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#parameters"><i class="fa fa-check"></i><b>19.8</b> Parameters</a></li>
<li class="chapter" data-level="19.9" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#miscellaneous-mathematical-background"><i class="fa fa-check"></i><b>19.9</b> Miscellaneous Mathematical Background</a><ul>
<li class="chapter" data-level="19.9.1" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#location-scale-families"><i class="fa fa-check"></i><b>19.9.1</b> Location-Scale Families</a></li>
<li class="chapter" data-level="19.9.2" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#scale-mixtures-of-normal-distributions"><i class="fa fa-check"></i><b>19.9.2</b> Scale Mixtures of Normal Distributions</a></li>
<li class="chapter" data-level="19.9.3" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#covariance-correlation-matrix-decomposition"><i class="fa fa-check"></i><b>19.9.3</b> Covariance-Correlation Matrix Decomposition</a></li>
<li class="chapter" data-level="19.9.4" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#qr-factorization"><i class="fa fa-check"></i><b>19.9.4</b> QR Factorization</a></li>
<li class="chapter" data-level="19.9.5" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#cholesky-decomposition"><i class="fa fa-check"></i><b>19.9.5</b> Cholesky Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="19.10" data-path="shrinkage-and-regularized-regression.html"><a href="shrinkage-and-regularized-regression.html#scaled-and-unscaled-variables"><i class="fa fa-check"></i><b>19.10</b> Scaled and Unscaled Variables</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>20</b> Distributions</a></li>
<li class="chapter" data-level="21" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html"><i class="fa fa-check"></i><b>21</b> Annotated Bibliography</a><ul>
<li class="chapter" data-level="21.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#textbooks"><i class="fa fa-check"></i><b>21.1</b> Textbooks</a></li>
<li class="chapter" data-level="21.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#syllabi"><i class="fa fa-check"></i><b>21.2</b> Syllabi</a></li>
<li class="chapter" data-level="21.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#topics"><i class="fa fa-check"></i><b>21.3</b> Topics</a></li>
<li class="chapter" data-level="21.4" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayes-theorem-1"><i class="fa fa-check"></i><b>21.4</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="21.5" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#article-length-introductions-to-bayesian-statistics"><i class="fa fa-check"></i><b>21.5</b> Article Length Introductions to Bayesian Statistics</a><ul>
<li class="chapter" data-level="21.5.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#why-bayesian"><i class="fa fa-check"></i><b>21.5.1</b> Why Bayesian</a></li>
<li class="chapter" data-level="21.5.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#modern-statistical-workflow"><i class="fa fa-check"></i><b>21.5.2</b> Modern Statistical Workflow</a></li>
<li class="chapter" data-level="21.5.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-philosophy"><i class="fa fa-check"></i><b>21.5.3</b> Bayesian Philosophy</a></li>
<li class="chapter" data-level="21.5.4" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>21.5.4</b> Bayesian Hypothesis Testing</a></li>
<li class="chapter" data-level="21.5.5" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-frequentist-debates"><i class="fa fa-check"></i><b>21.5.5</b> Bayesian Frequentist Debates</a></li>
<li class="chapter" data-level="21.5.6" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#categorical"><i class="fa fa-check"></i><b>21.5.6</b> Categorical</a></li>
<li class="chapter" data-level="21.5.7" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#variable-selection"><i class="fa fa-check"></i><b>21.5.7</b> Variable Selection</a></li>
<li class="chapter" data-level="21.5.8" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#multiple-testing"><i class="fa fa-check"></i><b>21.5.8</b> Multiple Testing</a></li>
<li class="chapter" data-level="21.5.9" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#rare-events-1"><i class="fa fa-check"></i><b>21.5.9</b> Rare Events</a></li>
<li class="chapter" data-level="21.5.10" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#identifiability"><i class="fa fa-check"></i><b>21.5.10</b> Identifiability</a></li>
<li class="chapter" data-level="21.5.11" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#shrinkage"><i class="fa fa-check"></i><b>21.5.11</b> Shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#software"><i class="fa fa-check"></i><b>21.6</b> Software</a><ul>
<li class="chapter" data-level="21.6.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#stan-2"><i class="fa fa-check"></i><b>21.6.1</b> Stan</a></li>
<li class="chapter" data-level="21.6.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#diagrams"><i class="fa fa-check"></i><b>21.6.2</b> Diagrams</a></li>
<li class="chapter" data-level="21.6.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#priors-2"><i class="fa fa-check"></i><b>21.6.3</b> Priors</a></li>
</ul></li>
<li class="chapter" data-level="21.7" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-model-averaging-1"><i class="fa fa-check"></i><b>21.7</b> Bayesian Model Averaging</a></li>
<li class="chapter" data-level="21.8" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#multilevel-modeling"><i class="fa fa-check"></i><b>21.8</b> Multilevel Modeling</a></li>
<li class="chapter" data-level="21.9" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#mixture-models"><i class="fa fa-check"></i><b>21.9</b> Mixture Models</a></li>
<li class="chapter" data-level="21.10" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#inference"><i class="fa fa-check"></i><b>21.10</b> Inference</a><ul>
<li class="chapter" data-level="21.10.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#discussion-of-bayesian-inference"><i class="fa fa-check"></i><b>21.10.1</b> Discussion of Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="21.11" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#model-checking-1"><i class="fa fa-check"></i><b>21.11</b> Model Checking</a><ul>
<li class="chapter" data-level="21.11.1" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#posterior-predictive-checks-1"><i class="fa fa-check"></i><b>21.11.1</b> Posterior Predictive Checks</a></li>
<li class="chapter" data-level="21.11.2" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#prediction-criteria"><i class="fa fa-check"></i><b>21.11.2</b> Prediction Criteria</a></li>
<li class="chapter" data-level="21.11.3" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#software-validation"><i class="fa fa-check"></i><b>21.11.3</b> Software Validation</a></li>
</ul></li>
<li class="chapter" data-level="21.12" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#hierarchical-modeling"><i class="fa fa-check"></i><b>21.12</b> Hierarchical Modeling</a></li>
<li class="chapter" data-level="21.13" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#shrinkageregularization"><i class="fa fa-check"></i><b>21.13</b> Shrinkage/Regularization</a></li>
<li class="chapter" data-level="21.14" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#empirical-bayes"><i class="fa fa-check"></i><b>21.14</b> Empirical Bayes</a></li>
<li class="chapter" data-level="21.15" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#history-of-bayesian-statistics"><i class="fa fa-check"></i><b>21.15</b> History of Bayesian Statistics</a></li>
<li class="chapter" data-level="21.16" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#sampling-difficulties"><i class="fa fa-check"></i><b>21.16</b> Sampling Difficulties</a></li>
<li class="chapter" data-level="21.17" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#complicated-estimation-and-testing"><i class="fa fa-check"></i><b>21.17</b> Complicated Estimation and Testing</a></li>
<li class="chapter" data-level="21.18" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#pooling-polls"><i class="fa fa-check"></i><b>21.18</b> Pooling Polls</a></li>
<li class="chapter" data-level="21.19" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#visualizing-mcmc-methods"><i class="fa fa-check"></i><b>21.19</b> Visualizing MCMC Methods</a></li>
<li class="chapter" data-level="21.20" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayesian-point-estimation-decision"><i class="fa fa-check"></i><b>21.20</b> Bayesian point estimation / Decision</a></li>
<li class="chapter" data-level="21.21" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#stan-modeling-language"><i class="fa fa-check"></i><b>21.21</b> Stan Modeling Language</a></li>
<li class="chapter" data-level="21.22" data-path="annotated-bibliography.html"><a href="annotated-bibliography.html#bayes-factors"><i class="fa fa-check"></i><b>21.22</b> Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-11.html"><a href="references-11.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Updating: A Set of Bayesian Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\median}{median}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\logistic}{Logistic}
\DeclareMathOperator{\logit}{Logit}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

% This follows BDA
\newcommand{\dunif}{\mathsf{Uniform}}
\newcommand{\dnorm}{\mathsf{Normal}}
\newcommand{\dhalfnorm}{\mathrm{HalfNormal}}
\newcommand{\dlnorm}{\mathsf{LogNormal}}
\newcommand{\dmvnorm}{\mathsf{Normal}}
\newcommand{\dgamma}{\mathsf{Gamma}}
\newcommand{\dinvgamma}{\mathsf{InvGamma}}
\newcommand{\dchisq}{\mathsf{ChiSquared}}
\newcommand{\dinvchisq}{\mathsf{InvChiSquared}}
\newcommand{\dexp}{\mathsf{Exponential}}
\newcommand{\dlaplace}{\mathsf{Laplace}}
\newcommand{\dweibull}{\mathsf{Weibull}}
\newcommand{\dwishart}{\mathsf{Wishart}}
\newcommand{\dinvwishart}{\mathsf{InvWishart}}
\newcommand{\dlkj}{\mathsf{LkjCorr}}
\newcommand{\dt}{\mathsf{StudentT}}
\newcommand{\dhalft}{\mathsf{HalfStudentT}}
\newcommand{\dbeta}{\mathsf{Beta}}
\newcommand{\ddirichlet}{\mathsf{Dirichlet}}
\newcommand{\dlogistic}{\mathsf{Logistic}}
\newcommand{\dllogistic}{\mathsf{LogLogistic}}
\newcommand{\dpois}{\mathsf{Poisson}}
\newcommand{\dBinom}{\mathsf{Binomial}}
\newcommand{\dmultinom}{\mathsf{Multinom}}
\newcommand{\dnbinom}{\mathsf{NegativeBinomial}}
\newcommand{\dnbinomalt}{\mathsf{NegativeBinomial2}}
\newcommand{\dbetabinom}{\mathsf{BetaBinomial}}
\newcommand{\dcauchy}{\mathsf{Cauchy}}
\newcommand{\dhalfcauchy}{\mathsf{HalfCauchy}}
\newcommand{\dbernoulli}{\mathsf{Bernoulli}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Reals}{\R}
\newcommand{\RealPos}{\R^{+}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Nats}{\N}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}

\DeclareMathOperator{\invlogit}{Inv-Logit}
\DeclareMathOperator{\logit}{Logit}
\DeclareMathOperator{\diag}{diag}

\]
<div id="shrinkage-and-regularized-regression" class="section level1">
<h1><span class="header-section-number">19</span> Shrinkage and Regularized Regression</h1>
<div id="prerequisites-11" class="section level2 unnumbered">
<h2>Prerequisites</h2>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;rstan&quot;</span>)</a>
<a class="sourceLine" id="cb144-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;rstanarm&quot;</span>)</a>
<a class="sourceLine" id="cb144-3" data-line-number="3"><span class="kw">library</span>(<span class="st">&quot;bayz&quot;</span>)</a>
<a class="sourceLine" id="cb144-4" data-line-number="4"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)</a>
<a class="sourceLine" id="cb144-5" data-line-number="5"><span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)</a>
<a class="sourceLine" id="cb144-6" data-line-number="6"><span class="kw">library</span>(<span class="st">&quot;glmnet&quot;</span>)</a>
<a class="sourceLine" id="cb144-7" data-line-number="7"><span class="kw">library</span>(<span class="st">&quot;recipes&quot;</span>)</a></code></pre></div>
</div>
<div id="introduction-6" class="section level2">
<h2><span class="header-section-number">19.1</span> Introduction</h2>
<p><em>Shrinkage estimation</em> deliberately introduces biases into the model to improve
<em>overall performance, often at the cost of individual estimates
</em><span class="citation">(Efron and Hastie 2016, 91)</span>.</p>
<p>This is opposed to MLE, which produces unbiased estimates (asymptotically,
given certain regularity conditions). Likewise, the Bayesian estimates with
non- or weakly-informative priors will produce estimates similar to the MLE.
With shrinkage, the priors are used to produce estimates <em>different</em> than the
MLE case.</p>
<p><em>Regularization</em> describes any method that reduces variability in high
dimensional estimation or prediction problems <span class="citation">(Efron and Hastie 2016)</span>.</p>
</div>
<div id="penalized-maximum-likelihood-regression" class="section level2">
<h2><span class="header-section-number">19.2</span> Penalized Maximum Likelihood Regression</h2>
<p>OLS finds the <span class="math inline">\(\beta\)</span> that minimize the in-sample sum of squared errors,
<span class="math display">\[
\hat{\beta}_{\text{OLS}} = \arg\min_{\beta} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2
\]</span></p>
<p>Penalized regressions add a penalty term increasing in the magnitude of <span class="math inline">\(\beta\)</span> to the minimization function.
<span class="math display">\[
\hat{\beta}_{\text{penalized}} = \argmin_{\beta} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2 + \underbrace{f(\beta)}_{\text{shrinkage penalty}},
\]</span>
where <span class="math inline">\(f\)</span> is some sort of penalty function on <span class="math inline">\(\beta\)</span> that penalizes larger (in magnitude) values of <span class="math inline">\(\beta\)</span>.</p>
<p>Penalized regression purposefully introduces bias into the regression in order
to reduce variance and improve out-of-sample prediction. The penalty term,
when chosen by cross-validation or an approximation thereof, allows for trading
off bias and variance.</p>
<p>Different penalized regression methods use different choices of <span class="math inline">\(f(\beta)\)</span>,
The two most commonly penalty functions are Ridge and Lasso.</p>
<div id="ridge-regression" class="section level3">
<h3><span class="header-section-number">19.2.1</span> Ridge Regression</h3>
<p>Ridge regression uses the following penalty <span class="citation">(Hoerl and Kennard 1970)</span>:
<span class="math display">\[
\hat{\beta}_{\text{ridge}} = \arg\min_{\beta} \underbrace{\sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2}_{\text{RSS}} + \underbrace{\lambda}_{\text{tuning parameter}} \underbrace{\sum_{k} \beta_k^2}_{\ell_2 \text{ norm}^2}
\]</span>
The <span class="math inline">\(\ell_2\)</span> norm of <span class="math inline">\(\beta\)</span> is,
<span class="math display">\[
||\beta||_{2} = \sqrt{\sum_{k = 1}^K \beta_k^2} .
\]</span></p>
<p>The ridge regression coefficients are smaller in magnitude than the OLS coefficients, <span class="math inline">\(|\hat{\beta}_{ridge}| &lt; |\hat{\beta}_{OLS}|\)</span>.
However, this bias in the coefficients can be offset by a lower variance, better MSE, and better out-of-sample performance than the OLS estimates.</p>
<p>Unlike many other penalized regression estimators, ridge regression has a
close-form solution.
The expected value and variance-covariance matrix of the ridge regression
coefficients is,
<span class="math display">\[
\begin{aligned}[t]
\E[\hat{\beta}_{\text{ridge}}] &amp;= M y \\
\Var[\hat{\beta}_{\text{ridge}}] &amp;=  \sigma^2 M&#39; M \\
M &amp;= (X&#39; X + \lambda I)^{-1} X&#39; .
\end{aligned}
\]</span></p>
<p>Some implications:</p>
<ul>
<li><p><span class="math inline">\(\hat{\vec{\beta}}\)</span> exists even if <span class="math inline">\(\hat{\vec{\beta}}_{\text{OLS}}\)</span>
(<span class="math inline">\((\mat{X}\T\mat{X})^{-1}\)</span>), i.e. cases of <span class="math inline">\(n &gt; p\)</span> and collinearity, does
not exist.</p></li>
<li><p>If <span class="math inline">\(\mat{X}\)</span> is orthogonal (mean 0, unit variance, zero correlation),
<span class="math inline">\(\mat{X}\T \mat{X} = n \mat{I}_p\)</span> then
<span class="math display">\[
\hat{\vec{\beta}}_{\text{ridge}} = \frac{n}{n + \lambda}
\hat{\vec{\beta}}_{\text{ols}}
\]</span>
meaning,
<span class="math display">\[
|\hat{\vec{\beta}}_{\text{ols}}| &gt;
|\hat{\vec{\beta}}_{\text{ridge}}| \geq 0
\]</span></p></li>
<li><p>Ridge does not produce sparse estimates, since
<span class="math inline">\((n / (n + \lambda)) \vec{\vec{\beta}}_{ols} = 0\)</span> iff <span class="math inline">\(\vec{\vec{\beta}}_{ols} = 0\)</span></p></li>
<li><p>If <span class="math inline">\(\lambda = 0\)</span>, then the ridge coefficients are the same as the OLS
coefficients,
<span class="math inline">\(\lambda \to 0 \Rightarrow \hat{beta}_{\text{ridge}} \to \hat{beta}_{OLS}\)</span></p></li>
<li><p>As <span class="math inline">\(\lambda\)</span> increases the coefficients are shrunk to 0,
<span class="math inline">\(\lambda \to \infty \Rightarrow \hat{\beta}_{\text{ridge}} = 0\)</span>.</p></li>
</ul>
</div>
<div id="lasso" class="section level3">
<h3><span class="header-section-number">19.2.2</span> Lasso</h3>
<p>The lasso (Least Absolute Shrinkage and Selection Operator) uses an <span class="math inline">\(\ell_1\)</span> norm of <span class="math inline">\(\beta\)</span> as a penalty <span class="citation">(Tibshirani 1996)</span>,
<span class="math display">\[
\hat{\beta}_{\text{lasso}} = \arg\min_{\beta} \frac{1}{2 \sigma} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2 + \lambda \sum_{k} |\beta_k|
\]</span>
where <span class="math inline">\(\lambda \geq 0\)</span> is a tuning or shrinkage parameter chosen by cross-validation or a plug-in statistic.</p>
<p>The <span class="math inline">\(\ell_1\)</span> norm of <span class="math inline">\(\beta\)</span> is the sum of the absolute values of its elements,
<span class="math display">\[
||\beta||_{1} = \sum_{k = 1}^K |\beta_k| .
\]</span></p>
<p>Properties:</p>
<ul>
<li><p>Unlike ridge regression, it sets some coefficients exactly to 0, producing
sparse solutions.</p></li>
<li><p>If variables are perfectly correlated, there is no unique solution
(unlike the ridge regression).</p></li>
<li><p>Used as the best convex approximation of the “best subset selection”
regression problem, which finds the number of nonzero entries in a vector.</p></li>
<li><p>Unlike ridge regression, there is no closed-form solution.
Since <span class="math inline">\(|\beta_k|\)</span> does not have a derivative, it was a more difficult
iterative problem than many other regression functions. However, now there
are several algorithms to estimate it.</p></li>
</ul>
</div>
<div id="constrained-optimization-interpretation" class="section level3">
<h3><span class="header-section-number">19.2.3</span> Constrained Optimization Interpretation</h3>
</div>
<div id="bayesian-interpretation" class="section level3">
<h3><span class="header-section-number">19.2.4</span> Bayesian Interpretation</h3>
<p>The penalty term in regressions can generally be interpreted a prior on the
coefficients.
Recall that although OLS does not require normal errors, the OLS coefficients are equivalent to the MLE of a probability model with normal errors,
<span class="math display">\[
\begin{aligned}
\hat{\beta}_{MLE} &amp;= \arg \max_{\beta} \dnorm(y | x \beta, \sigma) \\
&amp; = \arg \max_{\beta} {(2 \pi \sigma^2)}^{n / 2} \prod_{i = 1}^{n} \exp\left(-\frac{(y_i - x_i&#39; \beta)^2}{2 \sigma^2}\right) \\
&amp;= \arg \max_{\beta} \frac{n}{2} (\log 2 + \log \pi) + n \log \sigma + \sum_{i = 1}^{n} \left( -\frac{(y_i - x_i&#39; \beta)^2}{2 \sigma^2} \right) \\
&amp; = \arg \max_{\beta} \sum_{i = 1}^{n} - (y_i - x&#39;_i \beta)^2 \\
&amp;= \arg \min_{\beta} \sum_{i = 1} (y_i - x&#39;_i \beta)^2 \\
&amp;= \hat{\beta}_{OLS}
\end{aligned}
\]</span>
Likewise the shrinkage prior can be represented as a normal distribution with mean 0 and scale <span class="math inline">\(1 / \lambda\)</span>, since the <span class="math inline">\(\beta\)</span> that maximize the probability of that, minimize the <span class="math inline">\(\ell_2\)</span> norm of <span class="math inline">\(\beta\)</span>,
<span class="math display">\[
\begin{aligned}
\arg \max_{\beta} \dnorm(\beta | 0,  \tau) &amp;= \arg \max_{\beta} {(2 \pi \sigma^2)}^{K / 2} \prod_{k = 1}^{K} \exp\left(- \frac{(0 - \beta_k)^2}{2 \tau^2} \right)
\\
&amp;= \arg \max_{\beta} \sum_{k = 1}^{K} \left(-\frac{\beta_k^2}{2 \tau^2}\right) \\
&amp;= \arg \min_{\beta} \frac{1}{2 \tau^2} \sum_{k = 1}^K \beta_k^2
\end{aligned}
\]</span>
where <span class="math inline">\(\tau^2 = 1 / 2 \lambda\)</span>.</p>
<p>Thus ridge regression can be thought of as a MAP estimator of the model
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha + x&#39; \beta, \sigma) \\
\beta_k &amp;\sim \dnorm(0, (2 \lambda)^{-1/2} )
\end{aligned}
\]</span>
Similarly, the <span class="math inline">\(\beta\)</span> that minimize the <span class="math inline">\(\ell_1\)</span> norm also maximize the probability of random variables iid from the Laplace distribution, <span class="math inline">\(\dlaplace(\beta_k | 0, 1 / \lambda)\)</span>.
<span class="math display">\[
\begin{aligned}
\arg \max_{\beta} \dlaplace(\beta | 0, 1 / \lambda) &amp;= \arg \max_{\beta} \left(\frac{\lambda}{2}\right)^{K} \prod_{k = 1}^{K} \exp\left(- \lambda |0 -
\beta_k)| \right) \\
&amp;= \arg \max_{\beta} \sum_{k = 1}^{K} - \lambda |\beta_k| \\
&amp;= \arg \min_{\beta} \lambda \sum_{k = 1}^K |\beta_k|
\end{aligned}
\]</span>
Thus lasso regression can be thought of as a MAP estimator of the model,
<span class="math display">\[
\begin{aligned}[t]
y_i &amp;\sim \dnorm(\alpha + x&#39; \beta, \sigma) \\
\beta_k &amp;\sim \dlaplace(0, 1 / \lambda)
\end{aligned}
\]</span></p>
</div>
</div>
<div id="bayesian-shrinkage" class="section level2">
<h2><span class="header-section-number">19.3</span> Bayesian Shrinkage</h2>
<p>Consider the single output linear Gaussian regression model with several input variables, given by
<span class="math display">\[
\begin{aligned}[t]
y_i \sim \dnorm(\vec{x}_i&#39; \vec{\beta}, \sigma^2)
\end{aligned}
\]</span>
where <span class="math inline">\(\vec{x}\)</span> is a <span class="math inline">\(k\)</span>-vector of predictors, and <span class="math inline">\(\vec{\beta}\)</span> are the coefficients.</p>
<p>What priors do we put on <span class="math inline">\(\beta\)</span>?</p>
<ul>
<li><p><strong>Improper priors:</strong> <span class="math inline">\(\beta_k \propto 1\)</span> This produces the equivalent of
MLE estimates.</p></li>
<li><p><strong>Non-informative priors:</strong> These are priors which have such wide variance
that they have little influence on the posterior, e.g.
<span class="math inline">\(\beta_k \sim \dnorm(0, 1e6)\)</span>. The primary reason for these (as opposed to
simply using an improper prior) is that some MCMC methods, e.g. Gibbs sampling
as used in JAGS or BUGS, require proper prior distributions for all parameters.</p></li>
</ul>
<p><strong>Shrinkage priors</strong> have a few characteristics</p>
<ul>
<li><p>they push <span class="math inline">\(\beta_k \to 0\)</span></p></li>
<li><p>while in the other cases, the scale of the prior on <span class="math inline">\(\beta\)</span> is fixed, in
shrinkage priors there is often a hyperprior on it, e.g.,
<span class="math inline">\(\beta_k \sim \dnorm(0, \tau)\)</span>, where <span class="math inline">\(\tau\)</span> is also a parameter to be estimated.</p></li>
</ul>
<div id="priors-1" class="section level3">
<h3><span class="header-section-number">19.3.1</span> Priors</h3>
<p>Consider the regression:
<span class="math display">\[
y_i \sim \dnorm(\alpha + x_i&#39; \beta, \sigma)
\]</span></p>
<p>It is assumed that the outcome and predictor variables are standardized such that
<span class="math display">\[
\begin{aligned}[t]
\E[y_i] &amp;= 0 &amp; \V[y_i] &amp;= 1 \\
\E[x_i] &amp;= 0 &amp; \V[x_i] &amp;= 1. 
\end{aligned}
\]</span>
Then, the default weakly informative priors are,
<span class="math display">\[
\begin{aligned}[t]
\alpha &amp;\sim \dnorm(0, 10) \\
\beta_k &amp;\sim \dnorm(0, 2.5) &amp; k \in \{1, \dots, K\}
\end{aligned}
\]</span></p>
<p>The weakly informative priors will shrink all the coefficients towards zero.</p>
<p>The amount of shrinkage depends on the amount of data as the likelihood dominates the prior as the amount of data increases.
However, the amount of shrinkage is not estimated from the data.
The prior on each coefficient is independent, and its scale is a constant (2.5 in this example).</p>
<p>Regularization/shrinkage methods estimate the amount of shrinkage. The scale of the priors on the coefficients are hyperparameters, which are estimated from the data.</p>
</div>
<div id="spike-and-slab-prior" class="section level3">
<h3><span class="header-section-number">19.3.2</span> Spike and Slab prior</h3>
<p><span class="math display">\[
\begin{aligned}[t]
\beta_k | \lambda_k, c, \epsilon  &amp;\sim \lambda_k N(0, c^2) + (1 - \lambda_j) N(0, \epsilon^2) \\
\lambda_k &amp;\sim \dbern(\pi)
\end{aligned}
\]</span></p>
<p>In the case of the linear regression, an alternative to BMA is to use a
spike-and-slab prior <span class="citation">(Mitchell and Beauchamp 1988, <span class="citation">@GeorgeMcCulloch1993a</span>, <span class="citation">@IshwaranRao2005a</span>)</span>,
which is a prior that is a discrete mixture of a point mass at 0 and a
non-informative distribution.</p>
<p>The spike and slab prior is a “two-group” solution.
<span class="math display">\[
p(\beta_k) = (1 - w) \delta_0 + w \pi(\beta_k)
\]</span>
where <span class="math inline">\(\delta_0\)</span> is a Dirac delta function putting a point mass at 0, and <span class="math inline">\(\pi(\beta_k)\)</span> is an uninformative distribution, e.g. <span class="math inline">\(\pi(\beta_k) = \dnorm(\beta_k | 0, \sigma^2)\)</span> where <span class="math inline">\(\sigma\)</span> is large.</p>
<p>The posterior distribution of <span class="math inline">\(w\)</span> is the probability that <span class="math inline">\(\beta_k \neq 0\)</span>, and the conditional posterior distribution <span class="math inline">\(p(\beta_k | y, w = 1)\)</span> is the distribution of <span class="math inline">\(\beta_k\)</span> given that <span class="math inline">\(\beta_k \neq 0\)</span>.</p>
<p>See the R package <strong><a href="https://cran.r-project.org/package=spikeslab">spikeslab</a></strong> and he accompanying article <span class="citation">(Ishwaran, Kogalur, and Rao 2010)</span> for an implementation and review of spike-and-slab regressions.</p>
</div>
<div id="normal-distribution" class="section level3">
<h3><span class="header-section-number">19.3.3</span> Normal Distribution</h3>
<p>We can apply a normal prior to each <span class="math inline">\(\beta_k\)</span>.
Unlike the weakly informative priors, the prior distributions all share a scale parameter <span class="math inline">\(\tau\)</span>.
<span class="math display">\[
\beta_k | \tau \sim \dnorm(0, \tau)
\]</span>
We need to assign a prior to <span class="math inline">\(\tau\)</span>.</p>
<p>In MAP estimation this is often set to be an improper uniform distribution.
Since in the weakly informative prior, <span class="math inline">\(\beta_k \sim \dnorm(0, 2.5\)</span> for all <span class="math inline">\(k\)</span>, a prior on <span class="math inline">\(\tau\)</span> in which the central tendency is the same as the weakly informative prior makes sense.
One such prior is
<span class="math display">\[
\tau \sim \dexp(2.5)
\]</span>
TODO: look for better guidance for the prior of <span class="math inline">\(\tau\)</span>.</p>
<ul>
<li>This is equivalent to Ridge regression.</li>
<li>Unlike most shrinkage estimators, there is a closed form solution to the posterior distribution</li>
</ul>
</div>
<div id="laplace-distribution" class="section level3">
<h3><span class="header-section-number">19.3.4</span> Laplace Distribution</h3>
<p>We can also use the Laplace distribution as a prior for the coefficients.
This is called Bayesian Lasso, because the MAP estimator of this model is equivalent to the Lasso estimator.</p>
<p>The prior distribution for each coefficient <span class="math inline">\(\beta_k\)</span> is a Laplace (or double exponential) distribution with scale parameter (<span class="math inline">\(\tau\)</span>).
<span class="math display">\[
\beta_k | \tau \sim \dlaplace(0, \tau)
\]</span>
Like many priors that have been proposed and used for coefficient shrinkage, this can be represented as a local-global scale-mixture of normal distributions.
<span class="math display">\[
\begin{aligned}
\beta_k | \tau &amp;\sim \dnorm(0, \tau \lambda_k) \\
\lambda_k^{-2} &amp;\sim \dexp(1/2)
\end{aligned}
\]</span>
The global scale <span class="math inline">\(\tau\)</span> determines the overall amount of shrinkage.
The local scales, <span class="math inline">\(\lambda_1, \dots, \lambda_K\)</span>, allow the amount of shrinkage to vary among coefficients.</p>
</div>
<div id="student-t-and-cauchy-distributions" class="section level3">
<h3><span class="header-section-number">19.3.5</span> Student-t and Cauchy Distributions</h3>
<p>We can also use the Student-t distribution as a prior for the coefficients.
The Cauchy distribution is a special case of the Student t distribution where the degrees of freedom is zero.</p>
<p>The prior distribution for each coefficient <span class="math inline">\(\beta_k\)</span> is a Student-t distribution with degrees of freedom <span class="math inline">\(\nu\)</span>, location 0, and scale <span class="math inline">\(\tau\)</span>,
<span class="math display">\[
\beta_k | \tau \sim \dt(\nu, 0, \tau) .
\]</span>
Like many priors that have been proposed and used for coefficient shrinkage, this can be represented as a local-global scale-mixture of normal distributions.
<span class="math display">\[
\begin{aligned}
\beta_k | \tau, \lambda &amp;\sim \dnorm(0, \tau \lambda_k) \\
\lambda_k^{-2} &amp;\sim \dgamma(\nu/2, \nu/2)
\end{aligned}
\]</span></p>
<p>The degrees of freedom parameter <span class="math inline">\(\nu\)</span> can be fixed to a particular value or estimated.
If fixed, then common values are 1 for a Cauchy distribution, 2 to ensure that there is a finite mean, 3 to ensure that there is a finite variance, and 4 ensure that there is a finite kurtosis.</p>
<p>If estimated, then the
<span class="math display">\[
\nu \sim \dgamma(2, 0.1)
\]</span>
Additionally, it may be useful to truncate the values of <span class="math inline">\(\nu\)</span> to be greater
than 2 to ensure a finite variance of the Student t distribution.</p>
</div>
<div id="horseshore-prior" class="section level3">
<h3><span class="header-section-number">19.3.6</span> Horseshore Prior</h3>
<p>The Horseshoe prior is defined solely in terms of a global-local mixture.
<span class="math display">\[
\begin{aligned}
\beta_k | \tau, \lambda &amp;\sim \dnorm(0, \tau \lambda_k) \\
\lambda_k &amp;\sim \dhalfcauchy(1)
\end{aligned}
\]</span></p>
<p>The Hierarchical Shrinkage prior originally implemented in rstanarm and proposed by … replaces the half-Cauchy prior on <span class="math inline">\(\lambda_k\)</span> with a half-Student-t distribution with degrees of freedom <span class="math inline">\(\nu\)</span>.
<span class="math display">\[
\lambda_k \sim \dt(\nu, 0, 1)
\]</span>
The <span class="math inline">\(\nu\)</span> parameter is generally not estimated and fixed to a low value, with <span class="math inline">\(\nu = 4\)</span> being suggested.
The problem with estimating the Horseshoe prior is that the wide tails of the Cauchy prior produced a posterior distribution with problematic geometry that was hard to sample.
Increasing the degrees of freedom helped to regularize the posterior.
The downside of this method is that by increasing the degrees of freedom of the Student-t distribution it would also shrink large parameters, which the
Horseshoe prior was designed to avoid.</p>
<p>Regularized horseshoe prior
<span class="math display">\[
\begin{aligned}
\beta_k | \tau, \lambda &amp;\sim \dnorm(0, \tau \tilde{\lambda}_k) \\
\tilde{\lambda}^2_k &amp;= \frac{c^2 \lambda^2}{c^2 + \lambda^2} \\
\lambda_k &amp;\sim \dhalfcauchy(1)
\end{aligned}
\]</span>
where <span class="math inline">\(c &gt; 0\)</span> is a constant.
Like using a Student-t distribution, this regularizes the posterior distribution of a Horseshoe prior.
However, it is less problematic in terms of shrinking large coefficients.</p>
<p>Since there is little information about <span class="math inline">\(c\)</span>, <span class="math inline">\(c\)</span> is treated as a parameter,
and a prior is placed on it.
<span class="math display">\[
c \sim \dt(0, s^2)
\]</span></p>
</div>
</div>
<div id="understanding-shrinkage-models" class="section level2">
<h2><span class="header-section-number">19.4</span> Understanding Shrinkage Models</h2>
<p>Suppose that <span class="math inline">\(X\)</span> is a <span class="math inline">\(n \times K\)</span> matrix of predictors,
and <span class="math inline">\(y\)</span> is a <span class="math inline">\(n \times 1\)</span> vector of outcomes.
The conditional posterior for <span class="math inline">\(\beta\)</span> given <span class="math inline">\((X, y)\)</span> is
<span class="math display">\[
\begin{aligned}[t]
p(\beta | \Lambda, \tau, \sigma^2, D) &amp;= \dnorm(\beta | \bar{\beta}, \Sigma), \\
\bar{\beta} &amp;= \tau^2 \Lambda (\tau^2 \Lambda + \sigma^2 (X&#39;X)^{-1})^{-1} \hat{\beta}, \\
\Sigma &amp;= (\tau^{-2} \Lambda^{-1} + \frac{1}{\sigma^{2}} X&#39;X)^{-1}, \\
\Lambda &amp;= \diag(\lambda_1^{2}, \dots, \lambda^{2}_D), \\
\hat{\beta} &amp;= (X&#39;X)^{-1} X&#39;y .
\end{aligned}
\]</span>
If the predictors are uncorrelated with zero mean and variances <span class="math inline">\(\Var(x_k) = s_k^2\)</span>, then
<span class="math display">\[
X&#39;X \approx n \diag(s_1^2, \dots, s^2_K) ,
\]</span>
and we can use the approximations,
<span class="math display">\[
\bar{\beta}_k = (1 - \kappa_k) \hat{\beta}_k,  \\
\kappa_k = \frac{1}{1 + n \sigma^{-2} \tau^2 s_k^2 \lambda_k^2} .
\]</span>
The value <span class="math inline">\(\kappa_k\)</span> is called the <em>shrinkage factor</em> for coefficient <span class="math inline">\(\beta_k\)</span>.
When <span class="math inline">\(\kappa_k = 0\)</span>, then there is no shrinkage and the posterior coefficient is the same as the MLE solution, <span class="math inline">\(\bar{\beta} = \hat{\beta}\)</span>.
When <span class="math inline">\(\kappa_k = 1\)</span>, then there is complete shrinkage and the posterior coefficient is zero, <span class="math inline">\(\bar{\beta} = 0\)</span>.
It also follows that <span class="math inline">\(\bar{\beta} \to 0\)</span> as <span class="math inline">\(\tau \to 0\)</span>, and <span class="math inline">\(\bar{\beta} \to \hat{\beta}\)</span> as <span class="math inline">\(\tau \to \infty\)</span>.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1">shrinkage_factor &lt;-<span class="st"> </span><span class="cf">function</span>(n, <span class="dt">sigma =</span> <span class="dv">1</span>, <span class="dt">tau =</span> <span class="dv">1</span>, <span class="dt">sd_x =</span> <span class="dv">1</span>, <span class="dt">lambda =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb145-2" data-line-number="2">  <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>tau <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>sd_x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>lambda <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>sigma <span class="op">^</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb145-3" data-line-number="3">}</a></code></pre></div>
</div>
<div id="choice-of-hyperparameter-on-tau" class="section level2">
<h2><span class="header-section-number">19.5</span> Choice of Hyperparameter on <span class="math inline">\(\tau\)</span></h2>
<p>The value of <span class="math inline">\(\tau\)</span> and the choice of its hyper-parameter has a big influence on the sparsity of the coefficients.</p>
<!-- @CarvalhoPolsonScott2009a suggest -->
<!-- $$ -->
<!-- \tau \sim \dhalfcauchy(0, \sigma), -->
<!-- $$ -->
<!-- while @PolsonScott2011a suggest, -->
<!-- $$ -->
<!-- \tau \sim \dhalfcauchy(0, 1) . -->
<!-- $$ -->
<!-- @PasKleijnVaart2014a suggest -->
<!-- $$ -->
<!-- \tau \sim \dhalfcauchy(0, p^{*} / n) -->
<!-- $$ -->
<!-- where $p^*$ is the true number of non-zero parameters, -->
<!-- and $n$ is the number of observations. -->
<!-- They suggest $\tau = p^{*} / n$ or $\tau p^{*}  / n \sqrt{log(n / p^{*})}$. -->
<!-- Additionally, they suggest restricting $\tau$ to $[0, 1]$. -->
<p><span class="citation">Piironen and Vehtari (2017)</span> treat the prior on <span class="math inline">\(\tau\)</span> as the implied prior on the number of effective parameters.
The shrinkage can be understood as its influence on the number of effective parameters, <span class="math inline">\(m_{eff}\)</span>,
<span class="math display">\[
m_{\text{eff}} = \sum_{j = 1}^K (1 - \kappa_j) .
\]</span>
This is a measure of effective model size.</p>
<p><span class="citation">Piironen and Vehtari (2017)</span> show that for a given <span class="math inline">\(n\)</span> (data standard deviation), <span class="math inline">\(\tau\)</span>, <span class="math inline">\(\lambda_k\)</span>, and <span class="math inline">\(\sigma\)</span>, the and variance of <span class="math inline">\(m_{eff}\)</span>
<span class="math display">\[
\begin{aligned}[t]
\E[m_{eff} | \tau, \sigma] &amp;= \frac{\sigma^{-1} \tau \sqrt{n}}{1 + \sigma^{-1} \tau \sqrt{n}} K , \\
\Var[m_{eff} | \tau, \sigma] &amp;= \frac{\sigma^{-1} \tau \sqrt{n}}{2 (1 + \sigma^{-1} \tau \sqrt{n})2} K .
\end{aligned}
\]</span></p>
<p>Given a prior guess about the sparsity <span class="math inline">\(\beta\)</span>, a prior should be chosen such that it places mass near that guess.
Let <span class="math inline">\(k_0 \in [0, K]\)</span> be the expected number of non-zero elements of <span class="math inline">\(\beta\)</span>, then choose <span class="math inline">\(\tau_0\)</span> such that
<span class="math display">\[
\tau_0 = \frac{k_0}{K - k_0}\frac{\sigma}{\sqrt{n}}
\]</span></p>
<p>This prior depends on the expected sparsity of the solution, which depends on the problem.
PiironenVehtari2017a provide no guidence on how to select <span class="math inline">\(p_0\)</span>.
Perhaps a simpler model, e.g. lasso could be used to estimate <span class="math inline">\(p_0\)</span>.</p>
<ul>
<li><span class="citation">Datta and Ghosh (2013)</span> warn against empirical Bayes estimators of <span class="math inline">\(\tau\)</span> for the horseshoe prior as it can collapse to 0.</li>
<li><span class="citation">Scott and Berger (2010)</span> consider marginal maximum likelihood estimates of <span class="math inline">\(\tau\)</span>. –&gt;</li>
<li><span class="citation">Pas, Kleijn, and Vaart (2014)</span> suggest that an empirical Bayes estimator truncated below at <span class="math inline">\(1 / n\)</span>.</li>
</ul>
<!-- Densities of the shrinkage parameter, $\kappa$, for various shrinkage distributions where $\sigma^2 = 1$, $\tau = 1$, for $n = 1$. -->
<p><img src="regression-shrinkage_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="differences-between-bayesian-and-penalized-ml" class="section level2">
<h2><span class="header-section-number">19.6</span> Differences between Bayesian and Penalized ML</h2>
<p><span class="math display">\[
\log p(\theta|y, x) \propto \frac{1}{2 \sigma} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2 + \lambda \sum_{k} \beta_k^2
\]</span>
In the first case, the log density of a normal distribution is,
<span class="math display">\[
\log p(y | \mu, x) \propto \frac{1}{2 \sigma} (x - \mu)^2
\]</span>
The first regression term is the produce of normal distributions (sum of their log probabilities),
<span class="math display">\[
y_i \sim \dnorm(\vec{x}_i\T \vec{\beta}, \sigma)
\]</span>
The second term, <span class="math inline">\(\lambda \sum_{k} \beta_k^2\)</span> is also the sum of the log of densities of i.i.d. normal densities, with mean 0, and scale <span class="math inline">\(\tau = 1 / 2 \lambda\)</span>,
<span class="math display">\[
\beta_k \sim \dnorm(0, \tau^2)
\]</span></p>
<p>The only difference in the LASSO is the penalty term, which uses an absolute value penalty for <span class="math inline">\(\beta_k\)</span>.
That term corresponds to a sum of log densities of i.i.d. double exponential (Laplace) distributions.
The double exponential distribution density is similar to a normal distribution,
<span class="math display">\[
\log p(y | \mu, \sigma) \propto - \frac{|y - \mu|}{\sigma}
\]</span>
So the LASSO penalty is equivalent to the log density of a double exponential distribution with location <span class="math inline">\(0\)</span>, and scale <span class="math inline">\(1 / \lambda\)</span>.
<span class="math display">\[
\beta_k \sim \dlaplace(0, \tau)
\]</span></p>
<p>There are several differences between Bayesian approaches to shrinkage and penalized ML approaches.</p>
<p>The point estimates:</p>
<ul>
<li>ML: mode</li>
<li>Bayesian: posterior mean (or median)</li>
</ul>
<p>In Lasso</p>
<ul>
<li>ML: the mode produces exact zeros and sparsity</li>
<li>Bayesian: posterior mean is not sparse (zero)</li>
</ul>
<p>Choosing the shrinkage penalty:</p>
<ul>
<li>ML: cross-validation</li>
<li>Bayesian: a prior is placed on the shrinkage penalty, and it is estimated as part of the posterior.</li>
</ul>
</div>
<div id="examples-3" class="section level2">
<h2><span class="header-section-number">19.7</span> Examples</h2>

</div>
</div>



<h2>Prerequisites</h2>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)</a>
<a class="sourceLine" id="cb146-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;stringr&quot;</span>)</a>
<a class="sourceLine" id="cb146-3" data-line-number="3"><span class="kw">library</span>(<span class="st">&quot;bayz&quot;</span>)</a></code></pre></div>
</div>
<div id="parameters" class="section level2">
<h2><span class="header-section-number">19.8</span> Parameters</h2>
<table>
<thead>
<tr class="header">
<th align="left">Category</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">modeled data</td>
<td align="left">Data, assigned distribution</td>
</tr>
<tr class="even">
<td align="left">unmodeled data</td>
<td align="left">Data not given a distribution</td>
</tr>
<tr class="odd">
<td align="left">modeled parameters</td>
<td align="left">Parameters with an informative prior distribution</td>
</tr>
<tr class="even">
<td align="left">unmodeled parameters</td>
<td align="left">Parameters with non-informative prior distribution</td>
</tr>
<tr class="odd">
<td align="left">derived quantities</td>
<td align="left">Variables defined deterministicically</td>
</tr>
</tbody>
</table>
<p>See <span class="citation">A. Gelman and Hill (2007, 366)</span></p>
</div>
<div id="miscellaneous-mathematical-background" class="section level2">
<h2><span class="header-section-number">19.9</span> Miscellaneous Mathematical Background</h2>
<div id="location-scale-families" class="section level3">
<h3><span class="header-section-number">19.9.1</span> Location-Scale Families</h3>
<p>In a <a href="https://en.wikipedia.org/wiki/Location%E2%80%93scale_family">location-scale family</a> of distributions, if the random variable <span class="math inline">\(X\)</span> is distributed with mean 0 and standard deviation 1, then the random variable <span class="math inline">\(Y\)</span>,
<span class="math display">\[
Y = \mu + \sigma X ,
\]</span>
has mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p><strong>Normal distribution:</strong> Suppose <span class="math inline">\(X \sim \dnorm(0, 1)\)</span>, then
<span class="math display">\[
Y = \mu + \sigma X,
\]</span>
is equivalent to <span class="math inline">\(Y \sim \dnorm(\mu, \sigma)\)</span> (normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>).</p>
<p>** Student-t distribution** (including Cauchy):
<span class="math display">\[
\begin{aligned}[t]
X &amp;\sim \dt{\nu}(0, 1) \\
Y &amp;= \mu + \sigma X 
\end{aligned}
\]</span>
implies
<span class="math display">\[
Y \sim \dt{\nu}(\mu, \sigma),
\]</span>
i.e. <span class="math inline">\(Y\)</span> is distributed Student-<span class="math inline">\(t\)</span> with location <span class="math inline">\(\mu\)</span> and scale <span class="math inline">\(\sigma\)</span>.</p>
<p>In Stan, it can be useful parameterize distributions in terms of a mean 0, scale 1 parameters, and separate parameters for the locations and scales. E.g. with normal distributions,</p>
<pre><code>parameters {
  real mu;
  real&lt;lower = 0.0&gt; sigma;
  vector[n] eps;
}
transformed parameters {
  vector[n] y;
  y = mu + sigma * eps;
}
model {
  eps ~ normal(0.0, 1.0);
}</code></pre>
</div>
<div id="scale-mixtures-of-normal-distributions" class="section level3">
<h3><span class="header-section-number">19.9.2</span> Scale Mixtures of Normal Distributions</h3>
<p>Some commonly used distributions can be represented as scale mixtures of normal distributions.
For formal details of scale mixtures of normal distributions see <span class="citation">West (1987)</span>.
Distributions that are scale-mixtures of normal distributions can be written as,
<span class="math display">\[
Y \sim \dnorm(\mu, \sigma_i^2) \\
\sigma_i \sim \pi(\sigma_i)
\]</span>
As its name suggests, the individual variances (scales) themselves, have a distribution.</p>
<p>Some examples:</p>
<ul>
<li>Student-t</li>
<li>Double Exponential</li>
<li>Horseshoe or Hierarchical Shrinkage (HS)</li>
<li>Horseshoe Plus or Hierarchical Shrinkage Plus (HS+)</li>
</ul>
<p>Even when analytic forms of the distribution are available, representing them as scale mixtures of normal distributions may be convenient in modeling.
In particular, it may allow for drawing samples from the distribution easily.
And in HMC, it may induce a more tractable posterior density.</p>
</div>
<div id="covariance-correlation-matrix-decomposition" class="section level3">
<h3><span class="header-section-number">19.9.3</span> Covariance-Correlation Matrix Decomposition</h3>
<p>The suggested method for modeling covariance matrices in Stan is the separation strategy which decomposes a covariance matrix <span class="math inline">\(\Sigma\)</span> can be decomposed into a standard deviation vector <span class="math inline">\(\sigma\)</span>, and a correlation matrix <span class="math inline">\(R\)</span> <span class="citation">(Barnard, McCulloch, and Meng 2000)</span>,
<span class="math display">\[
\Sigma = \diag(\sigma) R \diag(\sigma) .
\]</span>
This is useful for setting priors on covariance because separate priors can be set
for the scales of the variables via <span class="math inline">\(\sigma\)</span>, and the correlation between them,
via <span class="math inline">\(R\)</span>.</p>
<p>The <a href="https://github.com/stan-dev/rstanarm/wiki/Prior-distributions">rstanarm</a> <code>decov</code> prior goes further and decomposes the covariance matrix into a correlation matrix, <span class="math inline">\(\mat{R}\)</span>,
a diagonal variance matrix <span class="math inline">\(\mat{\Omega}\)</span> with trace <span class="math inline">\(n \sigma^2\)</span>, a scalar global variance <span class="math inline">\(\sigma^2\)</span>, and a simplex <span class="math inline">\(\vec{\pi}\)</span> (proportion of total variance for each variable):
<span class="math display">\[
\begin{aligned}[t]
\mat{\Sigma} &amp;= \mat{\Omega} \mat{R}  \\
\diag(\mat{\Omega}) &amp;= n \vec{\pi} \sigma^2
\end{aligned}
\]</span>
Separate and interpretable priors can be put on <span class="math inline">\(\mat{R}\)</span>, <span class="math inline">\(\vec{\pi}\)</span>, and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The LKJ (Lewandowski, ) distribution is a distribution over correlation coefficients,
<span class="math display">\[
R \sim \dlkjcorr(\eta) ,
\]</span>
where
<span class="math display">\[
\dlkjcorr(\Sigma | \eta) \propto \det(\Sigma)^{(\eta - 1)} .
\]</span></p>
<p>This distribution has the following properties:</p>
<ul>
<li><span class="math inline">\(\eta = 1\)</span>: uniform correlations</li>
<li><span class="math inline">\(\eta \to \infty\)</span>: approaches the identity matrix</li>
<li><span class="math inline">\(0 &lt; \eta &lt; 1\)</span>: there is a trough at the identity matrix with higher probabilities placed on non-zero correlations.</li>
<li>For all positive <span class="math inline">\(\eta\)</span> (<span class="math inline">\(\eta &gt; 0\)</span>), <span class="math inline">\(\E(R) = \mat{I}\)</span>.</li>
</ul>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" data-line-number="1">lkjcorr_df &lt;-<span class="st"> </span><span class="cf">function</span>(eta, <span class="dt">n =</span> <span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb148-2" data-line-number="2">  out &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">rlkjcorr</span>(n, eta))</a>
<a class="sourceLine" id="cb148-3" data-line-number="3">  out<span class="op">$</span>.row &lt;-<span class="st"> </span><span class="kw">seq_len</span>(<span class="kw">nrow</span>(out))</a>
<a class="sourceLine" id="cb148-4" data-line-number="4">  out &lt;-<span class="st"> </span><span class="kw">gather</span>(out, .col, value, <span class="op">-</span>.row)</a>
<a class="sourceLine" id="cb148-5" data-line-number="5">  out<span class="op">$</span>.col &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">str_replace</span>(out<span class="op">$</span>.col, <span class="st">&quot;^V&quot;</span>, <span class="st">&quot;&quot;</span>))</a>
<a class="sourceLine" id="cb148-6" data-line-number="6">  out<span class="op">$</span>eta &lt;-<span class="st"> </span>eta</a>
<a class="sourceLine" id="cb148-7" data-line-number="7">  out  </a>
<a class="sourceLine" id="cb148-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb148-9" data-line-number="9"></a>
<a class="sourceLine" id="cb148-10" data-line-number="10">lkjsims &lt;-<span class="st"> </span>purrr<span class="op">::</span><span class="kw">map_df</span>(<span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">50</span>, <span class="dv">1000</span>), lkjcorr_df, <span class="dt">n =</span> <span class="dv">50</span>)</a></code></pre></div>
<p>This simulates a single matrix from the LKJ distribution with different values of <span class="math inline">\(\eta\)</span>.
As <span class="math inline">\(\eta \to \infty\)</span>, the off-diagonal correlations tend towards 0, and the correlation matrix to the identity matrix.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" data-line-number="1"><span class="kw">ggplot</span>(lkjsims,</a>
<a class="sourceLine" id="cb149-2" data-line-number="2">       <span class="kw">aes</span>(<span class="dt">x =</span> .row, <span class="dt">y =</span> .col, <span class="dt">fill =</span> value)) <span class="op">+</span></a>
<a class="sourceLine" id="cb149-3" data-line-number="3"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>eta, <span class="dt">ncol =</span> <span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb149-4" data-line-number="4"><span class="st">  </span><span class="kw">scale_fill_distiller</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">type =</span> <span class="st">&quot;div&quot;</span>, <span class="dt">palette =</span> <span class="st">&quot;RdYlBu&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb149-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb149-6" data-line-number="6"><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb149-7" data-line-number="7"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.grid =</span> <span class="kw">element_blank</span>(), <span class="dt">axis.text =</span> <span class="kw">element_blank</span>()) <span class="op">+</span></a>
<a class="sourceLine" id="cb149-8" data-line-number="8"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;&quot;</span>)</a></code></pre></div>
<p><img src="appendix_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The density of the off-diagonal correlations.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" data-line-number="1">lkjsims <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb150-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(.row <span class="op">&lt;</span><span class="st"> </span>.col) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb150-3" data-line-number="3"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">colour =</span> <span class="kw">factor</span>(eta))) <span class="op">+</span></a>
<a class="sourceLine" id="cb150-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_density</span>()</a></code></pre></div>
<p><img src="appendix_files/figure-html/unnamed-chunk-5-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>For other discussions of the LKJ correlation distribution, see these:</p>
<ul>
<li><a href="https://stats.stackexchange.com/questions/2746/how-to-efficiently-generate-random-positive-semidefinite-correlation-matrices/125017#125017" class="uri">https://stats.stackexchange.com/questions/2746/how-to-efficiently-generate-random-positive-semidefinite-correlation-matrices/125017#125017</a></li>
<li><a href="http://www.zinkov.com/posts/2015-06-09-where-priors-come-from/" class="uri">http://www.zinkov.com/posts/2015-06-09-where-priors-come-from/</a></li>
<li><a href="http://www.psychstatistics.com/2014/12/27/d-lkj-priors/" class="uri">http://www.psychstatistics.com/2014/12/27/d-lkj-priors/</a></li>
</ul>
</div>
<div id="qr-factorization" class="section level3">
<h3><span class="header-section-number">19.9.4</span> QR Factorization</h3>
<p>For a full-rank <span class="math inline">\(N \times K\)</span> matrix, the QR factorization is
<span class="math display">\[
\mat{X} = \mat{Q} \mat{R} 
\]</span>
where <span class="math inline">\(\mat{Q}\)</span> is an orthonormal matrix such that <span class="math inline">\(\mat{Q}\T \mat{Q}\)</span> and
<span class="math inline">\(\mat{R}\)</span> is an upper triangular matrix.</p>
<p>Stan function
<span class="citation">Stan Development Team (2016)</span> suggest writing it is
<span class="math display">\[
\begin{aligned}[t]
\mat{Q}^* = \mat{Q} \times \sqrt{N - 1} \\
\mat{R}^* = \frac{1}{\sqrt{N - 1}} \mat{R}
\end{aligned}
\]</span></p>
<p>This is used for solving linear model.</p>
<p>Suppose <span class="math inline">\(\vec{\beta}\)</span> is a <span class="math inline">\(K \times 1\)</span> vector, then
<span class="math display">\[
\vec{eta} = \mat{x} \vec{\beta} = \mat{Q} \mat{R} \vec{\beta} = \mat{Q}^* \mat{R}^* \vec{\beta} .
\]</span>
Suppose <span class="math inline">\(\mat{theta} = \mat{R}^* \vec{\beta}\)</span>, then <span class="math inline">\(\vec{eta} = \mat{Q}^* \mat{\theta}\)</span> and <span class="math inline">\(\vec{beta} = {\mat{R}^*}^{-1} \mat{\theta}\)</span>.</p>
<p><a href="https://cran.r-project.org/web/packages/rstanarm/vignettes/lm.html">rstanarm</a> provides a prior for a normal linear model which uses the QR decomposition to parameterize a prior in terms of <span class="math inline">\(R^2\)</span>.</p>
<p>Stan functions:</p>
<ul>
<li><code>qr_Q(matrix A)</code></li>
<li><code>qr_R(matrix A)</code></li>
</ul>
<p>See <span class="citation">Stan Development Team (2016 Sec 8.2)</span></p>
</div>
<div id="cholesky-decomposition" class="section level3">
<h3><span class="header-section-number">19.9.5</span> Cholesky Decomposition</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a> of a
positive definite matrix <span class="math inline">\(A\)</span> is,
<span class="math display">\[
\mat{A} = \mat{L} \mat{L}\T ,
\]</span>
where <span class="math inline">\(\mat{L}\)</span> is a lower-triangular matrix.</p>
<ul>
<li>It is similar to a square root for a matrix.</li>
<li><p>It often more numerically stable or efficient to work with the Cholesky decomposition, than with
a covariance matrix. When working with the covariance matrix, numerical precision can
result in a non positive definite matrix. However, working with <span class="math inline">\(\mat{L}\)</span> will ensure
that <span class="math inline">\(\mat{A} = \mat{L} \mat{L}\T\)</span> will be positive definite.</p></li>
<li><p>In Stan</p>
<ul>
<li>Types types <code>cholesky_factor_cov</code>, and <code>cholesky_factor_corr</code> represent the Cholesky factor
of covariance and correlation matrices, respectively.</li>
<li>Cholesky decomposition function is <code>cholesky_decompose(matrix A)</code></li>
</ul></li>
<li><p>Multiple functions in Stan are parameterized with Cholesky decompositions instead of or in addition
to covariance matrices. Use them if possible; they are more numerically stable.</p>
<ul>
<li><code>lkj_corr_chol_lpdf</code></li>
<li><code>multi_normal_cholesky_lpdf</code></li>
</ul></li>
</ul>
<p>The Cholesky factor is used for sampling from a multivariate normal distribution using i.i.d. standard normal distributions.
Suppose <span class="math inline">\(X_1, \dots, X_N\)</span> are <span class="math inline">\(N\)</span> i.i.d. standard normal distributions, <span class="math inline">\(\mat{\Omega}\)</span> is an <span class="math inline">\(N \times N\)</span> lower-triangular matrix such that <span class="math inline">\(\mat{\Omega} \mat{Omega}\T = \mat{\Sigma}\)</span>, and <span class="math inline">\(\mu\)</span> is an <span class="math inline">\(N \times 1\)</span> vector, then
<span class="math display">\[
\vec{\mu} + \mat{\Omega} X \sim \dnorm(\vec{\mu}, \mat{\Sigma})
\]</span></p>
<p>See <span class="citation">Stan Development Team (2016, 40, 147, 241, 246)</span></p>
</div>
</div>
<div id="scaled-and-unscaled-variables" class="section level2">
<h2><span class="header-section-number">19.10</span> Scaled and Unscaled Variables</h2>
<p>Though priors shouldn’t depend on the data itself, many priors depends</p>
<p>Suppose <span class="math inline">\(\tilde{Y}\)</span>, <span class="math inline">\(\tilde{X}\)</span>, <span class="math inline">\(\tilde{\alpha}\)</span>, <span class="math inline">\(\tilde{\beta}\)</span>, and <span class="math inline">\(\epsilon\)</span> are random variables, such that
<span class="math display">\[
\tilde{Y} = \tilde{\alpha} + \tilde{\beta} \tilde{X} + \epsilon .
\]</span>
These random variables have the following properties,
<span class="math display">\[
\begin{aligned}[t]
\tilde{Y} &amp;= \frac{Y - \bar{Y}}{\sigma_Y}, &amp; \E[\tilde{Y}] &amp;= 0, &amp; \sigma_Y^2 &amp;= \Var[\tilde{Y}] = 1 \\
\tilde{X} &amp;= \frac{X - \bar{X}}{\sigma_X}, &amp;  \E[\tilde{X}] &amp;= 0, &amp; \sigma_X^2 &amp;= \Var[\tilde{X}] = 1 , \\
&amp;&amp; \E[\epsilon] &amp;= 0 &amp; \sigma_{\tilde{\epsilon}}^2 &amp;= \Var[\tilde{\epsilon}]
\end{aligned}
\]</span>
where
<span class="math display">\[
\begin{aligned}[t]
\bar{X} &amp;= \E[X] , &amp; s_X^2 &amp;= \Var[X] , \\
\bar{Y} &amp;= \E[Y] , &amp; s_Y^2 &amp;= \Var[Y] . 
\end{aligned}
\]</span></p>
<p>Then via some algebra,
<span class="math display">\[
\begin{aligned}
Y &amp;= \underbrace{\sigma_{Y} \tilde{\alpha} + \bar{Y} - \frac{\sigma_Y }{\sigma_X} \tilde{\beta} \bar{X}}_{\alpha} + 
\underbrace{\frac{\sigma_Y}{\sigma_X} \tilde{\beta}}_{\beta} X + \underbrace{\sigma_Y \tilde{\epsilon}}_{\epsilon} \\ 
  &amp;= \alpha + \beta X + \epsilon .
\end{aligned}
\]</span>
The primary relationships of interest are those between <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\tilde{\alpha}\)</span>, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\tilde{\beta}\)</span>, and <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\tilde{\epsilon}\)</span>.
These can be used to convert between coefficients estimated with standardized data to the coefficients on the data scale, or to adjust scale-free weakly informative priors to the data scale.
<span class="math display">\[
\begin{aligned}[t]
\tilde{\alpha} &amp;= \sigma_Y^{-1}\left(\alpha - \bar{Y} + \beta \bar{X} \right) \\
&amp;= \sigma_Y^{-1}\left(\alpha - \bar{Y} + \frac{\sigma_Y}{\sigma_X} \tilde{\beta} \bar{X} \right), \\
\alpha &amp;= \sigma_Y \tilde{\alpha} + \bar{Y} - \frac{\sigma_Y}{\sigma_X} \tilde{\beta} \bar{X} \\
       &amp;= \sigma_Y \tilde{\alpha} + \bar{Y} - \beta \bar{X} ,  \\
\tilde{\beta} &amp;= \frac{\sigma_X}{\sigma_Y} \beta , \\
\beta &amp;= \frac{\sigma_Y}{\sigma_X} \tilde{\beta} , \\
\tilde{\epsilon} &amp;= \epsilon / \sigma_Y , \\
\epsilon &amp;= \sigma_Y \tilde{\epsilon} .
\end{aligned}
\]</span>
This implies the following relationships between their means and variances,
<span class="math display">\[
\begin{aligned}[t]
E(\alpha) &amp;= \sigma_{Y} E(\tilde{\alpha}) + \bar{Y} - \frac{\sigma_Y}{\sigma_X} E(\tilde{\beta}) \bar{X} , &amp;
V(\alpha) &amp;= \sigma_{Y}^2 V(\tilde{\alpha}) + \frac{\sigma_Y^2}{\sigma_X^2} V(\tilde{\beta}) \bar{X}^2 , \\
 &amp;= \sigma_{Y} E(\tilde{\alpha}) + \bar{Y} - \bar{X} E(\beta) , &amp;
 &amp;= \sigma_{Y}^2 V(\tilde{\alpha}) + \bar{X}^2 V(\beta) , \\
E(\tilde{\alpha}) &amp;= \frac{E(\alpha) - \bar{Y} + E(\beta) \bar{X} }{\sigma_Y} , &amp;
V(\tilde{\alpha}) &amp;= \sigma_Y^{-2} \left[ V(\alpha) + \bar{X}^2 V(\beta) \right]
\end{aligned}
\]</span>
For example, a weakly informative prior on <span class="math inline">\(\tilde{\alpha}\)</span> implies a prior on <span class="math inline">\(\alpha\)</span>,
<span class="math display">\[
\tilde{\alpha} \sim N(0, 10^2) \to \alpha \sim N \left( \frac{\beta \bar{X} - \bar{Y}}{\sigma_Y}, \sigma_Y^2 10^2 \right) .
\]</span></p>
<p><span class="math display">\[
\begin{aligned}[t]
E(\beta) &amp;= \frac{\sigma_Y}{\sigma_X} E(\tilde{\beta})  , &amp;
V(\beta) &amp;= \frac{\sigma_Y^2}{\sigma_X^2} V(\tilde{\beta})  , \\
E(\tilde{\beta}) &amp;= \frac{\sigma_X}{\sigma_Y} E(\beta)  , &amp;
V(\tilde{\beta}) &amp;= \frac{\sigma_X^2}{\sigma_Y^2} V(\beta) . 
\end{aligned}
\]</span></p>
<p>For example, a weakly informative prior on <span class="math inline">\(\tilde{\beta}\)</span> implies the following prior on <span class="math inline">\(\beta\)</span>,
<span class="math display">\[
\tilde{\beta} \sim N(0, 2.5^2) \to \beta \sim N\left(0, \frac{\sigma_Y^2}{\sigma_X^2} 2.5^2 \right) .
\]</span></p>
<p><span class="math display">\[
\begin{aligned}[t]
E(\epsilon) &amp;= 0 , &amp; V(\epsilon) &amp;= \sigma_Y^2 V(\tilde{\epsilon}), \\
E(\tilde{\epsilon}) &amp;= 0 , &amp; V(\tilde{\epsilon}) &amp;= \sigma_Y^{-2}  V(\epsilon) .
\end{aligned}
\]</span>
For example, a weakly informative prior on the variance of <span class="math inline">\(\tilde{\epsilon}\)</span> implies a weakly informative prior on the variance of <span class="math inline">\(\epsilon\)</span>,
<span class="math display">\[
\sigma_{\tilde{\epsilon}} \sim C^{+}\left(0, 5 \right) \to 
\sigma_{\epsilon} \sim C^{+}\left(0, 5 \sigma_Y \right) .
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shrinkage-and-hierarchical-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/bayesian_notes/edit/master/regression-shrinkage.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

---
title: Regression Shrinkage
---


# Lasso and Ridge Regression

```{r}
data("diabetes", package = "lars")
```

```{r}
fit_lasso <- glmnet(diabetes$x, diabetes$y, alpha = 1)
```
```{r}
plot(fit_lasso)
```

Choice of an optimal lambda:
```{r}
fit_lasso_cv <- cv.glmnet(diabetes$x, diabetes$y, parallel = TRUE)
```

```{r}
ggplot(tidy(fit_lasso_cv), aes(x = -log(lambda), y = estimate)) +
  geom_point()
```

```{r}
cv.glmnet(diabetes$x, diabetes$y, parallel = TRUE)
```



```{r}
fit_ridge <- glmnet(diabetes$x, diabetes$y, alpha = 0)
```
```{r}
plot(fit_ridge)
```

Comparison of coefficients
```{r}
beta_lm <- lm(y ~ x, data = diabetes)
```

## Bayesian Shrinkage

### Priors

Consider the regression:
$$
y_i \sim \dnorm(\alpha + x_i' \beta, \sigma)
$$

It is assumed that the outcome and predictor variables are standardized such that
$$
\begin{aligned}[t]
\E[y_i] &= 0 & \V[y_i] &= 1 \\
\E[x_i] &= 0 & \V[x_i] &= 1. 
\end{aligned}
$$
Then, the default weakly informative priors are,
$$
\begin{aligned}[t]
\alpha &\sim \dnorm(0, 10) \\
\beta_k &\sim \dnorm(0, 2.5) & k \in \{1, \dots, K\}
\end{aligned}
$$

The weakly informative priors will shrink all the coefficients towards zero.

The amount of shrinkage depends on the amount of data as the likelihood dominates the prior as the amount of data increases.
However, the amount of shrinkage is not estimated from the data.
The prior on each coefficient is independent, and its scale is a constant (2.5 in this example).

Regularization/shrinkage methods estimate the amount of shrinkage. The scale of the priors on the coefficients are hyperparameters, which are estimated from the data.

### Hiearchical Coefficient Priors

#### Normal Distribution

We can apply a normal prior to each $\beta_k$. 
Unlike the weakly informative priors, the prior distributions all share a scale parameter $\tau$.
$$
\beta_k | \tau \sim \dnorm(0, \tau)
$$
We need to assign a prior to $\tau$.

In MAP estimation this is often set to be an inmproper uniform distribution.
Since in the weakly informative prior, $\beta_k \sim \dnorm(0, 2.5$ for all $k$, a prior on $\tau$ in which the central tendency is the same as the weakly informative prior makes sense.
One such prior is
$$
\tau \sim \dexp(2.5)
$$
TODO: look for better guidance for the prior of $\tau$.

-   This is equivalent to Ridge regression.
-   Unlike most shrinkage estimators, there is a closed form solution to the posterior distribution

### Laplace Distribution

We can also use the Laplace distribution as a prior for the coefficients.
This is called Bayesian Lasso, because the MAP estimator of this model is equivalent to the Lasso estimator.

The prior distribution for each coefficient $\beta_k$ is a Laplace (or double exponential) distribution with scale parameter ($\tau$).
$$
\beta_k | \tau \sim \dlaplace(0, \tau)
$$
Like many priors that have been proposed and used for coefficient shrinkage, this can be represented as a local-global scale-mixture of normal distributions.
$$
\begin{aligned}
\beta_k | \tau &\sim \dnorm(0, \tau \lambda_k) \\
\lambda_k^{-2} &\sim \dexp(1/2)
\end{aligned}
$$
The global scale $\tau$ determines the overall amount of shrinkage.
The local scales, $\lambda_1, \dots, \lambda_K$, allow the amount of shrinkage to vary among coefficients.

## Student-t and Cauchy Distributions

We can also use the Student-t distribution as a prior for the coefficients.
The Cauchy distribution is a special case of the Student t distribution where the degrees of freedom is zero.

The prior distribution for each coefficient $\beta_k$ is a Student-t distribution with degrees of freedom $\nu$, location 0, and scale $\tau$,
$$
\beta_k | \tau \sim \dt(\nu, 0, \tau) .
$$
Like many priors that have been proposed and used for coefficient shrinkage, this can be represented as a local-global scale-mixture of normal distributions.
$$
\begin{aligned}
\beta_k | \tau, \lambda &\sim \dnorm(0, \tau \lambda_k) \\
\lambda_k^{-2} &\sim \dgamma(\nu/2, \nu/2)
\end{aligned}
$$

The degrees of freedom parameter $\nu$ can be fixed to a particular value or estimated.
If fixed, then common values are 1 for a Cauchy distribution, 2 to ensure that there is a finite mean, 3 to ensure that there is a finite variance, and 4 ensure that there is a finite kurtosis.

If estimated, then the 
$$
\nu \sim \dgamma(2, 0.1)
$$
Additionally, it may be useful to truncate the values of $\nu$ to be greater
than 2 to ensure a finite variance of the Student t distribution.

## Horseshore and Hierarchical Priors

The Horseshoe prior is defined solely in terms of a global-local mixture.
$$
\begin{aligned}
\beta_k | \tau, \lambda &\sim \dnorm(0, \tau \lambda_k) \\
\lambda_k &\sim \dhalfcauchy(1)
\end{aligned}
$$

The Hierarchical Shrinkage prior originally implemented in rstanarm and proposed by ... replaces the half-Cauchy prior on $\lambda_k$ with a half-Student-t distribution with degrees of freedom $\nu$.
$$
\lambda_k \sim \dt(\nu, 0, 1)
$$
The $\nu$ parameter is generally not estimated and fixed to a low value, with $\nu = 4$ being suggested.
The problem with estimating the Horseshoe prior is that the wide tails of the Cauchy prior produced a posterior distribution with problematic geometry that was hard to sample.
Increasing the degrees of freedom helped to regularize the posterior.
The downside of this method is that by increasing the degrees of freedom of the Student-t distribution it would also shrink large parameters, which the 
Horseshoe prior was designed to avoid.

Regularized horseshoe prior
$$
\begin{aligned}
\beta_k | \tau, \lambda &\sim \dnorm(0, \tau \tilde{\lambda}_k) \\
\tilde{\lambda}^2_k &= \frac{c^2 \lambda^2}{c^2 + \lambda^2} \\
\lambda_k &\sim \dhalfcauchy(1)
\end{aligned}
$$
where $c > 0$ is a constant.
Like using a Student-t distribution, this regularizes the posterior distribution of a Horseshoe prior.
However, it is less problematic in terms of shrinking large coefficients.

## Understanding Shrinkage Models

Two-group model



$$
\begin{aligned}[t]
p(\beta | \Lambda, \tau, \sigma^2, D) &= \dnorm(\beta | \bar{\beta}, \Sigma) \\
\bar{\beta} &= \tau^2 \Lambda (\tau^2 \Lambda + \sigma^2 (X'X)^{-1})^{-1} \hat{\beta} \\
\Sigma &= (\tau^{-2} \Lambda^{-1} + \frac{1}{\sigma^{2}} X'X)^{-1} \\
\Lambda &= \diag(\lambda_1^{2}, \dots, \lambda^{2}_D) \\
\hat{\beta} &= (X'X)^{-1} X'y
\end{aligned}
$$
If the predictors are uncorrelated with zero mean and variances $\Var(x_j) = s_j^2$, then
$$
X'X \approx n \diag(s_1^2, \dots, s^2_D) ,
$$
and we can use the approximations,
$$
\hat{\beta}_j = (1 - \kappa_j) \hat{\beta}_j \\
\kappa_j = \frac{1}{1 + n \sigma^{-2} \tau^2 s_j^2 \lambda_j^2}
$$

## Spike and Slab prior

$$
\begin{aligned}[t]
\beta_k | \lambda_k, c, \epsilon  &\sim \lambda_k N(0, c^2) + (1 - \lambda_j) N(0, \epsilon^2) \\
\lambda_k &\sim \dbern(\pi)
\end{aligned}
$$

In the case of the linear regression, an alternative to BMA is to use a
spike-and-slab prior [@MitchellBeauchamp1988a, @GeorgeMcCulloch1993a, @IshwaranRao2005a],
which is a prior that is a discrete mixture of a point mass at 0 and a
non-informative distribution.

The spike and slab prior is a "two-group" solution

$$
p(\beta_k) = (1 - w) \delta_0 + w \pi(\beta_k)
$$
where $\delta_0$ is a Dirac delta function putting a point mass at 0, and $\pi(\beta_k)$ is an uninformative distribution, e.g. $\pi(\beta_k) = \dnorm(\beta_k | 0, \sigma^2)$ where $\sigma$ is large.

The posterior distribution of $w$ is the probability that $\beta_k \neq 0$, and the conditional posterior distribution $p(\beta_k | y, w = 1)$ is the distribution of $\beta_k$ given that $\beta_k \neq 0$.

See the R package `r rpkg("spikeslab")` and he accompanying article [@IshwaranKogalurRao2010a] for an implementation and review of spike-and-slab regressions.


## Number of effective zeros


## Choice of Hyperparameter on $\tau$

The value of $\tau$ and the choice of its hyper-parameter has a big influence on the sparsity of the coefficients.

@CarvalhoPolsonScott2009a suggest
$$
\tau \sim \dhalfcauchy(0, \sigma),
$$
while @PolsonScott2011a suggest,
$$
\tau \sim \dhalfcauchy(0, 1) .
$$

@PasKleijnVaart2014a suggest
$$
\tau \sim \dhalfcauchy(0, p^{*} / n)
$$
where $p^*$ is the true number of non-zero parameters,
and $n$ is the number of observations.
They suggest $\tau = p^{*} / n$ or $\tau p^{*}  / n \sqrt{log(n / p^{*})}$.
Additionally, they suggest restricting $\tau$ to $[0, 1]$.

@PiironenVehtari2016a understand the choice of the prior on $\tau$ as the implied prior on the number of effective parameters.
The shrinkage can be understood as its influence on the number of effective parameters, $m_{eff}$,
$$
m_{eff} = \sum_{j = 1}^K (1 - \kappa_j) .
$$
This is a measure of effective model size.

The mean and variance of $m_{eff}$ given $\tau$ and $\sigma$ are,
$$
\begin{aligned}[t]
\E[m_{eff} | \tau, \sigma] &= \frac{\sigma^{-1} \tau \sqrt{n}}{1 + \sigma^{-1} \tau \sqrt{n}} K , \\
\Var[m_{eff} | \tau, \sigma] &= \frac{\sigma^{-1} \tau \sqrt{n}}{2 (1 + \sigma^{-1} \tau \sqrt{n})2} K .
\end{aligned}
$$

Based on this, a prior should be chosen so that the prior mass is located near,
$$
\tau_0 = \frac{p_0}{K - p_0}\frac{\sigma}{\sqrt{n}}
$$

Densities of the shrinkage parameter, $\kappa$, for various shrinkage distributions where $\sigma^2 = 1$, $\tau = 1$, for $n = 1$.

@DattaGhosh2013a warn against empirical Bayes estimators of $\tau$ for the horseshoe prior as it can collapse to 0.
@ScottBerger2010a consider marginal maximum likelihood estimates of $\tau$.
@PasKleijnVaart2014a suggest that an empirical Bayes estimator truncated below at $1 / n$.

```{r echo=FALSE}
library("tidyverse")
kappa <- seq(.005, .995, by = 0.005)
lambda <- (1 - kappa) / kappa

f <- function(x) sqrt(1 / x - 1)
f_jacobian <- function(x) 1 / (sqrt(1 / x - 1) * x ^ 2)
f2 <- function(x) 1 / x - 1
f2_jacobian <- function(x) x ^ (-2)

funs <- list(
  function(x) {
    tibble(kappa = x,
           dens = dt(f(kappa), df = 3) * f_jacobian(kappa),
           name = "HS (df = 3)")
  },
  function(x) {
    tibble(kappa = x,
           dens = dt(f(kappa), df = 2) * f_jacobian(kappa),
           name = "HS (df = 2)")
  },
  function(x) {
    tibble(kappa = x,
           dens = dcauchy(f(kappa)) * f_jacobian(kappa),
           name = "HS (df = 1)")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 0.5, 0.5) *
             (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Student t (df = 1)")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 3 / 2, 3 / 2) *
             (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Student t (df = 3)")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 1000, 1000) *
             (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Normal")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 0.0001, 0.0001) *
             (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Student t (df = 0)")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dexp(f2(kappa), 0.5) * f2_jacobian(kappa),
           name = "Double Exponential")
  }
)

shrinkages <- invoke_map_df(funs, x = kappa)

ggplot(shrinkages, aes(x = kappa, y = dens)) +
  geom_line() +
  facet_wrap(~ name, scales = "free_y")
```



## All Coefficients

TODO

### Zellner's g-prior

An alternative prior is the Zellner's g-prior.
Consider the regression,
$$
y_i | \alpha, \vec{\beta}, \sigma \sim \dnorm(\alpha + \mat{X} \vec{\beta}, \sigma^2)
$$
The $g$-prior is a non-informative, data-dependent prior,
$$
\vec{\beta} \sim \dnorm(0, \sigma^2 g \mat{X}\T \mat{X})
$$
It depends on only a single parameter $g$.
The prior for $g$ must be proper. Some common choices include,
$$
\begin{aligned}
g &= n \\
g &= k^2 \\
g &= \max(n, k^2)
\end{aligned}
$$
or putting a hyperprior on $g$.

See @LeySteel2012a for a recent overview of g-priors.



### Q-R Prior



## Issues

-   Other uses 

    -   Basis functions
    -   



## References

-   `r rpkg("rstanarm")`: estimates GLM regressions with various priors
-   `r rpkg("rmonomvn")`: estimates Bayesian ridge, lasso, horseshoe, and ridge regression.
-   `r rpkg("bayesreg")`: See @MakalicSchmidt2016a for documentation and a good review of Bayesian regularized regression.
-   [fastHorseshoe]( http://jingyuhe.com/fastHorseshoe.html)

---
output: html_document
editor_options:
  chunk_output_type: console
---
# Robust Regression

## Prerequisites {-}

```{r setup,message=FALSE}
library("rstan")
library("tidyverse")
library("rstanarm")
library("bayz")
library("jrnold.bayes.notes")
```

## Wide Tailed Distributions

Like OLS, Bayesian linear regression with normally distributed errors is
sensitive to outliers.
This is because the normal distribution has narrow tail probabilities,
with approximately 99.8% of the probability within three standard deviations.

[Robust regression](https://en.wikipedia.org/wiki/Robust_regression) refers to regression methods which are less sensitive to outliers.
Bayesian robust regression uses distributions with wider tails than the normal instead of the normal.
This plots the normal, Double Exponential (Laplace), and Student-t ($df = 4$)
distributions all with mean 0 and scale 1, and the surprise ($- log(p)$) at each point.
Both the Student-$t$ and Double Exponential distributions have surprise values well below the normal in the ranges (-6, 6). [^tailareas]
This means that outliers will have less of an affect on the log-posterior of models using these distributions.
The regression line would need to move less  incorporate those observations since the error distribution will not consider them as unusual.

```{r echo=FALSE}
z <- seq(-6, 6, length.out = 100)
bind_rows(
  tibble(z = z,
         p = dnorm(z, 0, 1),
         distr = "Normal"),
  tibble(z = z,
         p = dt(z, 4),
         distr = "Student-t (df = 4)"),
  tibble(z = z,
         p = VGAM::dlaplace(z, 0, 1),
         distr = "Double Exponential")) %>%
  mutate(`-log(p)` = -log(p)) %>%
  ggplot(aes(x = z, y = `-log(p)`, colour = distr)) +
  geom_line()
```

```{r echo=FALSE}
z <- seq(-6, 6, length.out = 100)
bind_rows(
  tibble(z = z,
         p = dnorm(z, 0, 1),
         distr = "Normal"),
  tibble(z = z,
         p = dt(z, 4),
         distr = "Student-t (df = 4)"),
  tibble(z = z,
         p = VGAM::dlaplace(z, 0, 1),
         distr = "Double Exponential")) %>%
  mutate(`-log(p)` = -log(p)) %>%
  ggplot(aes(x = z, y = p, colour = distr)) +
  geom_line()
```

## Student-t distribution

The most commonly used Bayesian model for robust regression is a linear regression with independent Student-$t$ errors [@Geweke1993; @BDA3, Ch. 17]:
$$
y_i \sim \dt\left(\nu, \mu_i, \sigma \right)
$$
where $\nu \in \R^{+}$ is a degrees of freedom parameter, $\mu_i \in \R$ are observation specific locations often modeled with a regression, and and $\sigma \in R^{+}$ is a
the scale parameter.

Note that as $\nu \to \infty$, this model approaches an independent normal model, since
the Student-t distribution asymptotically approaches the normal distribution as the degrees of freedom increases.
For the value of $\nu$, either a low degrees of freedom $\nu \in (4, 6)$ can be used, or
it can be given a prior distribution.
For the Student-t distribution, the existence of various moments depends on the value of $\nu$: the mean exists for $\nu > 1$, variance for $\nu > 2$, and kurtosis for $\nu > 3$.
As such, it is often useful to restrict the support of $\nu$ to at least 1 or 2 (or even higher) ensure the existence of a mean or variance.

A reasonable prior distribution for the degrees of freedom parameter is a Gamma
distribution with shape parameter 2, and an inverse-scale (rate) parameter of 0.1 [@JuarezSteel2010a,@Stan-prior-choices],
$$
\nu \sim \dgamma(2, 0.1) .
$$
```{r echo=FALSE}
tibble(x = 0:50,
       y = dgamma(x, shape = 2, rate = 0.1)) %>%
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "Density", x = expression(nu))
```
This density places the majority of the prior mass for values $\nu < 50$, in which 
the Student-$t$ distribution is substantively different from the Normal distribution,
and also allows for all prior moments to exist.

The Stan model that estimates this is `lm_student_t_1.stan`:
```{r echo=FALSE}
print_stanmodel("stan/lm_student_t_1.stan")
```

As noted in [Heteroskedasticity], the Student-t distribution can be represented as a 
scale-mixture of normal distributions, where the inverse-variances (precisions) follow 
a Gamma distribution,
$$
\begin{aligned}[t]
y_i &\sim \dnorm\left(\mu_i, \omega^2 \lambda_i^2 \right) \\
\lambda^{-2} &\sim \dgamma\left(\nu / 2, \nu / 2\right)
\end{aligned}
$$
The scale mixture distribution of normal parameterization of the Student t distribution is useful for computational reasons.
A Stan model that implements this scale mixture of normal distribution representation of the Student-t distribution is `lm_student_t_2.stan`:
```{r echo=FALSE}
print_stanmodel("stan/lm_student_t_2.stan")
```

Another reparameterization of these models that is useful computationally is 
The variance of the Student-t distribution is a function of the scale and the degree-of-freedom parameters. 
Suppose $X \sim \dt(\nu, \mu, \sigma)$, then
$$
\Var(X) = \frac{\nu}{\nu - 2} \sigma^2.
$$
So variance of data can be fit better by *either* increasing $\nu$ or increasing the scale $\sigma$.
This will create posterior correlations between the parameters, and make it more difficult to sample the posterior distribution.
We can reparameterize the model to make $\sigma$ and $\nu$ less correlated by multiplying the scale by the degrees of freedom. 
$$
\begin{aligned}
y_i \sim \dt\left(\nu, \mu_i, \sigma \sqrt{\frac{\nu - 2}{\nu}} \right)
\end{aligned}
$$
In this model, changing the value of $\nu$ has no effect on the variance of $y$, since
$$
\Var(y_i) = \frac{\nu}{\nu - 2} \sigma^2 \frac{\nu - 2}{\nu} = \sigma^2 .
$$

### Examples

Estimate some examples with known outliers and compare to using a normal
See the data examples `income_ineq`, `unionization`, and `econ_growth` in the
associated **jrnold.bayes.notes** package.

## Robit

The "robit" is a "robust" bivariate model.[@GelmanHill2007a, p. 125; @Liu2005a]
For the link-function the robit uses the CDF of the Student-t distribution with $d$ degrees of freedom.
$$
\begin{aligned}[t]
y_i &\sim \dbin \left(n_i, \pi_i \right) \\
\pi_i &= \int_{-\infty}^{\eta_i} \mathsf{StudentT}(x | \nu, 0, (\nu - 2)/ \nu) dx \\
\eta_i &= \alpha + X \beta
\end{aligned}
$$
Since the variance of a random variable distributed Student-$t$ is $d / d - 2$, the scale fixes the variance of the distribution at 1.
Fixing the variance of the Student-$t$ distribution is not necessary if $d$ is fixed, but is necessary if $d$ were modeled as a parameter.
Where $\nu$ is given a low degrees of freedom $\nu \in [3, 7]$, or a prior distribution.

## Quantile regression

A different form of robust regression and one that often serves a different purpose is quantile regression.

[Least absolute deviation](https://en.wikipedia.org/wiki/Least_absolute_deviations) (LAD) regression minimizes the following objective function,
$$
\hat{\beta}_{LAD} = \arg \min_{\beta} \sum | y_i - \alpha - X \beta | .
$$
The Bayesian analog is the [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution),
$$
\dlaplace(x | \mu, \sigma) = \frac{1}{2 \sigma} \left( - \frac{|x - \mu|}{\sigma} \right) .
$$
The Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is $|x - \mu|$, so minimizing the likelihood will also minimize the least absolute distances.

Thus, a linear regression with Laplace errors is analogous to a median regression.
$$
\begin{aligned}[t]
y_i &\sim \dlaplace\left( \alpha + X \beta, \sigma \right)
\end{aligned}
$$
This can be generalized to other quantiles using the asymmetric Laplace distribution [@BenoitPoel2017a, @YuZhang2005a].

### Questions

1.  OLS is a model of the conditional mean $E(y | x)$. A linear model with
    normal errors is a model of the outcomes $p(y | x)$. How would you estimate
    the conditional mean, median, and quantile functions from the linear-normal
    model? What role would quantile regression play? Hint: See @BenoitPoel2017a [Sec. 3.4].

1.  Implement the asymmetric Laplace distribution in Stan in two ways:

    -   Write a user function to calculate the log-PDF
    -   Implement it as a scale-mixture of normal distributions

## References

For more on robust regression see @GelmanHill2007a [sec 6.6], @BDA3 [ch 17], and @Stan2016a [Sec 8.4].

For more on heteroskedasticity see @BDA3 [Sec. 14.7] for models with unequal variances and correlations.
@Stan2016a discusses reparameterizing the Student t distribution as a mixture of gamma distributions in Stan.

[^tailareas]: The Double Exponential distribution still has a thinner tail than the Student-t at higher values.

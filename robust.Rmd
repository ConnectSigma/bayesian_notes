---
output: html_document
editor_options:
  chunk_output_type: console
---
# Heteroskedasticity and Robust Regression

## Prerequisites {-}

```{r setup,message=FALSE}
library("rstan")
library("tidyverse")
library("rubbish")
```

## Linear Regression with Student t distributed errors

Like OLS, Bayesian linear regression with normally distributed errors is
sensitive to outliers. 
This is because the normal distribution has narrow tail probabilities, 
with 99.8% of the probability within three standard deviations.
Thus, if we estimate 

This plots the normal, Double Exponential (Laplace), and Student-t ($df = 4$)
distributions all with mean 0 and scale 1, and the surprise ($- log(p)$) at each point.
Higher surprise is a lower log-likelihood. Both the Student-t and Double
Exponential distributions have surprise values well below the normal in the ranges (-6, 6). [^tailareas]
This means that outliers impose less of a penalty on the log-posterior models
using these distributions, and the regression line would need to move less to
incorporate those observations since the error distribution will not consider them as unusual.

```{r}
z <- seq(-6, 6, length.out = 100)
bind_rows(
  tibble(z = z,
         p = dnorm(z, 0, 1),
         distr = "Normal"),
  tibble(z = z,
         p = dt(z, 4),
         distr = "Student-t (df = 4)"),
  tibble(z = z,
         p = VGAM::dlaplace(z, 0, 1),
         distr = "Double Exponential")) %>%
  mutate(`-log(p)` = -log(p)) %>%
  ggplot(aes(x = z, y = `-log(p)`, colour = distr)) +
  geom_line()
```

```{r}
z <- seq(-6, 6, length.out = 100)
bind_rows(
  tibble(z = z,
         p = dnorm(z, 0, 1),
         distr = "Normal"),
  tibble(z = z,
         p = dt(z, 4),
         distr = "Student-t (df = 4)"),
  tibble(z = z,
         p = VGAM::dlaplace(z, 0, 1),
         distr = "Double Exponential")) %>%
  mutate(`-log(p)` = -log(p)) %>%
  ggplot(aes(x = z, y = p, colour = distr)) +
  geom_line()

```

[^tailareas]: The Double Exponential distribution still has a thinner tail than the Student-t at higher values.

```{r include=FALSE}
mod_t <- stan_model("stan/lm_student_t.stan")
```
```{r cache=FALSE}
mod_t
```

```{r}
unionization <- read_tsv("data/western1995/unionization.tsv",
         col_types = cols(
              country = col_character(),
              union_density = col_double(),
              left_government = col_double(),
              labor_force_size = col_number(),
              econ_conc = col_double()
            ))
mod_data <-
  lm_preprocess(union_density ~ left_government +
                  log(labor_force_size) + econ_conc,
                data = unionization)

mod_data <- within(mod_data, {
  b_loc <- 0
  b_scale <- 1000
  sigma_scale <- sd(y)
})

```

The `max_treedepth` parameter needed to be increased because in some runs it was hitting the maximum tree depth.
This is likely due to the wide tails of the Student t distribution.
```{r}
mod_t_fit <- sampling(mod_t, data = mod_data,
                      control = list(max_treedepth = 11))
```

```{r output=FALSE}
summary(mod_t_fit, pars = c("b"))$summary
```

Compare those results when using a model with
```{r include=FALSE}
mod_normal <- stan_model("stan/lm.stan")
```
```{r}
mod_normal
```

```{r}
mod_normal_fit <- sampling(mod_normal, data = mod_data)
```

```{r}
summary(mod_normal_fit, pars = c("b"))$summary
```



## References

For more on robust regression see @GelmanHill2007a [sec 6.6], @BDA3 [ch 17], and @Stan2016a [Sec 8.4].

For more on heteroskedasticity see @BDA3 [Sec. 14.7] for models with unequal variances and correlations.
@Stan2016a discusses reparameterizing the Student t distribution as a mixture of gamma distributions in Stan.

### Quantile regression

-   @BenoitPoel2017a
-   @YuZhang2005a for the three-parameter asymmetric Laplace distribution

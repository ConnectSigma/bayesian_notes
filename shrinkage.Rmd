Consider the single output linear Gaussian regression model with several input variables, given by
$$
\begin{aligned}[t]
y_i \sim N(\vec{x}_i' \vec{\beta}, \sigma^2)
\end{aligned}
$$
where $\vec{x}$ is a $k$-vector of predictors, and $\vec{\beta}$ are the coefficients.

What priors do we put on $\beta$? 

- **Improproper priors:** $\beta_k \propto 1$ This produces the equivalent of MLE estimates.
- **Non-informative priors:** These are priors which have such wide variance that they have little influence on the posterior, e.g. $\beta_k \sim N(0, 1e6)$. The primary reason for these (as opposed to simply using an improper prior) is that some MCMC methods, e.g. Gibbs sampling as used in JAGS or BUGS, require proper prior distributions for all parameters.
- **Weakly informative priors:** This is described in @GelmanJakulinPittauEtAl2008a, @Betancourt2017a, and are the defaults suggested in `r rpkg("rstanarm")`. Examples would include $\beta_k \sim \dnorm(0, 2.5)$ or $\beta_k \sim \dcauchy(0, 2.5)$. The purpose of these priors is to ex ante rule out implausible priors. This can help with computation, produce proper posterior distributions, and can help with some estimation issues such as separation or quasi-separation in discrete response models. These priors may help reduce the variance of posterior distribution estimates, they generally are intended to be weak enough to not affect the posterior mean point estimates too much.

**Shrinkage priors** have a couple characteristics

- they push $\beta_k \to 0$
- while in the other cases, the scale of the prior on $\beta$ is fixed, in shrinkage priors there is often a hyperprior on it. E.g. $\beta_k \sim N(0, \tau)$, where $\tau$ is also a parameter to be estimated.

Penalized Regression
$$
\hat{\beta}_{penalized} = \argmin_{\beta} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2 + f(\beta)
$$
where $f$ is some sort of penalty function on $\beta$ that penalizes larger (in magnitude) values of $\beta$.

Two common penalized regressions are lasso and ridge.

Ridge Regression

$$
\hat{\beta}_{ridge} = \argmin_{\beta} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2 + \lambda \sum_{k} \beta_k^2
$$
This penalty produces smaller in magnitude coefficients, $|\hat{\beta}_{ridge}| < |\hat{\beta}_{OLS}|$.
However, this "bias" in the coefficients can be offset by a lower variance, better MSE, and better out-of-sample performance than the OLS estimates.

LASSO: The LASSO replaces squared the penalty on $\beta$ with an absolute value penalty.
$$
\hat{\beta}_{lasso} = \argmin_{\beta} \frac{1}{2 \sigma} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2 + \lambda \sum_{k} |\beta_k|
$$
The absolute value penalty will put some $\hat{\beta}_k = 0$, producing a "sparse" solution.


## Bayesian formulation

$$
\log p(\theta|y, x) \propto \frac{1}{2 \sigma} \sum_{i = 1}^n (\vec{x}_i\T \vec{\beta} - y_i)^2 + \lambda \sum_{k} \beta_k^2
$$
In the first case, the log density of a normal distribution is,
$$
\log p(y | \mu, x) \propto \frac{1}{2 \sigma} (x - \mu)^2
$$
The first regression term is the produce of normal distributions (sum of their log probabilities),
$$
y_i \sim N(\vec{x}_i\T \vec{\beta}, \sigma) 
$$
The second term, $\lambda \sum_{k} \beta_k^2$ is also the sum of the log of densities of i.i.d. normal densities, with mean 0, and scale $\tau = 1 / 2 \lambda$,
$$
\beta_k \sim N(0, \tau^2)
$$

The only difference in the LASSO is the penalty term, which uses an absolute value penalty for $\beta_k$.
That term corresponds to a sum of log densities of i.i.d. double exponential (Laplace) distributions.
The double exponential distribution density is similar to a normal distribution,
$$
\log p(y | \mu, \sigma) \propto - \frac{|y - \mu|}{\sigma}
$$
So the LASSO penalty is equaivalent to the log density of a double exponential distribution with location $0$, and scale $1 / \lambda$.
$$
\beta_k \sim \dlaplace(0, \tau)
$$


There are several differences between Bayesian approaches to shrinkage and penalized ML approaches.

The most important is the primary point estimate.
In penalized ML, the point estimate is the mode. 
In full Bayesian estimation, the primary point estimate of interest is the mean. 
This difference will be most apparent in sparse shrinkage esimators such as the LASSO.

Using a double exponential penalty induces a sparse solution for $\hat\beta$ by placing the modes of the coefficients at zero. 
However, using the double exponential as a prior does not place the means of coefficients at zero.

Comparing the normal and double exponential distributions: 

1. the double exponential distribution has a spike at zero, unlike the normal
2. however, the probability mass of these distributions (important for the posterior mean) is not too different.


Difference

- The point estimates are the posterior modes. In Bayesian estimation,
    posterior means are the preferred point estimate.
- The shrinkage parameter $\lambda$ is often a chosen by cross-validation
    or an approximation thereof. In Bayesian estimation, the estimation 
    marginalizes over uncertainty about the shrinkage parameter.



Hierarchical Shrinkage (HS-$t_{\nu}$) 

$$
\begin{aligned}
\beta_k &\sim \dnorm(0, \lambda_i^2 \tau^2) \\
\lambda_i &\sim \dt{\nu}^{+}(0, 1)
\end{aligned}
$$
If $\nu = 1$, then this is the Horseshoe prior
[@CarvalhoPolsonScott2010a, @CarvalhoPolsonScott2009a, @PasKleijnVaart2014a, @DattaGhosh2013a, @PolsonScott2011a, @PiironenVehtari2016a]

Hierarchical Shrinkage Plus (HS-$t_{\nu}$+)

$$
\begin{aligned}
\beta_k &\sim \dnorm(0, \lambda_i^2 \eta_i^2 \tau^2) \\
\lambda_i &\sim \dt{\nu}^{+}(0, 1) \\
\eta_i &\sim \dt{\nu}^{+}(0, 1)
\end{aligned}
$$
This induces even more shrinkage towards zero than the 

If $\nu = 1$, then this is the Horseshoe+ prior as introduced by @BhadraDattaPolsonEtAl2015a.

@PiironenVehtari2015b, "Projection predictive variable selection using Stan+R"


In linear regression
$$
\begin{aligned}[t]
p(\beta | \Lambda, \tau, \sigma^2, D) &= N(\beta, \bar{\beta}, \Sigma) \\
\bar{\beta} &= \tau^2 \Lambda (\tau^2 \Lambda + \sigma^2 (X'X)^{-1})^{-1} \hat{\beta} \\
\Sigma &= (\tau^{-2} \Lambda^{-1} + \frac{1}{\sigma^2} X'X)^{-1}
\end{aligned}
$$
where $\Lambda = \diag(\lambda_1^2, \dots, \lambda_D^2)$, and $\hat{\beta}$ is the MLE estimate, $(X'X)^{-1} X' y$.
If predictors are uncorrelated with mean zero and unit variance, then 
$$
X'X \approx n I
$$
and
$$
\bar{\beta}_j = (1 - \kappa_j) \hat{\beta}_j
$$
where
$$
\kappa_j = \frac{1}{1 + n \sigma^{-2} \tau^2 \lambda_j^2}
$$
where $\kappa_j$ is the *shrinkage factor* for the coefficient $\beta_j$, which is how much it is shrunk towards zero from the MLE.
$\kappa_j = 1$ is complete shrinkage, and $\kappa_j = 0$ is no shrinkage.
So $\bar{\beta} \to 0$ as $\tau \to 0$ and $\bar{\beta} \to \hat{\beta}$ as $\tau \to \infty$.

Using a plug-in estimate of $\tau$ using cross-validatio or max marginal likelihood. 
The danger is that $\hat{\tau} = 0$ if it is very sparse.

van de Pas et al (2014) show that the optimal value (upp to a log factor) in terms of MSE and posterior contraction rates compared to the true $\beta^*$ is
$$
\tau^* = \frac{p^*}{n}
$$
where $p^*$ is the number of non-zero coefficients in the true coefficient vector $\beta^*$.

The effective number of nonzero coefficients is,
$$
m_{eff} = \sum_{j = 1}^D (1 - \kappa_j)
$$


- Allan Riddell. [Epistemology of the corral: regression and variable selection with Stan and the Horseshoe prior](https://www.ariddell.org/horseshoe-prior-with-stan.html) March 10, 2014.

## Worked Example

See the [documentation](https://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.info.txt).

```{r message = FALSE}
library("rstan")
library("loo")
library("glmnet")
library("tidyverse")
library("forcats")
library("rubbish")
```


### Example

```{r message = FALSE}
URL <- "https://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data"

col_types <- cols(
  X1 = col_integer(),
  lcavol = col_double(),
  lweight = col_double(),
  age = col_integer(),
  lbph = col_double(),
  svi = col_integer(),
  lcp = col_double(),
  gleason = col_integer(),
  pgg45 = col_integer(),
  lpsa = col_double(),
  train = col_logical()
)
prostate <- read_tsv(URL, col_types = col_types,
                     skip = 1,
                     col_names = names(col_types$cols))
```

Recall the prostate data example: we are interested in the level of prostate-specific antigen (PSA), elevated in men who have prostate cancer. 
The data `prostate` has data on on the level of prostate-specific antigen (PSA), which is elevated in men with prostate cancer, for `r nrow(prostate)` men with
prostate cancer, and clinical predictors. 


```{r}
f <- lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45 - 1L
```

```{r}
prostate_data <- lm_preprocess(f, data = prostate)[c("y", "X")] %>%
  within({
    X <- scale(X)
    K <- ncol(X)
    N <- nrow(X)  
  })
```

```{r}
run_with_tau <- function(tau, mod, data, ...) {
  cat("Tau = ", tau)
  data$tau <- tau
  fit <- sampling(mod, data = data, open_progress = FALSE, 
                   show_messages = FALSE, verbose = FALSE, ...)
  out <- list()
  out$summary <- summary(fit, par = "b")$summary %>%
    as.data.frame() %>%
    rownames_to_column("parameter")
  
  ## calculate posterior modes
  out$summary$mode <- apply(rstan::extract(fit, "b")[[1]], 2, LaplacesDemon::Mode)
  
  out$summary$tau <- tau
  out$loo <- loo(extract_log_lik(fit))
  out$lppd <- mean(extract_log_lik(fit))
  out$tau <- tau
  out
}
```

```{r results='hide',message=FALSE}
mod_lm_coef_normal_1 <- stan_model("stan/lm-coef-normal-1.stan")
```
```{r}
mod_lm_coef_normal_1
```

```{r}
tau_values <- 2 ^ seq(2, -5, by = -.5)
coefpath_normal <-
  map(tau_values, run_with_tau,
      mod = mod_lm_coef_normal_1, data = prostate_data)
```

```{r plot_coefpaths}
plot_coefpaths <- function(coefpaths, stat = "mean") {
  ggplot(map_df(coefpaths, "summary"), aes_string(x = "log2(tau)", y = stat,
                       colour = "fct_reorder2(parameter, tau, mean)", fill = "parameter")) +
    modelr::geom_ref_line(h = 0) +
    geom_line() +
    labs(colour = "Parameter")  
}
```
```{r}
plot_coefpaths(coefpath_normal)
```


```{r plot_coefpath_loo}
plot_coefpath_loo <- function(x) {
  map_df(x,
       function(x) {
         tibble(tau = x$tau,
                elpd = x$loo$elpd_loo,
                lppd = x$loo$llpd,
                p = x$loo$p_loo)
       }) %>%
    gather(parameter, value, -tau) %>%
    ggplot(aes(x = tau, y = value)) +
    geom_point() +
    geom_line() +
    facet_wrap(~ parameter, scale = "free_y", ncol = 1)
}
```
```{r}
plot_coefpath_loo(coefpath_normal)
```


Which is the "best" $tau$?
```{r get_best_tau}
get_best_tau <- function(coefpath) {
  map_df(coefpath,
       function(x) {
         tibble(tau = x$tau,
                elpd = x$loo$elpd_loo,
                p = x$loo$p_loo)
       }) %>%
    filter(elpd == max(elpd)) 
}
```

```{r}
get_best_tau(coefpath_normal)
```

The mean estimate of $\tau$ is higher than the best estimate, and there is some uncertainty over it. 
```{r}
mod_lm_coef_normal_2 <- stan_model("stan/lm-coef-normal-2.stan")
```
```{r}
fit_normal <- sampling(mod_lm_coef_normal_2, data = prostate_data,
                 control = list(adapt_delta = 0.95))
```

```{r}
summary(fit_normal, "tau")$summary
```

```{r}
loo(extract_log_lik(fit_normal))
```


```{r}
mcmc_dens(as.array(fit_normal), "tau")
```

```{r}
mcmc_dens(as.array(fit_normal), regex_pars = "^b")
```

### Double Exponential (Laplace) Prior

A second prior to consider for $\vec\beta$ is the Double Exponential.

```{r}
mod_lasso_1 <- stan_model("stan/lm-coef-lasso-1.stan")
```

```{r}
coefpath_lasso <- map(tau_values,
                      run_with_tau,
                   mod = mod_lasso_1,
                   data = prostate_data)
```

```{r}
plot_coefpaths(coefpath_lasso)
```
```{r}
plot_coefpaths(coefpath_lasso, "mode")
```

```{r}
plot_coefpath_pars <- function(coefpath) {
  ggplot(map_df(coefpath, "summary"), aes(x = log10(tau), y = mean)) +
    facet_wrap(~ parameter) +
    modelr::geom_ref_line(h = 0) +
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`), alpha = 0.2) +
    geom_line()  
}
plot_coefpath_pars(coefpath_lasso)
```


Which is the "best" $tau$?
```{r}
get_best_tau(coefpath_lasso)
```

```{r}
mod_lasso_2 <- stan_model("stan/lm-coef-lasso-2.stan")
```

```{r}
fit_lasso <- sampling(mod_lasso_2, data = prostate_data,
                 control = list(adapt_delta = 0.9))
```

```{r}
summary(fit_lasso, "tau")$summary
```

```{r}
loo(extract_log_lik(fit_lasso))
```

```{r}
mcmc_dens(as.array(fit_lasso), "tau")
```

```{r}
mcmc_dens(as.array(fit_lasso), regex_pars = "^b")
```



### Hierarchical Prior (HS)

The Hierarchical or Horseshoe Prior is defined as as a scale mixture of normals,
$$
\begin{aligned}[t]
\lambda_i &\sim \dt{\nu}(0, 1) \\
\end{aligned}
$$
In the original formulation [@CarvalhoPolsonScott2009a,@CarvalhoPolsonScott2010a] use a half-Cauchy ($\nu = 1$), but Stan suggests and `r rpkg("rstanarm")` uses 
a Student-t with $\nu = 3$, finding that it has better sampling performance than the half-Cauchy.

```{r}
mod_lm_coef_hs_1 <- stan_model("stan/lm-coef-hs-1.stan")
```

```{r}
coefpath_hs <- map(tau_values,
                   run_with_tau, 
                   mod = mod_lm_coef_hs_1,
                   data = c(prostate_data, list(df_local = 3)),
                   control = list(adapt_delta = 0.999, max_treedepth = 12))

```

```{r}
plot_coefpaths(coefpath_hs)
```
```{r}
plot_coefpaths(coefpath_hs, "mode")
```
```{r}
get_best_tau(coefpath_hs)
```

```{r}
plot_coefpath_loo(coefpath_hs)
```


```{r}
mod_lm_coef_hs_2 <- stan_model("stan/lm-coef-hs-2.stan")
```

```{r}
fit_hs <- sampling(mod_lm_coef_hs_2, data = c(prostate_data, list(df_local = 3, df_global = 3)),
                 control = list(adapt_delta = 0.995))
```

```{r}
summary(fit_hs, "tau")$summary
```

```{r}
loo(extract_log_lik(fit_hs))
```

```{r}
mcmc_dens(as.array(fit_hs), "tau")
```

```{r}
mcmc_dens(as.array(fit_hs), regex_pars = "^b\\[\\d+\\]$")
```

```{r}
mod_lm_coef_hs_3 <- stan_model("stan/lm-coef-hs-3.stan")
```

```{r}
fit_hs3 <- sampling(mod_lm_coef_hs_3, data = c(prostate_data, list(df_local = 3, df_global = 3, p0 = 2)),
                 control = list(adapt_delta = 0.995))
```

### Comparison

Let's compare the various coefficient paths:

```{r}
all_coefpaths <-
  bind_rows(mutate(map_df(coefpath_normal, "summary"), model = "normal"),
          mutate(map_df(coefpath_lasso, "summary"), model = "lasso"),
          mutate(map_df(coefpath_hs, "summary"), model = "hs"))
ggplot(all_coefpaths, aes(x = log2(tau), y = mean, colour = model)) + 
  modelr::geom_ref_line(h = 0) +
  geom_line() +
  facet_wrap(~ parameter)
```


## Shrinkage Parameters

Given the linear Gaussian regression model
$$
y_i &\sim \dnorm(\vec{\beta}\T \vec{x}, \sigma^2)
$$
for $i = 1, \dots, n$, where $\vec{x}$ is the $K$ dimensional vector of predictors.
Suppose a prior
$$
\begin{aligned}[t]
\beta_j | \lambda_j, \tau &\sim N(0, \lambda_j^2 \tau^2)
\end{aligned}
$$
The $\lambda_j$ are local scales - it allows some weights to escape the shrinkage.
The global parameter $\tau$ pulls all weights towards zero, and effectively controls the sparsity.

The posterior distribution is
$$
\begin{aligned}[t]
p(\vec{\beta} | \mat{\Lambda}, \tau, \sigma^2, \mat{X}, \vec{y}) &= \dnorm(\vec{\beta}, \bar{\vec{\beta}}, \mat{\Sigma}) \\
\bar{\vec{\beta}} &= \tau^2 \mat{\Lambda}(\tau^2 \mat{\Lambda} + \sigma^2 (\mat{X}\T \mat{X})^{-1})^{-1} \hat{\vec{\beta}} \\
\mat{\Sigma} &= (\tau^{-2} \mat{\Lambda}^{-1} + \frac{1}{\sigma^2} \mat{X}\T \mat{X})^{-1},
\end{aligned}
$$
where 
$$
\mat{\Lambda} = \diag(\lambda_1^2, \dots, \lambda_K^2) 
$$
and 
$$
\hat{\vec{\beta}} = (\mat{X}\T \mat{X})^{-1} \mat{X}\T \vec{y}
$$
is the MLE solution if $(\mat{X}\T \mat{X})^{-1}$ exists.

It the predictors are uncorrelated with zero mean and unit variance, then $\mat{X}\T \mat{X} \approx n \mat{I}$, and approximate
$$
\bar{\beta}_j = (1 - \kappa_j) \hat{\beta}_j
$$
where $\kappa_j$ is the shrinkage factor for coefficient $j$,
$$
\kappa_j = \frac{1}{1 + n \sigma^{-2} \tau^2 \lambda^2_j}
$$
When $\kappa = 1$, it is complete shrinkage, and the coefficient is zero.
When $\kappa = 0$, then there is no shrinkages, and the coefficient is equal to the MLE solution.
As $\tau \to 0$, then $\bar{\beta} \to 0$, and as $\tau \to \infty$, then $\bar{\beta} \to \hat{\beta}$.

```{r}
library("tidyverse")
kappa <- seq(.005, .995, by = 0.005)
lambda <- (1 - kappa) / kappa

f <- function(x) sqrt(1 / x - 1)
f_jacobian <- function(x) 1 / (sqrt(1 / x - 1) * x ^ 2)
f2 <- function(x) 1 / x - 1
f2_jacobian <- function(x) x ^ (-2)

funs <- list(
  function(x) {
    tibble(kappa = x, 
           dens = dt(f(kappa), df = 3) * f_jacobian(kappa),
           name = "HS (df = 3)")
  },
  function(x) {
    tibble(kappa = x, 
           dens = dt(f(kappa), df = 2) * f_jacobian(kappa),
           name = "HS (df = 2)")
  },  
  function(x) {
    tibble(kappa = x,
           dens = dcauchy(f(kappa)) * f_jacobian(kappa),
           name = "HS (df = 1)")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 0.5, 0.5) * (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Student t (df = 1)")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 3 / 2, 3 / 2) * (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Student t (df = 3)")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 1000, 1000) * (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Normal")
  },
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dgamma(x / (1 - x), 0.0001, 0.0001) * (1 / (1 - x) + x / (1 - x) ^ 2),
           name = "Student t (df = 0)")
  },  
  function(x) {
    df <- 3
    tibble(kappa = x,
           dens = dexp(f2(kappa), 0.5) * f2_jacobian(kappa),
           name = "Double Exponential")
  }    
)

shrinkages <- invoke_map_df(funs, x = kappa)

ggplot(shrinkages, aes(x = kappa, y = dens)) +
  geom_line() +
  facet_wrap(~ name, scales = "free_y")

```

Note that for these distributions:

- Normal: prior puts weight only on a single point
- HS for df = 0: prior on shrinkage parameter puts weight on either completely shrunk ($\kappa = 1$) or unshrunk ($\kappa = 0$)
- HS for df = 3: prior on shrinkage parameter puts a lo of weight on it being completely shrunk ($\kappa = 1$), but truncates the density for completely unshrunk.


## Choice of Hyperparameter on $\tau$

The value of $\tau$ and the choice of its hyper-parameter has a big influence on the sparsity of the coefficients.

@CarvalhoPolsonScott2009a suggest 
$$
\tau \sim \dhalfcauchy(0, \sigma),
$$
while @PolsonScott2011a suggest,
$$
\tau \sim \dhalfcauchy(0, 1) .
$$

@PasKleijnVaart2014a suggest 
$$
\tau \sim \dhalfcauchy(0, p^* / n)
$$
where $p^*$ is the true number of non-zero parameters,
and $n$ is the number of observations.

@PiironenVehtari2016a understand the choice of the prior on $\tau$ as the implied prior on the number of effective parameters.
The shrinkage can be understood as its influence on the number of effective parameters, $m_{eff}$,
$$
m_{eff} = \sum_{j = 1}^K (1 - \kappa_j) .
$$
This is a measure of effective model size.

The mean and variance of $m_{eff}$ given $\tau$ and $\sigma$ are,
$$
\begin{aligned}[t]
\E[m_{eff} | \tau, \sigma] &= \frac{\sigma^{-1} \tau \sqrt{n}}{1 + \sigma^{-1} \tau \sqrt{n}} K , \\
\Var[m_{eff} | \tau, \sigma] &= \frac{\sigma^{-1} \tau \sqrt{n}}{2 (1 + \sigma^{-1} \tau \sqrt{n})2} K .
\end{aligned}
$$

Based on this, a prior should be chosen so that the prior mass is located near,
$$
\tau_0 = \frac{p_0}{K - p_0}\frac{\sigma}{\sqrt{n}}
$$

Densities of the shrinkage parameter, $\kappa$, for various shrinkage distributions where $\sigma^2 = 1$, $\tau = 1$, for $n = 1$.

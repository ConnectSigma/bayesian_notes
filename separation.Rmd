---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Separation

```{r message=FALSE}
library("rstan")
library("rstanarm")
library("tidyverse")
library("recipes")
```

Separation is when a predictor perfectly predicts a binary response variable [@Rainey2016a, @Zorn2005a].

-   *complete separation*: the predictor perfectly predicts both 0's and 1's.
-   *quasi-complete separation*: the predictor perfectly predicts either 0's or 1's.

This is related and similar to identification in MLE and multicollinearity in OLS.

The general solution is to penalize the likelihood, which in a Bayesian context is equivalent to placing a proper prior on the coefficient of the separating variable.

Using a weakly informative prior such as those suggested by is sufficient to solve separation,
$$
\beta_k \sim \dnorm(0, 2.5)
$$
where all the columns of $\code{x}$ are assumed to mean zero, unit variance (or otherwise standardized).
The half-Cauchy prior, $\dhalfcauchy(0, 2.5)$, suggested in @GelmanJakulinPittauEtAl2008a is insufficiently informative to  to deal with separation [@GhoshLiMitra2015a], but finite-variance weakly informative Student-t or Normal distributions will work.

These are the priors suggested by [Stan](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) and 
used by default in **rstanarm** `r rdoc("rstanarm", "stan_glm")`.

## Example: Complete Separation Data

The following data is an example of data with complete separation.[^fake-separation]
```{r}
data1 <- tribble(
  ~y, ~x1, ~x2,
  0, 1, 3,
  0, 2, 2,
  0, 3, -1,
  0, 3, -1,
  1, 5, 2,
  1, 6, 4,
  1, 10, 1,
  1, 11, 0  
)
```

```{r}
count(data1, y, x1) %>%
  group_by(x1) %>%
  mutate(p = n / sum(n)) %>%
  select(-n) %>% 
  spread(y, p, fill = 0)
```

The variable `x1` perfectly separates `y`, since when `x1 <= 3`, `y = 0`,
and when `x1 > 3`, `y = 1`.

```{r}
glm(y ~ x1 + x2, data = data1, family = binomial()) %>%
  summary()
```

## Example: Quasi-Separation

The following generated data is an example of quasi-separation.[^quasi-separation]

```{r}
data2 <- tibble(
  y = c(0, 0, 0, 0, 1, 1, 1, 1, 1, 1),
  x1 = c(1, 2, 3, 3, 3, 4, 5, 6, 10, 11),
  x2 = c(3, 0, -1, 4, 1, 0, 2, 7, 3, 4)
)
```

```{r}
count(data2, y, x1) %>%
  group_by(x1) %>%
  mutate(p = n / sum(n)) %>%
  select(-n) %>% 
  spread(y, p, fill = 0)
```

The variable `x1` almost perfectly separates `y`.
When `x1 < 3`, then `y = 0`.
When `x1 > 3`, then `y = 1`.
Only when `x1 = 3`, does `y` takes values of either 0 or 1.

```{r}
glm(y ~ x1 + x2, data = data2, family = binomial()) %>%
  summary()
```

## Example: Support of ACA Medicaid Expansion

This example is from @Rainey2016a from the original paper @BarrilleauxRainey2014a
with replication code [here](https://github.com/carlislerainey/separation).
Load the data included in the **jrnold.bayes.notes** package:
```{r}
data("politics_and_need", package = "jrnold.bayes.notes")
```

What happens when estimated with GLM?
```{r}
glm(oppose_expansion ~ gop_governor + percent_favorable_aca + gop_leg +
         percent_uninsured + bal2012 + multiplier + percent_nonwhite +
         percent_metro,
       data = politics_and_need, family = binomial()) %>%
      summary()
```

For Stan, preprocess the data:
```{r}
rec <- recipe(oppose_expansion ~ gop_governor + percent_favorable_aca + 
                gop_leg + percent_uninsured + bal2012 + multiplier +
                percent_nonwhite + percent_metro,
              data = politics_and_need) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  prep(politics_and_need, retain = TRUE)

X <- juice(rec, composition = "matrix")
y <- juice(rec, composition = "matrix")
```


Estimate with **rstanarm**.
```{r}
f <-oppose_expansion ~ gop_governor + percent_favorable_aca + 
      gop_leg + percent_uninsured + bal2012 + multiplier +
      percent_nonwhite + percent_metro
fit1 <- stan_glm(f, data = politics_and_need, family = "binomial")
```

What if no prior is used? Compare estimates and efficiency.
```{r}
fit2 <- stan_glm(f, data = politics_and_need, prior = NULL, family = "binomial")
```

## References

@Rainey2016a provides a mixed MLE/Bayesian simulation based approach to apply a prior to the variable with separation, while keeping the other coefficients at their MLE values.
Since the results are highly sensitive to the prior, multiple priors should be tried (informative, skeptical, and enthusiastic).

@Firth1993a suggests a data-driven Jeffreys invariant prior. This prior was also recommended in @Zorn2005a.

@GreenlandMansournia2015a suggest a log-F prior distribution which has an intuitive interpretation related to the number of observations.

[^fake-separation]: [FAQ: What is Complete or Quasi-Complete Separation in Logistic/Probit Regression and How do We Deal With Them?](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them/)

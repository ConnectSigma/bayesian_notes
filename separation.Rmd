---
output: html_document
editor_options:
  chunk_output_type: console
---

# Separation

```{r message=FALSE}
library("rstan")
library("rstanarm")
library("tidyverse")
library("recipes")
library("jrnold.bayes.notes")
```

Separation is when a predictor perfectly predicts a binary response variable [@Rainey2016a, @Zorn2005a].

For classification problems, there are three cases.

-   *complete separation*: the predictor perfectly predicts both 0's and 1's.
-   *quasi-complete separation*: the predictor perfectly predicts either 0's or 1's.
-   *overlap*: the predictor does not perfectly predict either 0's or 1's.

Both **complete separation** and **quasi-complete** separation cause problems for binomial maximum likelihood estimators.

## Example: Complete Separation Data

The following data is an example of data with complete separation.[^fake-separation]
```{r}
CompleteSeparation
```

```{r}
count(CompleteSeparation, y, x1) %>%
  group_by(x1) %>%
  mutate(p = n / sum(n)) %>%
  select(-n) %>%
  spread(y, p, fill = 0)
```
The variable `x1` perfectly separates `y`, since when `x1 <= 3`, `y = 0`,
and when `x1 > 3`, `y = 1`.

The MLE of the binomial likelihood with a logistic link function for this data has a finite log-likelihood, but the optimal line is a step function. This pushes the coefficient for the separating variable to $\infty$.
```{r}
ggplot(CompleteSeparation, aes(x = x1, y = y)) +
  geom_point() +
  geom_smooth(method = "glm", formula = y ~ x, se = FALSE,
              method.args = list(family = binomial()), colour = "gray")
```

If we estimate a binomial model with this data, it will warn that some observations have predicted probabilities close to zero or one.
```{r fit_cs1}
fit_cs1 <- glm(y ~ x1 + x2, data = CompleteSeparation, family = binomial())
summary(fit_cs1)
```
Additionally, the standard errors are implausibly large.

## Example: Quasi-Separation

The following generated data is an example of quasi-separation.[^quasi-separation]
```{r}
knitr::kable(QuasiSeparation)
```

The variable `x1` *almost* separates `y`.
When `x1 < 3`, `y = 0`, and when `x1 > 3`, `y = 1`.
Only when `x1 = 3`, does `y` takes values of both 0 and 1.
```{r}
count(QuasiSeparation, y, x1) %>%
  group_by(x1) %>%
  mutate(p = n / sum(n)) %>%
  select(-n) %>%
  spread(y, p, fill = 0)
```

In the quasi-separation case, like the complete separation case, the best line is something close to a step function.
Unlike the **complete separation** case, the coefficient for the separating variable takes a finite, but very large value.
```{r}
ggplot(QuasiSeparation, aes(x = x1, y = y)) +
  geom_point() +
  geom_smooth(method = "glm", formula = y ~ x, se = FALSE,
              method.args = list(family = binomial()), colour = "gray")
```

```{r}
fit_qs1 <- glm(y ~ x1 + x2, data = QuasiSeparation, family = binomial())
summary(fit_qs1)
```

## Weak Priors

While the likelihood is unidentified, weakly informative priors on the regression coefficients will deal with separation.
$$
\beta_k \sim \dnorm(0, 2.5)
$$
where all the columns of $\code{x}$ are assumed to have unit variance (or be otherwise standardized).
The half-Cauchy prior, $\dhalfcauchy(0, 2.5)$, suggested in @GelmanJakulinPittauEtAl2008a is insufficiently informative to  to deal with separation [@GhoshLiMitra2015a], but finite-variance weakly informative Student-t or Normal distributions will work.

These are the priors suggested by [Stan](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) and
used by default in **rstanarm** `r rdoc("rstanarm", "stan_glm")`.

When estimated with `stan_glm()`, the coefficients of both the complete separation and quasi-separated data are finite.
```{r message=FALSE}
fit_cs2 <- stan_glm(y ~ x1 + x2, data = CompleteSeparation,
                    family = binomial())
summary(fit_cs2)
```
```{r}
fit_qs2 <- stan_glm(y ~ x1 + x2, data = QuasiSeparation, family = binomial())
summary(fit_qs2)
```

## Example: Support of ACA Medicaid Expansion

This example is from @Rainey2016a from the original paper @BarrilleauxRainey2014a
with replication code [here](https://github.com/carlislerainey/separation).
Load the data included in the **jrnold.bayes.notes** package:
```{r}
data("politics_and_need", package = "jrnold.bayes.notes")
```

The observations are the governors of the US states.
The outcome variable is their votes on the Affordable Care Act (ACA) Medicaid Expansion.
The dataset includes multiple predictors, including whether the governor is a Republican (`gop_governor`).
Add Democratic governors supported the expansion (`gop_governor == 0`),
and only Republican governors (`gop_governor == 1`) opposed it (though not all).
```{r}
# count(politics_and_need, gop_governor, oppose_expansion)
```
This is a case of quasi-separation.

What happens when this model is estimated with MLE by `glm()`?
```{r}
aca_fmla <-
  oppose_expansion ~ gop_governor + percent_favorable_aca + gop_leg +
         percent_uninsured + bal2012 + multiplier + percent_nonwhite +
         percent_metro
fit_aca1 <- glm(aca_fmla, data = politics_and_need, family = binomial())
summary(fit_aca1)
```

Now estimate with **rstanarm** using the default weakly informative priors.
```{r message=FALSE}
fit_aca2 <- stan_glm(aca_fmla, data = politics_and_need,
                     family = "binomial",
                     show_messages = FALSE, refresh = -1,
                     verbose = FALSE)
```
```{r}
summary(fit_aca2)
```

### Questions

-   Estimate the model using `stan_glm()` with a flat prior, and a Student-t
    distribution with `df = 3`. Compare the coefficient estimates and the
    efficiency ($\hat{R}$, ESS).

-   @GhoshLiMitra2015a suggest that a Half-Cauchy prior distribution is
    insufficient for dealing with separation. Try estimating this model with
    a Cauchy prior with a scale of 2.5. Compare the coefficient estimates
    and efficiency ($\hat{R}$, ESS).

-   See the other application in @Rainey2016a on nuclear proliferation and
    war. Replicate the analysis with the informative, skeptical, and
    enthusiastic priors.  The data can be found at [carlislerainey/priors-for-separation](https://github.com/carlislerainey/priors-for-separation/tree/master/bm-replication).

## References

See @AlbertAnderson1984a, @HeinzeSchemper2002a, and @Heinze2006a for discussion
about separation.

@Rainey2016a provides a mixed MLE/Bayesian simulation based approach to apply a prior to the variable with separation, while keeping the other coefficients at their MLE values.
Since the results are highly sensitive to the prior, multiple priors should be tried (informative, skeptical, and enthusiastic).

@Firth1993a suggests a data-driven Jeffreys invariant prior. This prior was also recommended in @Zorn2005a.

@GreenlandMansournia2015a suggest a log-F prior distribution which has an intuitive interpretation related to the number of observations.

[^fake-separation]: [FAQ: What is Complete or Quasi-Complete Separation in Logistic/Probit Regression and How do We Deal With Them?](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them/)

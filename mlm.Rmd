# Hierarchical Models

## Baseball Example



```{r}
data("bball1970", package = "rstanarm")
```


```{r}
bball1970_data <- list(
  N = nrow(bball1970),
  k = bball1970$AB,
  y = bball1970$Hits,
  k_new = bball1970$RemainingAB,
  y_new = bball1970$RemainingHits
)
```

```{r results='hide'}
binomial_nopool <- stan_model("stan/binomial-no-pooling.stan")
```
```{r results='hide}
binomial_pool <- stan_model("stan/binomial-complete-pooling.stan")
```
```{r results='hide'}
binomial_partial <- stan_model("stan/binomial-partial-pooling.stan")
```


```{r}
bball1970_pool <- sampling(binomial_pool, data = bball1970_data)
loo_pool <- loo(extract_log_lik(bball1970_pool, "log_lik"))
elpd_loo <- loo_pool$elpd_loo / bball1970_data$N
elpd_new <- mean(log(colMeans(exp(rstan::extract(bball1970_pool)[["log_lik_new"]]))))
```
```{r}
bball1970_nopool <- sampling(binomial_nopool, data = bball1970_data)
loo_partial <- loo(extract_log_lik(bball1970_nopool, "log_lik"))
lliknew_nopool <- mean(log(colMeans(exp(rstan::extract(bball1970_nopool)[["log_lik_new"]]))))
```
```{r}
bball1970_partial <- sampling(binomial_partial, data = bball1970_data)
loo_partial <- loo(extract_log_lik(bball1970_partial, "log_lik"))

elpd_new <- mean(log(colMeans(exp(rstan::extract(bball1970_partial)[["log_lik_new"]]))))
```

### Eight Schools



## Equivalent Models

### Group Varying Intercepts

This is a regression, with a different intercept per group:
$$
\begin{aligned}[t]
y_i &\sim N(\alpha_j[i] + \beta x_i, \sigma_y^2) \\
\end{aligned}
$$
The second level model of the group intercepts models them as distributed around a common mean, $\mu_\alpha$, with error:
$$
\alpha_j = \mu_\alpha + \eta_j \\
\eta_j \sim N(0, \sigma_\alpha^2)
$$

### Separate local regressions

For each group, run a regression,
$$
\begin{aligned}[t]
y_i \sim N(\alpha_j + \beta x_i, \sigma_y^2) & \text{for all $i$ in group $j$}
\end{aligned}
$$
And now model the group-level means,
$$
\begin{aligned}[t]
\alpha &= \gamma u_j + \eta_j \\
\eta_j &\sim N(0, \sigma^2_\alpha)
\end{aligned}
$$


### Modeling the coefficients of a large regression model

Suppose that $X$ includes all predictors and $J$ indicators for the $J$ groups. 

We could also put the constant in the second distribution. The coefficients $\beta$ for the coefficients on the group indicators are centered around a $\mu_\alpha$.
$$
\begin{aligned}[t]
y_i &\sim N(x_i \beta, \sigma_y^2) \\
\beta_j &\sim N(\mu_{\alpha}, \sigma_{\alpha}^2)
\end{aligned}
$$

### Regression with multiple error terms

$$
\begin{aligned}[t]
y_i &\sim N(x_i \beta + \eta_{j[i]}, \sigma_y^2) \\
\eta_j &\sim N(0, \sigma_{\alpha}^2)
\end{aligned}
$$

### Regresion with correlated errors

$$
\begin{aligned}[t]
y_i = X_i \beta + \omega_i, & \omega \sim N(0, \Sigma)
\end{aligned}
$$
The errors have an $n \times n$ covaraince matrix, and are
equivalent to the sum of individual and group errors.
$$
\omega_i = \eta_{j[i]} + \epsilon_i
$$

The variances and covariances in $\Sigma$ are

- for unit $i$: $\Sigma_{ii} = \var(\omega_i) = \sigma_y^2 + \sigma_{\alpha}^2$
- for units $i$, $k$ in same group $j$: $\Sigma_{ik} = \cov(\omega_i, \omega_k) = \sigma_{\alpha}^2$
- for units $i$, $k$ in the same group $j$: $\Sigma_{ik} = 0$
